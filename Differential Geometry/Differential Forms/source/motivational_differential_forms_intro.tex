\documentclass[a4paper,12pt]{article}

\usepackage[left=25mm, right=25mm, top=25mm, bottom=25mm]{geometry}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage[style=iso]{datetime2}
\usepackage[explicit]{titlesec}
\usepackage{amsthm}
\usepackage{mathrsfs}
\usepackage{array}
\usepackage{graphicx}
\usepackage{mathtools} % Provides \mathrlap command
\usepackage{xparse}
\usepackage{float}
\usepackage{fixdif}
\usepackage[skip=1em,indent]{parskip}
\usepackage{caption}
\usepackage{tocloft}
\usepackage{enumitem}
\usepackage[toc, page]{appendix}
\usepackage{accents}
\usepackage[perpage, symbol*]{footmisc} % for dagger footnotes
\DefineFNsymbols{sym}{\textdagger \textdaggerdbl \textsection \textparagraph %
{\textdagger\textdagger} {\textdaggerdbl\textdaggerdbl} %
{\textsection\textsection} {\textparagraph\textparagraph}}
\setfnsymbol{sym}
\interfootnotelinepenalty=10000

\usepackage{tikz}
\usepackage{tikz-3dplot}
\tikzset{>=latex} % for LaTeX arrow head
\usepackage{pgfplots} % for the axis environment
\usetikzlibrary{calc,decorations.markings,external}
\tikzexternalize[prefix=tikz/]

\pgfplotsset{compat=1.18, every tick label/.append style={font=\footnotesize}}

\graphicspath{ {./Images/} }

\usepackage[backend=biber, style=numeric, sorting=none]{biblatex}
\addbibresource{motivational_differential_forms_intro.bib}

% ---------------- %
\usepackage{etoolbox}
\usepackage[dvipsnames]{xcolor}
\usepackage[linkcolor=black]{hyperref}
\usepackage[nameinlink]{cleveref}

% Configure cleveref for appendix sections and figures
\crefname{appsec}{Appendix}{appendices}
\Crefname{appsec}{Appendix}{Appendices}
\crefname{figure}{Figure}{Figures}
\Crefname{figure}{Figure}{Figures}


\definecolor{RoyalRed}{RGB}{155, 28, 49}
\definecolor{RoyalOrange}{RGB}{249, 146, 69}
\definecolor{PigmentBlue}{RGB}{51,51,153}

\makeatletter %* Creates a footnote colour option
\def\@footnotecolor{black}
\define@key{Hyp}{footnotecolor}{%
 \HyColor@HyperrefColor{#1}\@footnotecolor%
}
\patchcmd{\@footnotemark}{\hyper@linkstart{link}}{\hyper@linkstart{footnote}}{}{}
\makeatother

\hypersetup{
  colorlinks = true,
  linkcolor = PigmentBlue,
  urlcolor = RoyalRed,
  citecolor = RoyalOrange,
  footnotecolor = black,
}
% ---------------- %
\makeatletter
\def\th@plain{%
  \thm@notefont{}% same as heading font
  \itshape % body font
}
\def\th@definition{%
  \thm@notefont{}% same as heading font
  \normalfont % body font
}
\makeatother

\newcommand*\diff{\mathop{}\!d} % for the differential in integrals
% ---------------- %
\ExplSyntaxOn %* This command is for row vectors; unstarred version has no delimiter scaling for inline; starred version has delimiter scaling for display
\seq_new:N \l_user_rvec_seq

% expl3 worker (does the splitting/formatting)
\cs_new_protected:Npn \dbacc_rvec:n #1
 {
  \seq_clear:N \l_user_rvec_seq
  \seq_set_split:Nnn \l_user_rvec_seq { , } { #1 }
  \seq_use:Nn \l_user_rvec_seq { \enspace }
 }

% plain-name wrapper (no ':' in its name) — safe to call outside ExplSyntaxOn
\cs_new_protected:Npn \dbaccrvec #1 { \dbacc_rvec:n { #1 } }

% user-level \rvec interface (star + optional size + mandatory arg)
\NewDocumentCommand \rvec { s o m }
 {
  \IfBooleanTF{#1}
    { \left[\,\dbaccrvec{#3}\,\right] } % starred -> automatic \left...\right
    { \IfValueTF{#2}
        { \mathopen{#2[}\,\dbaccrvec{#3}\,\mathclose{#2]} } % sized optional arg
        { [\,\dbaccrvec{#3}\,] } % default small delimiters
    }
 }
\ExplSyntaxOff
% ---------------- %
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\Z}{\mathbb{Z}}

\NewCommandCopy{\oldIm}{\Im}
\renewcommand{\Im}{\mathop{\oldIm\mathfrak{m}}}
\NewCommandCopy{\oldRe}{\Re}
\renewcommand{\Re}{\mathop{\oldRe\mathfrak{e}}}

\newcommand{\overbar}[1]{\mkern 1.5mu\overline{\mkern-1.5mu#1\mkern-1.5mu}\mkern 1.5mu}

\DeclareMathOperator{\Res}{Res}

\DeclarePairedDelimiterX{\norm}[1]{\lVert}{\rVert}{#1}
\DeclarePairedDelimiterX{\abs}[1]{\lvert}{\rvert}{#1}
\NewDocumentCommand{\eval}{s m m g}{%
  \IfBooleanTF{#1}
    {% starred version - no scaling
      #2 \vert%
      _{#3}%
      \IfNoValueTF{#4}{}{^{#4}}%
    }
    {% unstarred version - auto-scaled
      \left. #2 \right|%
      _{#3}%
      \IfNoValueTF{#4}{}{^{#4}}%
    }%
}

\makeatletter
\NewDocumentCommand{\extp}{e{^}}{%
  \mathop{\mathpalette\extp@{#1}}\nolimits
}
\NewDocumentCommand{\extp@}{mm}{%
  \textstyle\bigwedge\nolimits\IfValueT{#2}{^{\extp@@{#1}#2}}%
  \IfValueT{#1}{\kern-2\scriptspace\nonscript\kern2\scriptspace}%
}
\newcommand{\extp@@}[1]{%
  \mkern
    \ifx#1\displaystyle-1.8\else
    \ifx#1\textstyle-1\else
    \ifx#1\scriptstyle-1\else
    -0.5\fi\fi\fi
  \thinmuskip
}
\makeatletter
% ---------------- %
\newcommand{\ihat}{\boldsymbol{\hat{\imath}}}
\newcommand{\jhat}{\boldsymbol{\hat{\jmath}}}
\newcommand{\khat}{\boldsymbol{\hat{k}}}
\newcommand{\ehat}{\mathbf{\hat{e}}}
\newcommand{\T}{\mathrm{T}}
% ---------------- %

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}{Corollary}[theorem]

\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}

% \setlength{\cftsecnumwidth}{3em}

\begin{titlepage}
\title{A Motivational Introduction to Differential Forms}
\author{Abdul Musthakin}
\date{November 2025}
\end{titlepage}

% \renewcommand{\thesection}{\Roman{section}}

\allowdisplaybreaks

\setlength{\parindent}{0pt}

\begin{document}
\maketitle

\tableofcontents

\pagebreak

\section*{Prerequisites}

This exposition aims to require as little prerequisite knowledge as possible in order for one to be able to understand the presented concepts.
However, there are still some topics that are absolutely essential.
This includes concepts in single and multivariable calculus, as well as introductory linear algebra.
The rigorous underpinnings of these topics, such as the contents of a course on real analysis, are not required.
If relevant, they will be attached as an appendix.

\subsection*{1. Linear Algebra}
Expertise in linear algebra is by no means necessary.
A surface-level understanding of the main ideas should suffice:
\begin{itemize}
    \item The intuitive idea of vectors as ordered lists of numbers or arrows in a plane.
    \item Elements of $\R^n$ as vectors.
    \item The concept of matrices being linear maps and representing transformations, as well as basic ideas relating to them (linear maps in general are important).
    \item Vector operations: dot (scalar) product, inner product, cross product.
    \item Matrix multiplication, determinants, transpose, and inverse of matrix.
    \item The idea of vectors being more general objects (elements of a vector space).
    \item The basis and dimension of a vector space, and the concept of vector subspaces.
\end{itemize}
Besides general knowledge of vectors and linear algebra, the most important concept here to be aware of is that vectors are much more general than coordinates and arrows.
We will encounter objects that are vectors but are completely different from the aforementioned `typical' examples.
See \cref{appendix:vector_space} for the definition of a vector space, but note that this is only intended to be a reference.

\subsection*{2. Multivariable Calculus}
Knowledge of some concepts in multivariable calculus is mandatory.
Single-variable calculus is obviously a requirement by extension.
Whilst the topic of differential geometry -- where differential forms are usually found -- is quite a bit more advanced, this introductory exposition is supposed to be a natural part or extension to the concepts of multivariable calculus.
Thus, full familiarity with the subject is not required; many ideas will be developed along the way.
It assumed that one is aware of:
\begin{itemize}
    \item functions of the type $\mathbb{R}^m \to \mathbb{R}^n$;
    \item partial derivatives and the chain rule;
    \item directional derivatives and their computation;
    \item the intuitive ideas behind gradients, divergence, and curl, as well as how to compute them; and
    \item the Jacobian matrix of a multivariable function.
\end{itemize}
A formal introduction to vector calculus is presented in \cref{appendix:vector calculus}.
This will include the definition of the limit of vector-valued functions and the derivation of the Jacobian matrix.
Knowledge of some concepts from real analysis is required to understand this, but they are not required for the main body of the exposition.
The more experience one has with multivariable calculus, the easier it will be for them to understand this exposition.
Just as importantly, it will better enable us to appreciate the value of what is to be introduced.

The last point holds true for any other topic in maths that may be related to differential forms.
This comes both from the perspective of acquiring more knowledge, and gaining experience and maturity.
For instance, abstract algebra (e.g. group theory) is specifically not a prerequisite.
However, knowledge of concepts from it will frequently be useful.
These concepts are intuitive, so they can be mentioned whenever in an informal manner, avoiding the associated jargon.
We encounter isomorphisms between spaces, describing them as associating objects in a specific way, and so on.

\pagebreak

\section{Introduction \& Motivation}

Understanding what an integral means, in a general sense, is not particularly difficult.
Given a function, it is pretty easy to draw it as a curve on a graph.
By looking at its graph and choosing some interval on the $x$-axis, we can see the area between the curve and the $x$-axis.
This (signed) area is what is calculated by an integral.

\begin{figure}[!htbp]
\centering
\begin{tikzpicture}
\begin{axis}[
    width=12cm,
    height=6cm,
    axis lines=middle,
    xlabel={$x$},
    ylabel={$y$},
    xmin=0, xmax=6.5,
    ymin=-1.5, ymax=1.5,
    xtick={1.5708, 5.49779},
    xticklabels={},
    ytick=\empty,
    samples=200,
    domain=0:2*pi,
    enlargelimits=false,
    tick style={black, thick},
    major tick length=0.15cm,
]

% Labels for a and b
\node[below] at (axis cs:1.4, 0) {$a$};
\node[below] at (axis cs:5.7, 0) {$b$};

% Shaded area for positive region (a to π)
\addplot[fill=blue!30, opacity=0.5, domain=1.5708:3.14159, draw=none] 
    {sin(deg(x))} \closedcycle;

% Shaded area for negative region (π to b)
\addplot[fill=red!30, opacity=0.5, domain=3.14159:5.49779, draw=none] 
    {sin(deg(x))} \closedcycle;

% The sin(x) function
\addplot[black, thick, domain=0:2*pi] {sin(deg(x))};

% Vertical lines at x=a and x=b
\addplot[dashed, black] coordinates {(1.5708, -1.5) (1.5708, 1.5)};
\addplot[dashed, black] coordinates {(5.49779, -1.5) (5.49779, 1.5)};

% Label for the function
\node[above, black] at (axis cs:4.71239, 1) {$y = f(x)$};

% Labels for areas
\node[blue!70!black, font=\Large] at (axis cs:2.1, 0.45) {$+$};
\node[red!70!black, font=\Large] at (axis cs:4.52, -0.5) {$-$};

\end{axis}
\end{tikzpicture}
\caption{A definite integral}
\label{fig:definite_integral_sin}
\end{figure}

The area illustrated by the above figure is given by
\begin{equation*}
    \int_{a}^{b} f(x) \diff x.
\end{equation*}
Now, everything that was just stated has many asterisks.
We can have functions that cannot be integrated, or at least, not in the `usual' sense.
Even if we can integrate a function, we might not be able to get our integral (and thus desired area) in terms of functions that we are used to.
Even if we ignore all of that, there is plenty of theory regarding how we define integrals -- extending and generalizing to different applications.
So it turns out that integrals are not that simple, but their idea is.

The above refers specifically to definite integrals, and so too will further discussion.
It would thus be useful to have a brief mention of indefinite integrals for the sake of completeness.
An indefinite integral of some function $f$ is itself not a number, nor even a function.
It is a class of functions,\footnote{An equivalence class.} whose shared property is that their derivative is $f$.
A member of this class is referred to as an antiderivative of $f$, and it can be written as
\begin{equation*}
    F(x) \coloneq c + \int_{a}^{x} f(t) \diff t,
\end{equation*}
where $a$ and $c$ are some constants.
Since any antiderivative (and thus the indefinite integral) of a function can be rewritten in terms of a definite integral, there is nothing regarding the former concept that warrants its mention in the discussions ahead.

Consider the notation we use for integrals.
We have a unique symbol, $\int$, to always know when we are integrating.
We have the integrand after that, which is what we are integrating.
Also, we have the start and end points of our interval which we are integrating over.
Finally, we have something on the end.
It is definitely not useless, as it tells us what variable we are integrating with respect to.
For instance, it is true that
\begin{equation*}
    \int_{a}^{b} f(x) \diff x = \int_{a}^{b} f(t) \diff t = \int_{a}^{b} f(u) \diff u,
\end{equation*}
just as it is true that
\begin{equation*}
    \sum_{i=1}^{n} f(i) = \sum_{j=1}^{n} f(j) = \sum_{k=1}^{n} f(k).
\end{equation*}
The exact symbol we use for the variable does not matter; they are just labels for the same thing.
Now, in the context of sums, you can simply expand out to show that they are the same thing.
They all refer to $f(1) + f(2) + f(3) + \ldots + f(n)$.
It is not so straightforward for integrals, but the idea holds.
We could leave it at that, and say that the differential is just a part of the notation that conveys some useful information.
There is much more to it than that, however.

The word `differential' has many meanings in mathematics, and it can refer to completely different concepts in different fields (or even in the same field).
One thing that it can refer to is an infinitesimal -- a number closer to zero than any real number that is not itself zero.
Leibniz was the first to coin the term for that usage, although the idea of infinitesimals date back to \href{https://plato.stanford.edu/entries/continuity/}{ancient Greek mathematicians}.

Early developments in calculus throughout the 17th and 18th centuries by the likes of Newton and Leibniz were noticeably reliant on the idea of infinitesimals.
That itself was not a bad thing, but that the lack of rigour was.
We would see the development of a more rigorous foundation of calculus by mathematicians in the 19th century, particularly Cauchy.
Infinitesimals would be left in favour of limits, and the latter is the formulation that is universally taught in schools.

\begin{figure}[!htbp]
\centering
\begin{tikzpicture}
\begin{axis}[
    width=12cm,
    height=7cm,
    axis lines=middle,
    xlabel={$x$},
    ylabel={$y$},
    xmin=0, xmax=10,
    ymin=0, ymax=100,
    xtick={2, 7},
    xticklabels={$a$, $b$},
    ytick=\empty,
    samples=100,
    domain=0:10,
    enlargelimits=false,
    tick style={black, thick},
    major tick length=0.15cm,
    xticklabel style={font=\normalsize},
]


% Draw Riemann sum rectangles (left endpoint)
\foreach \i in {0,...,9} {
    \pgfmathsetmacro{\xleft}{2 + \i*0.5}
    \pgfmathsetmacro{\xright}{\xleft + 0.5}
    \pgfmathsetmacro{\height}{-15*(\xleft-5) + (\xleft-5)^3 + 50}
    \addplot[fill=blue!20, draw=blue!60!black, thick] coordinates {
        (\xleft, 0) (\xleft, \height) (\xright, \height) (\xright, 0)
    } -- cycle;
}

% The curve
\addplot[black, very thick, domain=0:10] {-15*(x-5) + (x-5)^3 + 50};

% Label for the function
\node[above right, black] at (axis cs:7, 65) {$y = f(x)$};

\end{axis}
\end{tikzpicture}
\caption{Riemann sum approximation of an integral}
\label{fig:riemann_sum}
\end{figure}

This information about differentials allows us to view the one attached to every integral in way beyond just useful notation.
Putting the formalism and rigorous definitions of integrals aside, they are fancy sums.
The area under a graph can be approximated by rectangles, as is shown by the above figure.
As the width of the rectangles decrease, the sums \href{https://commons.wikimedia.org/wiki/File:Riemann_sum_(leftbox).gif}{get closer} to the actual area under the curve.
That is, the limit of this sum is an integral.

Instead of thinking about it in terms of limits, we can imagine summing an infinite number of rectangles with infinitesimal width.
The height of a rectangle at some value of $x$ can be given by $f(x)$, so if the integral sums over areas, the $dx$ is the infinitesimal width.
This is all very imprecise and non-technical language.
However, it is enough to give us an intuitive feel of what an integral is and what the differential that is always attached to it means.

There is more to the peculiar symbol, though, and integrals are probably not the first place one would have spotted them.
That would instead be with regard to derivatives.
Indeed, the notation $dy/dx$ -- also attributed to Leibniz -- is very commonly used to denote the derivative of $y$ with respect to $x$.
When it is first taught to students at school, the warning will be given that it does not represent a fraction.
It just looks like a fraction, and treating it like a fraction works out almost all the time.

The resemblance it has to a ratio between two quantities is no coincidence, as Leibniz intended it to be just that: a ratio of infinitesimals.
The symbols $dx$ and $dy$ were used to represent infinitely small changes in $x$ and $y$ respectively.
Whilst we have, for the most part, left these imprecise notions behind, they will prove to be somewhat useful in understanding these symbols.\footnote{At the end of the day, the symbols mean whatever we want as long as it is consistent with the rest of our maths. That is why I gave the easy way out at the beginning, saying that the differential symbol in the integral can just be considered a part of the notation. However, as is being demonstrated, we can go further.}

\section{Differential of a Function}

Firstly, consider the fact that the derivative has many different notations.
The derivative of a function $f$ at $x$ may be expressed as $f'(x)$.
It follows that
\begin{equation*}
    \frac{dy}{dx} = f'(x).
\end{equation*}
This is just equating two things that are defined to mean the exact same thing.
Now, although the left-hand side is not a fraction, what if we rewrote the equation as follows?
\begin{equation*}
    dy = f'(x) \diff x
\end{equation*}
The right-hand side now looks exactly like an integrand.
This looks promising, but we did say beforehand that $dy/dx$ is not a fraction.
So, are we allowed to do that?
No. The symbols $dx$ and $dy$ have not been given a meaning outside of when they appear in $dy/dx$, so it makes no sense to separate them.
However, when something does not have meaning, we can always attempt to give it one and see where that leads us.

First, let $\Delta x$ be some real number, just as $x$ is.
Then, we can call $df$ the differential of $f$, a function of $x$.
The differential itself is a function of both $x$ and $\Delta x$, is defined as
\begin{equation*}
    df(x, \Delta x) \coloneq f'(x) \Delta x.
\end{equation*}
What was the point of that?
We went from performing an algebraic manipulation in a situation where it does not make sense, to creating a perfectly fine definition.
Since we have made it clear that $df$ is a function of two variables, we can write $df(x, \Delta x)$ as $df(x)$ or even $df$ (we are not changing the meaning).
The differential of the function $f(x) = x$ is given by
\begin{equation*}
    dx = 1 \cdot \Delta x = \Delta x
\end{equation*}
for all values of $x$.
This means that, even though $dx$ is technically still a function of $x$, we can just write $dx = \Delta x$.
With this, and by letting $y = f(x)$, we can rewrite our equation as
\begin{equation*}
    dy = f'(x) \diff x.
\end{equation*}
This looks exactly like what we already had, but now we have reached it by valid means.
We see that the differential $dy$ is defined in terms of $x$ and $dx$, and the differential $dx$ is essentially just some real number.
If we desired, we could go further to obtain $dy/dx = f'(x)$, but this can lead to confusion.
By that equation, $f'(x)$ equals a ratio of differentials that have each been separately defined.
However, $f'(x)$ is can be expressed with Leibniz's notation as $dy/dx$ -- which is not a ratio.\footnote{We cannot redefine the derivative to be a ratio of our two differentials. Their definitions involve the derivative, so it would be circular. Instead, view it as evidence for the self-consistency of Leibniz's notation}
This is just something to note.

We have now obtained our third interpretation of the differential, a function of two real numbers $x$ and $\Delta x$.
This can be extended to higher-order differentials, as well as to the differentials of multivariable functions.
Its usefulness is in the fact that it represents the principal part of the change of a function.
To see what that exactly means, consider some function\footnote{Technically, $f$ is the function and $f(x)$ is the value of the function at $x$.} $y=f(x)$ between two points $x$ and $x + \Delta x$.
Let $x$ be fixed.
The change in $x$ values is $\Delta x$ and the change in $y$ values is $\Delta y \coloneq f(x + \Delta x) - f(x)$.

By the definition of the derivative, $\Delta y / \Delta x \to f'(x)$ as $\Delta x \to 0$.
This means that for small $\Delta x$, we have $\Delta y / \Delta x \approx f'(x)$.
If the difference (or error) is $\varepsilon$, then $\Delta y / \Delta x = f'(x) + \varepsilon$.
Rearranging gives us
\begin{align*}
    \Delta y &= (f'(x) + \varepsilon) \Delta x \\
    &= f'(x) \Delta x + \varepsilon \Delta x \\
    &= dy + \varepsilon \Delta x.
\end{align*}
The change in $y$ is equal to a linear function of $\Delta x$, $dy$, plus a (generally) nonlinear error term.
Thus, $dy$ is the linear (or principal) part of $\Delta y$.
As $\Delta x \to 0$, we require that $\varepsilon \Delta x / \Delta x = \varepsilon \to 0$, i.e. the error term shrinks to zero faster than $\Delta x$.
This means that $\Delta y \to d y$, and $\Delta y \approx dy$ for small $\Delta x$.
An equivalent way to view the differential is as the best linear approximation to a function at some point.

Let us look at one use case of this: approximating the values of functions.
What is the value of $\sqrt{90}$?
Since the function $y = \sqrt{x}$ is strictly increasing, we know that $\sqrt{90}$ will be between $\sqrt{81}=9$ and $\sqrt{100}=10$.
Let $x = 100$ and $\Delta x = -10$, so that $x + \Delta x = 90$.
Thus, $\Delta y = \sqrt{90} - \sqrt{100} = \sqrt{90} - 10$.
This means that $10 - \Delta y$ is the exact value of $\sqrt{90}$, but we do not know what $\Delta y$ is.
Our approximation from before comes in handy here.

The derivative of $\sqrt{x}$ is given by
\begin{equation*}
    f'(x) = \frac{1}{2\sqrt{x}}.
\end{equation*}
At $x=100$, we have $f'(x) = 1/20 = 0.05$.
This is all we need for our approximation.
\begin{align*}
    \sqrt{90} &= \Delta y + 10 \\
    &\approx dy + 10 \\
    &= f'(10) \Delta x + 10 \\
    &= 0.05 \cdot -10 + 10 \\
    &= 9.5
\end{align*}
The actual value of $\sqrt{90}$ is $9.47213595\ldots$, so we were just 0.62\% off.
The computations were incredibly easy, which is the case as long as we have nice values of the function and its derivative to work with.
We can illustrate the efficacy of this method with a diagram.

\begin{figure}[!htbp]
\centering
\begin{tikzpicture}
\begin{axis}[
    width=12cm,
    height=7cm,
    axis lines=middle,
    xlabel={$x$},
    ylabel={$y$},
    xmin=0, xmax=196,
    ymin=0, ymax=14,
    xtick={90, 100},
    xticklabels={90, 100},
    ytick={sqrt(90)},
    yticklabels={$\sqrt{90}$},
    samples=100,
    domain=0:196,
    enlargelimits=false,
    tick style={black, thick},
    major tick length=0.15cm,
    xticklabel style={font=\small},
]

\addplot[red, very thick, domain=30:170] {x/(2*sqrt(90)) + (sqrt(90) - 90/(2*sqrt(90)))};

% The curve
\addplot[black, very thick, domain=0:196] {sqrt(x)};

\draw[fill] (axis cs:{90,sqrt(90)}) circle [radius=1.5pt];
\addplot[black, semithick, dashed, domain=0:100] {sqrt(90)};
\addplot[black, semithick, dashed, domain=0:90] (90, {sqrt(x)});

\addplot[black, semithick, dashed, domain=0:100] (100, {sqrt(x)});

% Label for the function
\node[above right, black] at (axis cs:150, 9) {$y = f(x)$};

\end{axis}
\end{tikzpicture}
\caption{Linear approximation of $\sqrt{90}$}
\label{fig:linaer_approximation}
\end{figure}

We calculated $dy$, the increase in the tangent, as an approximation to $\Delta y$, the increase in the function.
As can be seen from the figure, these are very similar values.
Since $f'(x) \to 0$ as $x \to \infty$ (the curve flattens), this method of approximation gets better for larger square roots.
That does not mean that this method is not useful for other functions, as it is generally applicable to functions that are sufficiently `nice' -- where a discussion on `niceness' is beside the point.

Another example is approximating $\ln 3$.
We know that $\ln e$ is 1, and $e \approx 3$.
Let $x = e$ and $\Delta x = 3 - e$, so that $x + \Delta x = 3$.
Then, $\Delta y = \ln 3 - \ln e = \ln 3 - 1$.
The derivative of $\ln x$ is simply $1/x$, so we have
\begin{equation*}
    \ln 3 \approx \frac{1}{e} \cdot (3 - e) + 1 = \frac{3}{e}.
\end{equation*}
The number $e$ is well-known enough for us to use its value for the approximation, giving us $\ln 3 \approx 1.104$.
The actual value is $1.09861229\ldots$, which means that there is a 0.46\% error.
These approximations are known as linear approximations, and they have a fair bit of applications.
The idea of calculating the linear part of $\Delta y$ can be made more precise with \emph{Taylor's theorem}, though it is not necessary to understand the method itself.

\section{Reinterpreting Formulae}

Now that we have a formal definition of a differential, it would be useful to use what we have to look at commonly-known formulae in this new context.
First is the chain rule for derivatives.
Given two functions $f$ and $g$, the derivative of their composition $f \circ g$ is given by
\begin{equation*}
    f'(g(x)) g'(x).
\end{equation*}
If we write $y = f(g(x))$ and $u = g(x)$, then the previous fact can be rewritten as
\begin{equation*}
    \frac{dy}{dx} = \frac{dy}{du} \frac{du}{dx}
\end{equation*}
using Leibniz's notation.
The latter way is writing is more elegant both in this case, and for compositions of more than two functions.
It reinforces the idea of the derivative being a fraction, even though it is not.
If we were to, however, consider the terms on the RHS as being differentials, then we do have fractions -- which can cancel.
\begin{align*}
    \frac{dy}{du} \frac{du}{dx} = \frac{y'(x) \diff x}{u'(x) \diff x} \frac{u'(x) \diff x}{dx} = y'(x)
\end{align*}
Of course this is not a proof of the chain rule.
It does not show that the fraction $dy/du$ is the same as the derivative of $y$ with respect to $u$, and it does not work when $u'(x) = 0$ wherever we are differentiating at.
Regardless, it shows the chain rule to be intuitively very simple -- just `cancelling' fractions.

Our definition of differentials can also be shown to be insightful with regard to integration by substitution and the method of solving differential equations by separation of variables.
That would make sense, as they are both linked to the chain rule (with the former being an inverse to some degree).
Given two functions $f$ and $g$, the integration by substitution formula states that
\begin{equation*}
    \int_{a}^{b} f(g(x))g'(x) \diff x = \int_{g(a)}^{g(b)} f(u) \diff u.
\end{equation*}
By making the substitution $u = g(x)$, we know that the differential of $u$ is given by $du = g'(x) \diff x$.
This suggests that the formula above is a literal substitution of one variable with another.
The limits being transferred from $x=a$ and $x=b$ to $u=g(a)$ and $u=g(b)$ make sense as well.\footnote{We could have had the limits on the RHS integral be $x=a$ and $x=b$ if we really wanted. This shows that nothing `actually' changes between the two integrals.}
Without the integral symbols, the equation would just be something that follows directly from the definition of a differential.
If it were possible to integrate a differential in the sense of it being the whole integrand -- not just integrating a function -- then the formula would be completely trivial.

Now, on to the topic of differential equations.
Let $f$, $g$, and $h$ be functions that satisfy the differential equation
\begin{equation*}
    \frac{dy}{dx} = g(x)h(y),
\end{equation*}
with $y=f(x)$.
If we treat the $dy$ in $dy/dx$ as a differential (which is a bit awkward notationally but valid), then we have a fraction that can be separated.
Multiplying both sides by $dx$ and dividing by $h(y))$ gives
\begin{equation*}
    \frac{dy}{h(y)} = g(x) \diff x.
\end{equation*}
This is really the same as
\begin{equation*}
    \frac{f'(x)}{h(f(x))} \diff x = g(x) \diff x.
\end{equation*}
Since $dx$ is some real number, we can cancel it out and then integrate both sides with respect to $x$ in order to solve the equation.
However, our integrals will have $dx$ at the end anyway.
What was the point of cancelling them out beforehand?
The $dx$ in an integral is still different from the $dx$ we have defined to be equal to some real number $\Delta x$.
Once again, if we could just integrate the differentials directly, things would be much simpler.

\section{Generalizing Differentials}

As was suggested at the end of the previous section, there is a different way to handle differentials that works well with integration.
Before that, we will have to perform quite a bit of work to generalize the notion of a differential; our current definition only works for functions of the type $\R \to \R$.
We will first cover higher-order differentials, which correspond to higher-order derivatives.
This is technically not a `generalization' in that it requires no new definitions.
The second-order differential, or second differential, of a function is given by finding the differential of the differential.
\begin{equation*}
    d^2 y \coloneq d(dy) = d(f'(x)\diff x) = (df'(x)) \diff x = f''(x) (dx)^2
\end{equation*}
Dividing through by $(dx)^2$ gives an equation very similar to Leibniz's notation for higher-order derivatives.
However, note that it is not a definition (as it would be if we were considering it as just notation).
\begin{equation*}
    \frac{d^2 y}{(dx)^2} = f''(x)
\end{equation*}
This is an interesting detail, again showing the efficacy and practicality of the notation.
Repeated applications of the differential gives us the general formula for the $n$-th order differential of $f$.
\begin{equation*}
    d^n y = d^n f(x, dx) = f^{(n)}(x) (dx)^n,
\end{equation*}
where $f^{(n)}$ is the $n$-th derivative of $f$.
This is pretty simple, but it only works under the assumption that $dx$ is itself not a function of $x$.
This might not be the case.
An example where $dx$ depends on $x$ is if $x$ depends on some parameter $t$.
The first-order differential is still given by $df(x, \Delta x) = f'(x) \Delta x = f'(x)\diff x$.
Since $x$ depends on $t$, $\Delta x$ depends on $t$ as well.\footnote{This is not a logical consequence, but rather a sensible choice we make. We choose to define $\Delta x$ as an actual change $x(t+\Delta t) - x(t)$, now that we are able to, instead of it just being some real number which represents the concept of change only when put in context.}
The principal part of the change $\Delta x$ is given by $dx = x'(t) \diff t$.
The reasoning for this is not new, as we are simply applying our definition of the differential to $x$ now that it is a function of $t$.
Then, $df$ is given by
\begin{equation*}
    df = f'(x)x'(t) \diff t.
\end{equation*}
This is essentially just the chain rule applied to differentials.
Finding $d^2 f$ will involve differentiating with respect to $t$ instead of $x$ to avoid concerning ourselves with whether $dx$ can be expressed as a function of $x$.
For this, we need the product rule.
\begin{equation*}
    d^2 f = f''(x) (x'(t)\diff t)^2 + f'(x) x''(t) (dt)^2
\end{equation*}
Simplifying this using $dx = x'(t)\diff t$ and $d^2 x = x''(t) (dt)^2$ gives us
\begin{equation*}
    d^2 f = f''(x) (dx)^2 + f'(x) d^2 x.
\end{equation*}
Note that differentiating with respect to $x$ directly would still have given us the same answer (even if it might not be technically valid).
This result generalizes to third derivatives, fourth derivatives, and so on.
The general formula for the $n$-th order differential is not at all insightful, so it will not be provided.
Higher-order differentials in general are not particularly relevant to the rest of our discussion, but they have been included for completeness.

Now we will consider multivariable functions.
Let $f\colon \R^n \to \R$, so it maps $n$ real numbers to a single real number.
It can equivalently be considered a function of $n$ independent variables, and we may write $y = f(x_1, \ldots, x_n)$.
There are multiple sensible ways of defining the differential.
We will first cover the simpler of the two approaches, before we cover the second and show that they are actually the same thing.
The total change in $y$ is given by
\begin{equation*}
    \Delta y = \Delta f \coloneq f(x_1 + \Delta x_1, \ldots, x_n + \Delta x_n) - f(x_1, \ldots, x_n),
\end{equation*}
where $\Delta x_i$ is a real number.
We are able to split this change into a linear (principal) part and a nonlinear part (which we call the error).
Each $x_i$ will contribute to the total change, both in its principal part and error part.
The former is given by
\begin{equation*}
    \frac{\partial y}{\partial x_i} \Delta x_i,
\end{equation*}
which we will cause the partial differential of $y$ with respect to $x_i$.
The error is simply $\varepsilon_i \Delta x_i$.
This is a natural extension of the one-dimensional case, with the primary difference being that we have now many more terms.
We can find the total principal change in $y$ due to the changes in all the variables by summing the partial differentials.
The result of this sum will be called the total differential of $y$.
\begin{equation*}
    dy \coloneq \sum_{i=1}^{n} \frac{\partial y}{\partial x_i} \Delta x_i
\end{equation*}
Since $dx_i(\Delta x_1, \ldots, \Delta x_n) = \Delta x_i$ (the partial derivative equals one for this term and zero for all others), we can rewrite the differential as
\begin{equation*}
    dy = \sum_{i=1}^{n} \frac{\partial y}{\partial x_i} \diff x_i.
\end{equation*}
As all $\Delta x_i \to 0$, we have $\varepsilon_i \to 0$, so $dy \to \Delta y$.
Just as in the case of one variable, this is approximately equal to the actual change in $y$: $\Delta y \approx \diff y$ (for small changes in $x_i$).
Their use cases are also similar.
However, their increased generality makes them much more applicable to situations -- including estimating error in measurements.

Since our function $f$ takes a list of numbers, and a list of numbers can be represented by a vector, we can consider $f$ to map $n$-dimensional vectors to real numbers.
This will not change anything about the above formula; we are now just considering the components of $\mathbf{x}$ and $d\mathbf{x}$.
We can go a step further by consider functions that output vectors as well.
Let $\mathbf{f}\colon \R^n \to \R^m$, with $y = \mathbf{f}(\mathbf{x})$.
The only thing that changes is that we are now differentiating vectors, which we do by differentiating their components separately.
This gives us a slightly modified formula for the total differential.
It is just a change of notation.
\begin{equation*}
    d\mathbf{f}(\mathbf{x}) = \sum_{i=1}^{n} \frac{\partial\mathbf{f}}{\partial x_i} \diff x_i
\end{equation*}
When we generalize to vector-valued functions, the changes in our function are no longer a real number.
It thus makes sense that the differential is not a real number any more, either.
Functions that map vectors to vectors will be as general as we go for now.
Although, as we stated before, we will offer a different perspective on what we just derived.
Once again, consider $\mathbf{f}\colon \R^n \to \R^m$, with $\mathbf{x}$ and $\Delta\mathbf{x}$ being in $\R^n$.
The total change in $\mathbf{f}$ is given by
\begin{equation*}
    \Delta \mathbf{f} = \mathbf{f}(\mathbf{x} + \Delta\mathbf{x}) - \mathbf{f}(\mathbf{x}).
\end{equation*}
This is a rewritten version of what we had before.
We want this total change to be equal to a linear change, as well as an error term.
Note that we had $\Delta f = df + \varepsilon\Delta x$ in the one-dimensional case, with $df = f'(x) \diff x$.
Our multivariable version of the derivative is the Jacobian matrix, $\mathbf{J_f}$.
The linear change is then $\mathbf{J_f}(\mathbf{x})\Delta\mathbf{x}$.
The error term is given by\footnote{It is not $\norm{\boldsymbol{\varepsilon}}\Delta\mathbf{x}$ instead because that would give an $n$-dimensional vector whilst the other terms are $m$-dimensional.} $\norm{\Delta\mathbf{x}}\boldsymbol{\varepsilon}$, with $\boldsymbol{\varepsilon} \to \mathbf{0}$ as $\Delta\mathbf{x} \to \mathbf{0}$.
We thus know that the total change in $\mathbf{f}$ can be expressed as follows.
\begin{equation*}
    \Delta\mathbf{f} = \mathbf{J_f}(\mathbf{x})\Delta\mathbf{x} + \norm{\Delta\mathbf{x}}\boldsymbol{\varepsilon}
\end{equation*}
The differential of $\mathbf{f}$ is given by
\begin{equation*}
    d\mathbf{f}(\mathbf{x}, \Delta\mathbf{x}) = \mathbf{J_f}(\mathbf{x})\Delta\mathbf{x}.
\end{equation*}
For $\mathbf{f}(\mathbf{x}) = \mathbf{x}$, it can be shown that for all $\mathbf{x} \in \R^n$, the Jacobian $\mathbf{J_f}(\mathbf{x}) = \mathbf{I}$, the identity matrix.
This means that $d\mathbf{x} = \mathbf{I}\Delta\mathbf{x} = \Delta\mathbf{x}$ for all $\mathbf{x}$.
We can therefore rewrite the differential of $\mathbf{f}$ as
\begin{equation*}
    d\mathbf{f}(\mathbf{x}) = \mathbf{J_f}(\mathbf{x}) \diff\mathbf{x}.
\end{equation*}
This differential is exactly the same as the total differential we previously derived.
To show this, we simply perform the matrix multiplication.
\begin{equation*}
    \mathbf{J_f}(\mathbf{x}) \diff\mathbf{x} = \begin{bmatrix}
        \dfrac{\partial f_1}{\partial x_1} & \cdots & \dfrac{\partial f_1}{\partial x_n} \\[0.5em]
        \vdots & \ddots & \vdots \\[0.5em]
        \dfrac{\partial f_m}{\partial x_1} & \cdots & \dfrac{\partial f_m}{\partial x_n}
    \end{bmatrix}
    \begin{bmatrix}
        dx_1 \vphantom{\dfrac{\partial f_1}{\partial x_n}}\\[0.5em]
        \vdots \\[0.5em]
        dx_n \vphantom{\dfrac{\partial f_m}{\partial x_n}}
    \end{bmatrix}
    = \begin{bmatrix}
        \dfrac{\partial f_1}{\partial x_1}\diff x_1 + \cdots + \dfrac{\partial f_1}{\partial x_n}\diff x_n\\[0.5em]
        \vdots \\[0.5em]
        \dfrac{\partial f_m}{\partial x_1}\diff x_1 + \cdots + \dfrac{\partial f_m}{\partial x_n}\diff x_n
    \end{bmatrix}
    = \sum_{i=1}^{n} \frac{\partial\mathbf{f}}{\partial x_i} \diff x_i
\end{equation*}
Whilst the two results are equivalent, the form involving the Jacobian is usually preferred.
It is much more compact and clear (as long as the definition and notation is understood).
The Jacobian better represents the nature of a derivative of as a linear map and not necessarily a number.
As a bonus, $d\mathbf{f}(\mathbf{x}) = \mathbf{J_f}(\mathbf{x})\diff\mathbf{x}$ maintains a similar form to the one-dimensional version of the differential.
The other form may be better-suited for quick computations, however.

We have now obtained an interpretation of the differential for a more general class of functions. 
It is a function of $\mathbf{x}$ and $d\mathbf{x}$, representing the principal change in the function.
This generalization, and the work done to formalize the derivative of functions of type $\R^m \to \R^n$, falls under the concept of the \href{https://en.wikipedia.org/wiki/Fr%C3%A9chet_derivative}{Fre\'chet derivative}.
This derivative can be defined on even more general spaces, but that is beyond the scope of our discussion.

A useful concept that is related to -- but distinct from -- our definition of the differential is the directional derivative.
\begin{equation*}
    \nabla_\mathbf{v}\mathbf{f}(\mathbf{x}) \coloneq \lim_{t \to 0} \frac{\mathbf{f}(\mathbf{x}+t\mathbf{v}) - \mathbf{f}(\mathbf{x})}{t}
\end{equation*}
Here, $\nabla_\mathbf{v}\mathbf{f}$ is the directional derivative of $\mathbf{f}$ along some vector $\mathbf{v}$.
This is a weaker concept of the derivative than what we were previously using.
If function $\mathbf{f}$ is differentiable at a point $\mathbf{x}$ (the Jacobian exists), then all directional derivatives at that point will exist.
The converse of this statement is not true.
The relationship between the directional derivative and the Jacobian is that $\mathbf{J_f}(\mathbf{x})\mathbf{v} = \nabla_\mathbf{v}\mathbf{f}(\mathbf{x})$, meaning that $d\mathbf{f}(\mathbf{x}, \mathbf{v}) = \nabla_\mathbf{v}\mathbf{f}(\mathbf{x})$.
The directional derivative can be thus be written as
\begin{equation*}
    \nabla_\mathbf{v} \mathbf{f}(\mathbf{x}) = \sum_{i=1}^{n} \frac{\partial\mathbf{f}}{\partial x_i} v_i,
\end{equation*}
where $\mathbf{v} = \langle v_1, v_2, \ldots, v_n \rangle$.
The map $\eval*{\nabla_\mathbf{v}}{\mathbf{x}}$ can similarly be written as a sum of maps.
\begin{equation*}
    \eval{\nabla_\mathbf{v}}{\mathbf{x}} = \sum_{i=1}^{n} v_i \eval{\frac{\partial}{\partial x_i}}{\mathbf{x}}
\end{equation*}
This will be useful later.

\section{Tangent Spaces}

Whilst our notation for the differential is standard when concerning functions defined on Euclidean space ($\R^n$), it will overlap with the notation for new concepts that we will introduce.
We will use $D\mathbf{f}$ instead of $d\mathbf{f}$ to denote the differential of function.
This also replaces $d\mathbf{x}$ with $D\mathbf{x}$, but we will instead keep to using $\Delta\mathbf{x}$; even this will be replaced later.
Unfortunately, there is no better way to handle the many different notations available whilst always attempting to use relatively standard ones for each topic.

In order to move on to the next step of developing a `differential' that works well with integration, we will have to shift our perspective and create new definitions.
Previously, we have been considering $D\mathbf{f}$ as a function of two numbers, $\mathbf{x}$ and $\Delta\mathbf{x}$.
Another way to view it is by fixing the variable $\mathbf{x}$ to consider the differential at a specific point.
We then have $D\mathbf{f_x}$ as a function of just $\Delta\mathbf{x}$.
Currently, $\Delta\mathbf{x}$ just represents some number which can be thought of as the change between $\mathbf{x}$ and some other point.
In practice, we want it to be small change.
The smaller it is, the close the principal change $D\mathbf{f}$ is to the total change $\Delta\mathbf{f}$.

Although the following language is not precise, we can say that these changes would be equal if $\Delta\mathbf{x}$ was infinitely small.
A point established near the start of this exposition was that the concept of infinitesimals is dated.
However, instead of abandoning them completely, we will show one way in which they can be formalized.
Whilst the concept of an infinitesimal itself is vague (as is), it is closely related to tangents.
Regardless of whether a derivative is interpreted as the ratio of infinitesimals, or as a limit, it can be thought of as the gradient of a tangent line.

Quantities that are the derivative of other quantities are always tangents.
For example, the instantaneous velocity vector $\mathbf{v} = \mathbf{\dot{r}}$ of a particle with position vector $\mathbf{r}$ is tangent to the trajectory of the particle.
The trajectory or path of the particle is the space of all points that $\mathbf{r}$ lands on.
With this information, we make the choice to reinterpret an infinitesimal change as a tangent vector.
Instead of being a change at all, $\Delta\mathbf{x}$ can be considered a vector tangent to $\R^n$ at the point $\mathbf{x}$.
The differential $D\mathbf{f_x}$ can now be considered a function of tangent vectors.

The set of all the tangent vectors of $\R^n$ at $\mathbf{x}$, the domain of $D\mathbf{f_x}$, forms a vector space.
This vector space is called the \emph{tangent space} of $\R^n$ at $\mathbf{x}$, denoted by $T_\mathbf{x}\R^n$.
What actually are the tangent vectors here?
To see that, it would be helpful to simplify to the plane $\R^2$ first.
Here, $T_\mathbf{x} \R^2$ is the collection of vectors starting at $\mathbf{x}$ tangent to $\R^2$; they all lie flat on the plane.
This seems almost identical to $\R^2$ itself, except for the origin of the vectors.
Indeed, there is a natural way to associate $\R^2$ and $T_\mathbf{x} \R^2$, so the spaces can be considered essentially the same.

The tangent spaces of $\R^2$ can be thought of as a copy of $\R^2$ attached to a particular point.
This generalizes to $\R^n$.
Now, we have found a way to describe the domain of the differential.
What about the codomain?
In all cases, $D\mathbf{f_x}$ represents a (very small) change in $\mathbf{f}(\mathbf{x})$.
Using the same reasoning as before, an infinitesimal change can be formalized be considering $D\mathbf{f_x}$ to be a tangent vector.
Thus, the codomain is the tangent space of $\R^m$ at the point $\mathbf{f}(\mathbf{x})$: $T_{\mathbf{f}(\mathbf{x})}\R^m$.
Putting this together, we have the differential $D\mathbf{f_x} \colon T_\mathbf{x}\R^n \to T_{\mathbf{f}(\mathbf{x})}\R^m$ defined by $D\mathbf{f_x}(\Delta\mathbf{x}) = \mathbf{J_f} \Delta \mathbf{x}$.

It would seem that the notion of a tangent space is not particularly useful, since $T_\mathbf{x} \R^n$ is practically the same as $\R^n$.
This is the case if we look at nothing but Euclidean spaces.
What other spaces can we look at?
For this, we will introduce the concept of a \emph{manifold}.
A manifold is a space where the idea of closeness between points exists, but not necessarily the ability to measure distances and angles.\footnote{The technical term for this is that a manifold is a topological space.}
Additionally, a manifold resembles Euclidean space near each point.
This is a very informal definition of a manifold, though it is all we need for this exposition.

Examples of a one-dimensional manifold include line segments, the real line $\R$, circles.
It can also be a collection of these, since a manifold can have multiple disconnected pieces.
Requiring a manifold to resemble a single Euclidean space $\R^n$ at every point, which is in the usual definition, means that these pieces must have the same dimension. 
Examples of two-dimensional manifolds include spheres, the plane $\R^2$, and a torus (doughnut).
In general, most shapes and spaces that one would imagine which do not cross themselves are examples of manifolds.

An example of a space that is not a manifold is an infinity symbol, due to the curve crossing with itself at the centre.
If we remove the central point, then the neighbourhood around it looks like four connected parts.
If we remove a point from the real line, then its neighbourhood looks like two connected parts.
Thus, the shape does not resemble Euclidean space near every point.
Other examples are two cones combines to make an hourglass shape, and more exotic shapes such as many fractals.
All it takes is for at least one point to not locally resemble Euclidean space.

Another important aspect of manifolds is the distinction between the manifold itself and the ambient space that a manifold may be embedded in.
For instance, a circle may be embedded in $\R^2$, or $\R^3$, or even inside a sphere.
When visualizing manifolds, we always use some embedding, e.g. a circle is drawn in a plane.
Properties of manifolds, such as its tangent spaces, these are intrinsic to the manifold itself; they do not depend on what the manifold is embedded in, if there is any embedding at all.
In general, we will talk about manifolds by themselves and their intrinsic properties, as opposed to extrinsic properties.

Whenever we mention manifolds, we will actually be referencing smooth manifolds in particular.
They are `smooth' in the way one would usually think of, so regular manifolds such as triangles, squares, and cones are excluded due to their sharp corners.
However, it more importantly means that we are able to do perform calculus with them.
This additional structure is required for everything we wish to do with manifolds, including tangent spaces.
Additional structure can be added to a manifold to allow for distances and angles to be measured, though our manifolds will only have this if specified.
We will also usually consider smooth (infinitely differentiable) functions on these manifolds.

We may now talk about tangent spaces that are actually interesting.
Let us consider the unit circle, which is denoted by $\mathbb{S}^1$.
The tangent space of $\mathbb{S}^1$ at some point $p$, $T_p \mathbb{S}^1$, is the line tangent to the circle at $p$.
This can be seen in \cref{fig:circle_tangent}.

\begin{figure}[!htbp]
\centering
\begin{tikzpicture}[scale=1.4]
    % Draw the unit circle
    \draw[black, thick] (0,0) circle (2cm);
    
    % Mark the point in the first quadrant (at 45 degrees)
    \pgfmathsetmacro{\angle}{45}
    \pgfmathsetmacro{\px}{2*cos(\angle)}
    \pgfmathsetmacro{\py}{2*sin(\angle)}
    
    % Draw the tangent line
    \draw[red, thick] ({\px - 1.5*sin(\angle)}, {\py + 1.5*cos(\angle)}) -- 
                       ({\px + 1.5*sin(\angle)}, {\py - 1.5*cos(\angle)});
    
    % Draw the point
    \filldraw (\px,\py) circle (2pt) node[above right] {$p$};
    
    % Draw axes
    \draw[-latex] (-2.5,0) -- (2.5,0) node[right] {$x$};
    \draw[-latex] (0,-2.5) -- (0,2.5) node[above] {$y$};
\end{tikzpicture}
\caption{Tangent space of $\mathbb{S}^1$ at point $p$}
\label{fig:circle_tangent}
\end{figure}

Similarly, the tangent space of a sphere at $p$, $T_p\mathbb{S}^2$ is the plane tangent to the sphere at $p$.
This is shown in \cref{fig:sphere_tangent}.
An interesting thing to note, and the reason why a circle is $\mathbb{S}^1$ and a sphere is $\mathbb{S}^2$, is that the dimension of these manifolds are one and two, respectively.
A circle resembles a line near a point, and a sphere resembles a plane near a point.
On the other hand, a disk is two-dimensional and ball is three-dimensional.
Note that their boundaries are the circle and sphere, which have one fewer dimension than them.
This relationship between the dimension of a manifold and its boundary is true in general.

When people first learn about shapes, they learn that a circle is two-dimensional and a sphere is three-dimensional.
This is in reference to the smallest Euclidean space the shape can be embedded in; one cannot embed a circle into a line.
Under this definition, the dimension of the disk and ball are still the same.
This is an extrinsic definition.
Both interpretations are correct \cite{shape_dimension_convention} depending on the convention we have for what the dimension of a shape is.
We use the intrinsic dimension.

\begin{figure}[!htbp]
\centering
\tdplotsetmaincoords{70}{110}

\begin{tikzpicture}[scale=1.2]
    %--- 2D shaded "sphere" (visual only) ---
    \def\R{2.5} % sphere radius
    \shade[ball color=white!90!blue, opacity=0.85] (0,0) circle (\R);

    %--- 3D drawing using tikz-3dplot ---
    \begin{scope}[tdplot_main_coords]

        % coordinates of point p on sphere: p = R * (1,1,1)/sqrt(3)
        \pgfmathsetmacro{\pval}{\R/sqrt(3)}
        \coordinate (O) at (0,0,0);
        \coordinate (P) at (\pval,\pval,\pval);

        % Build an orthonormal tangent basis at P:
        % u = (1,-1,0)/sqrt(2)
        \pgfmathsetmacro{\ux}{1/sqrt(2)}
        \pgfmathsetmacro{\uy}{-1/sqrt(2)}
        \pgfmathsetmacro{\uz}{0}

        % n = (1,1,1)/sqrt(3)  (unit normal)
        \pgfmathsetmacro{\nx}{1/sqrt(3)}
        \pgfmathsetmacro{\ny}{1/sqrt(3)}
        \pgfmathsetmacro{\nz}{1/sqrt(3)}

        % v = n x u  (already unit length because n and u are unit & orthogonal)
        \pgfmathsetmacro{\vx}{\ny*\uz - \nz*\uy} % = 1/sqrt(6)
        \pgfmathsetmacro{\vy}{\nz*\ux - \nx*\uz} % = 1/sqrt(6)
        \pgfmathsetmacro{\vz}{\nx*\uy - \ny*\ux} % = -2/sqrt(6)

        % plane half-size (controls how large the drawn patch is)
        \pgfmathsetmacro{\psize}{1.6}

        % compute plane corners: P + s*(±u ± v)
        \pgfmathsetmacro{\Ax}{\pval + \psize*(-\ux - \vx)}
        \pgfmathsetmacro{\Ay}{\pval + \psize*(-\uy - \vy)}
        \pgfmathsetmacro{\Az}{\pval + \psize*(-\uz - \vz)}
        \coordinate (A) at (\Ax,\Ay,\Az);

        \pgfmathsetmacro{\Bx}{\pval + \psize*( \ux - \vx)}
        \pgfmathsetmacro{\By}{\pval + \psize*( \uy - \vy)}
        \pgfmathsetmacro{\Bz}{\pval + \psize*( \uz - \vz)}
        \coordinate (B) at (\Bx,\By,\Bz);

        \pgfmathsetmacro{\Cx}{\pval + \psize*( \ux + \vx)}
        \pgfmathsetmacro{\Cy}{\pval + \psize*( \uy + \vy)}
        \pgfmathsetmacro{\Cz}{\pval + \psize*( \uz + \vz)}
        \coordinate (C) at (\Cx,\Cy,\Cz);

        \pgfmathsetmacro{\Dx}{\pval + \psize*(-\ux + \vx)}
        \pgfmathsetmacro{\Dy}{\pval + \psize*(-\uy + \vy)}
        \pgfmathsetmacro{\Dz}{\pval + \psize*(-\uz + \vz)}
        \coordinate (D) at (\Dx,\Dy,\Dz);

        % draw the tangent plane patch
        \filldraw[fill=red!20,opacity=0.6,draw=red!60] (A) -- (B) -- (C) -- (D) -- cycle;

        % optional: draw the plane edges a bit thicker and semi-transparent
        \draw[red!60,very thick,opacity=0.6] (A) -- (B) -- (C) -- (D) -- cycle;

        % draw the great circle (equator in z=0 plane)
        % Front half (solid) - top half, y > 0
        \draw[thick,domain=-75:100,samples=40,variable=\t]
            plot ({\R*cos(\t)},{\R*sin(\t)},0);
        % Back half (dashed) - bottom half, y < 0
        \draw[dashed,thick,domain=90:290,samples=40,variable=\t]
            plot ({\R*cos(\t)},{\R*sin(\t)},0);

        % 3D axes
        \draw[-latex] (0,0,0) -- (7,0,0) node[right] {$x$};
        \draw[-latex] (0,0,0) -- (0,3.5,0) node[above] {$y$};
        \draw[-latex] (0,0,0) -- (0,0,3.5) node[above] {$z$};

        % mark P
        \filldraw (P) circle (1.8pt) node[above right=2pt] {$p$};

    \end{scope}

    % outer sphere outline in 2D (to sit on top of the shading)
    \draw[thick] (0,0) circle (\R);

\end{tikzpicture}
\caption{Tangent space of $\mathbb{S}^2$ at point $p$}
\label{fig:sphere_tangent}
\end{figure}

With these examples, we can see that a tangent space is generally distinct from underlying manifold in a meaningful way.
Now, it only makes sense to generalize to functions $f \colon M \to N$, where $M$ and $N$ are manifolds.
The more common name for the differential $Df_p \colon T_p M \to T_{f(p)} N$ in this context is the \emph{pushforward}.
It pushes tangent vectors on $M$ forward to tangent vectors on $N$.
This is a point where we can stop generalizing.
Although we have focused on Euclidean spaces, the idea of the pushforward is applicable to manifolds in general without needing to stray from our initial intuition.

Before we go on further, we will make some more changes to our notation.
Normally, we denote the components of a vector $\mathbf{v}$ are denoted by $v_i$, with the index always on the bottom right.
We will now denote vector components with their index on the top right, e.g. $v^i$.
For basis vectors, the index is still on the bottom right, e.g. $\mathbf{\hat{e}}_i$.
The reason for this relates to how the components change under coordinate transformations (\emph{covariance} and \emph{contravariance}).
However, this is not important for our discussion, so it can simply be taken as a fact that this is something we do.

Another minor change to our notation will be that we will now only use boldface letters for vectors that are explicitly denoted to be Euclidean.
This means that tangent vectors will be written just like regular variables, also meaning that we will now use $f$ instead of $\mathbf{f}$ when concerned with functions of tangent vectors.
This is the case for manifolds in general, though they are not necessarily vector spaces in the first place.
All of this is to align ourselves with the standard conventions that exist concerning the topics we are discussing.

We now wish to provide a formal definition for a tangent space.
If our manifold is $\R^n$, this is easy to do.
A perfectly valid definition of $T_\mathbf{x} \R^n$ is the set $\mathbf{x} \times \R^n = \{(\mathbf{x}, \mathbf{v}) : \mathbf{v} \in \R^n\}$, so a tangent vector is a pair $(\mathbf{x}, \mathbf{v})$.
Attempting to generalize this to manifolds in general runs into the problem we just mentioned.
For example, the tangent space of the unit circle $\mathbb{S}^1$ at a point $p$ can be considered to be the set $\{(p, \mathbf{v}) : \mathbf{v} \cdot \mathbf{\hat{r}} = 0, \mathbf{v} \in \R^2\}$, where $\mathbf{\hat{r}}$ is the radial unit vector that points to $p$.
The ordered pair $(p, \mathbf{v})$ can indeed be thought of as an arrow starting at the point $p$ which is tangent to the circle, since $\mathbf{v}$ is orthogonal to the radius.

However, an arbitrary manifold does not have an inner product to work with.
This means that our approach will not work in general if the manifold is not embedded in Euclidean space.
This also relates to our previous visualizations of the tangent spaces for a circle and sphere.
Thinking about tangent vectors as actual arrows tangent to a manifold will not work, because there is not an underlying space for them to point out into.
We mentioned earlier that tangent spaces are intrinsic, so there should be a way of describing them without an embedding.

It follows that a formal, intrinsic definition of a tangent space will be more abstract than we may have initially thought.
This does not mean that we will not be able to have any intuition for it.
In fact, we already have the intuition.
Using the definition we have, which Lee calls a ``geometric tangent space'' in his \textit{Smooth Manifolds} \cite[pp. 52--53]{Lee2012}, a tangent vector $(\mathbf{x}, \mathbf{v}) \in T_\mathbf{x} \R^n$ can be associated with the directional derivative map $\eval*{\nabla_\mathbf{v}}{\mathbf{x}}$.
As we know, it maps functions to real numbers.
Additionally, it satisfies the following properties, with $f, g \in C^\infty(\R^n)$ being smooth functions on $\R^n$ and $\lambda, \mu \in \R$.
\begin{itemize}
    \item $\eval{\nabla_\mathbf{v}}{\mathbf{x}}(\lambda f + \mu g) = \lambda\eval{\nabla_\mathbf{v}}{\mathbf{x}} f + \mu\eval{\nabla_\mathbf{v}}{\mathbf{x}} g$
    \item $\eval{\nabla_\mathbf{v}}{\mathbf{x}} fg = f(\mathbf{x})\eval{\nabla_\mathbf{v}}{\mathbf{x}}g + g(\mathbf{x})\eval{\nabla_\mathbf{v}}{\mathbf{x}}f$
\end{itemize}
That is, the map is linear, and it satisfies a condition very similar to the product rule for derivatives, which we call the \emph{Leibniz identity}.
Any map that satisfies these two properties is called a \emph{derivation}, and it can be seen as generalizing the concept of a derivative by possessing two of its important properties.
It turns out to be the case that there is a one-to-one association between the space of all derivations at $\mathbf{x}$, linear maps $v \colon C^\infty (\R^n) \to \R$ that satisfy the Leibniz identity, and our ``geometric tangent space''.
This association is via the map from $(\mathbf{v}, \mathbf{x})$ to $\eval*{\nabla_\mathbf{v}}{\mathbf{x}}$.

We have shown that the space of derivations and our old tangent space are essentially the same in a way.
We are also able to easily define derivations on arbitrary manifolds $M$.
Thus, it is reasonable to define the tangent space of $M$ at a point $p \in M$, $T_p M$, as the space of all derivations $v \colon C^\infty (M) \to \R$.
This includes $T_p \R^n$, replacing the ``geometric tangent space'' definition.
Now, it is still perfectly valid to think about tangent vectors as literal tangents to a manifold intuitively.
Abstracting the formal definition has not lost this intuition, which can be very helpful when thinking about these topics.

This allows us to give a proper definition of the pushforward $D\varphi_p \colon T_p M \to T_{\varphi(p)} N$ of a smooth function $\varphi \colon M \to N$, which were unable to do before.
We simply need a definition that maps derivations to derivations and simplifies to the old definition when we are dealing with Euclidean space.
Let $v \in T_p M$ be a tangent vector.
We define $D\varphi_p(v)$ to act on a smooth function $f \in C^\infty (N)$ as follows.
\begin{equation*}
    D\varphi_p(v)(f) \coloneq v(f \circ \varphi)
\end{equation*}
This is linear by the linearity of $v$.
Furthermore, it satisfies the Leibniz identity.
For any two smooth functions $f, g \in C^\infty (N)$, we have the following.
\begin{align*}
    D\varphi_p(v)(fg) &= v((fg) \circ \varphi) \\
    &= v((f \circ \varphi)(g \circ \varphi)) \\
    &= f \circ \varphi(p)v(g \circ \varphi) + g \circ \varphi(p)v(f \circ \varphi) \\
    &= f(\varphi(p)) D\varphi_p(v)(g) + g(\varphi(p)) D\varphi_p(v)(f)
\end{align*}
Hence, $D\varphi_p(v) \in T_{\varphi(p)} N$ is a derivation, so our definition is valid.
In the case where $M = \R^n$ and $N = \R^m$ (the letters are unfortunately mismatched), consider $v = \eval*{\nabla_\mathbf{v}}{p}$ with the associated Euclidean vector $\mathbf{v} \in \R^n$
We will make use of the expression of the directional derivative a sum using the standard basis for $\R^n$.
Let $x^1, x^2, \ldots, x^n$ represent the coordinates of $\R^n$, and let $y^1, y^2, \ldots, y^m$ represent the coordinates of $\R^m$.
We can apply the multivariable chain rule to $f \circ \varphi$.
This gives us
\begin{align*}
    D\varphi_p(v)(f) &= \eval{\nabla_\mathbf{v}}{p} (f \circ \varphi) \\
    &= \sum_{i=1}^{n}v^i \eval{\frac{\partial(f \circ \varphi)}{\partial x^i}}{p} \\
    &= \sum_{i=1}^{n} v^i \left(\sum_{j=1}^{m} \eval{\frac{\partial\varphi^j}{\partial x^i}}{p} \eval{\frac{\partial f}{\partial y^j}}{\varphi(p)}\right) \\
    &= \sum_{j=1}^{m} \left(\sum_{i=1}^{n} v^i \eval{\frac{\partial\varphi^j}{\partial x^i}}{p} \right) \eval{\frac{\partial f}{\partial y^j}}{\varphi(p)} \\
    &= \sum_{j=1}^{m} \eval{\nabla_\mathbf{v}}{p} (\varphi^j)\eval{\frac{\partial f}{\partial y^j}}{\varphi(p)}.
\end{align*}
Now, by letting $D\varphi_p (v) = w \in T_{\varphi(p)} \R^m$, we can identify the derivation $w$ with the associated Euclidean vector $\mathbf{w} \in \R^m$ via the directional derivative along $\mathbf{w}$.
This acts on $f$ to give
\begin{equation*}
    w(f) = \eval{\nabla_{\mathbf{w}}}{\varphi(p)} (f) = \sum_{j=1}^{m} w^j \eval{\frac{\partial f}{\partial y^j}}{\varphi(p)}.
\end{equation*}
Comparing this with the result we got above, we can see that
\begin{equation*}
    w^j = \eval{\nabla_\mathbf{v}}{p} (\varphi^j) = \nabla_\mathbf{v}\varphi(p),
\end{equation*}
Therefore,
\begin{equation*}
    \mathbf{w} = \nabla_\mathbf{v} \varphi(p) = \mathbf{J}_\varphi(p) \mathbf{v}.
\end{equation*}
Whilst it is technically incorrect to write $D\varphi_p(v) = \mathbf{J}_\varphi(p) \mathbf{v}$, they are essentially the same thing by the one-to-one association we have between tangent vectors and Euclidean vectors.
The pushforward for manifolds in general maintains the same interpretation as the old differential, representing the principal change in $\varphi$ or equivalently the best linear approximation to $\varphi$ at $p$.
Notice that our definition of the pushforward is completely independent of coordinates.
The fact that we can do this makes sense, as it captures geometric information about the manifold.
In general, we value coordinate-independent results for being more insightful as well as for the fact that we can apply them to any suitable coordinate system.
However, we need some coordinate system in order to preform calculations.
A basis for the tangent space $T_p \R^n$ can easily be created.
We know that each tangent vector can be represented by a directional derivative, and a directional derivative is a linear combination of partial derivatives.
For $v \in T_p \R^n$, we have
\begin{equation*}
    v = \eval{\nabla_\mathbf{v}}{p} = \sum_{i=1}^{n} v^i \eval{\frac{\partial}{\partial x_i}}{p} = \sum_{i=1}^{n} v^i \eval{\partial_{i}}{p},
\end{equation*}
for the associated $\mathbf{v} \in \R^n$; here, $\eval*{\partial/\partial x_i}{p}$ and $\eval*{\partial_{i}}{p}$ are just different notations.
It follows that $\{\eval*{\partial_1}{p}, \eval*{\partial_2}{p}, \ldots, \eval*{\partial_n}{p}\}$ is the standard basis for $T_p\R^n$.
The partial derivative $\eval*{\partial/\partial x_i}{p}$ is just a directional derivative along the Euclidean basis vector $\ehat_i$.
We now need a basis for the tangent space of an arbitrary smooth manifold $M$.
For this, we need coordinates for $M$.
The problem is that there is not necessarily one set of coordinates that can be used for the entirety of $M$.
There is generally not a sense of independent directions on a manifold, so we instead consider a coordinate system local to a point $p$, where $M$ resembles $\R^n$.

Given that $M$ has $n$ dimensions, our local coordinates $x^1, x^2, \ldots, x^n$ near $p$ are smooth functions from an (open) set $U \subseteq M$ to $\R$.
Although we have defined tangent vectors in relation to functions that are smooth on the entirety of $M$, we can identify a tangent vector in $T_p M$ with one in $T_p U$ without making any arbitrary choices.
The set $U$ can be arbitrarily small, showing that a tangent vector is a local object.
If two smooth functions $f$ and $g$ are equal in a neighbourhood around $p$, then $v(f) = v(g)$.
It reasonably follows that a tangent vector $v \in T_p M$ is entirely determined by its action on the local coordinates.

Now, consider $n$ derivations $b_1, b_2, \ldots, b_n$ in $T_p M$.
We want these to be our basis tangent vectors, so we would want them to pick out the coordinates from a point.
\begin{equation*}
    b_i (x^j) = \begin{cases}
        1 & \text{if } i = j \\
        0 & \text{if } i \neq j
    \end{cases}
\end{equation*}
These act exactly like the partial derivative operators, which pick out a certain component of the change of a function.
We otherwise do not have a notion of partial derivatives for manifolds, so we will denote these basis tangent vectors $b_i$ by $\eval*{\partial_i}{p}$.
Any tangent vector $v \in T_p M$ is can be expressed as a linear combination of these basis tangent vectors, and its $i$-th component is $v^i \coloneq v(x^i)$.
Thus, $\{\eval*{\partial_1}{p}, \eval*{\partial_2}{p}, \ldots, \eval*{\partial_n}{p}\}$ is a basis for $T_p M$.
\begin{equation*}
    v = \sum_{i=1}^{n} v^i \eval{\partial_i}{p}
\end{equation*}
This looks identical to the definition we had for $T_p \R^n$.
If $M = \R^n$, then the basis tangent vectors are actually partial derivatives, so it becomes the same definition -- which is desirable.
Although they are not literally partial derivatives in general, thinking about them as such for the purpose of intuition is fine.
An important detail is the fact that our choice of local coordinates was not special in any way.
Since our basis depends on those coordinates, the same applies to it.
Different choices of coordinates will give different bases, and there is generally no `natural' or `preferred' basis.

Note that our basis for $T_p M$ shows that a tangent space has the same dimension as the manifold itself.
This has not been explicitly stated yet, but it may have been apparent from the examples of a circle and sphere.
A manifold resembles Euclidean space near a point, and its tangent space can intuitively be viewed as that Euclidean space (though $T_p M$ is technically distinct from any $\R^n$).
The tangent space can be seen as the best linear approximation of the manifold at $p$, so it follows that they have the same dimension.

Let us apply what we have developed to the simple example of the unit circle, $\mathbb{S}^1$, though be aware of the fact that it will not be a formal proof.
Since $\mathbb{S}^1$ is one-dimensional, $T_p \mathbb{S}^1$ is one-dimensional, so it will be spanned by a single basis tangent vector.
Given the local coordinate $\theta$, our tangent space is equal to $\operatorname{span}\{\eval*{\partial_\theta}{p}\}$.
This does not seem particularly insightful, because it is not.
A particular basis for a tangent space does not give us any new geometric information, but it allows us to explicitly express any tangent vector.
Without having more theory developed, there is not much more we can do about it.

If we consider $\mathbb{S}^1$ to be embedded in $\R^2$, we can express the basis tangent vector in terms of Cartesian coordinates.
Consider the (inclusion) map $\imath \colon \mathbb{S}^1 \to \R^2$ as the object that preforms this embedding, so $\imath(p) = (x,y)$.
It acts via the parameterization $\gamma \colon [0, 2\pi) \to \R^2$ given by $\imath(p) = \gamma(\theta) = (\cos \theta, \sin \theta)$; this is just one of many valid choices.
The tangent space $T_p \mathbb{S}^1$ as a subspace of $T_{\imath(p)} \R^2$.
We can consider the tangent vector $\eval*{\partial_\theta}{p}$ acting on a smooth function $f$ on $\R^2$, which can then be expressed in terms of $\eval*{\partial_x}{p}$ and $\eval*{\partial_y}{p}$ via the chain rule.
\begin{align*}
    \eval{\frac{\partial f}{\partial \theta}}{p} &= \frac{\partial x}{\partial \theta} \eval{\frac{\partial f}{\partial x}}{p} + \frac{\partial y}{\partial \theta} \eval{\frac{\partial f}{\partial y}}{p} \\
    &= -\sin\theta \eval{\frac{\partial f}{\partial x}}{p} + \cos\theta \eval{\frac{\partial f}{\partial y}}{p} \\
    &= -y \eval{\frac{\partial f}{\partial x}}{p} + x \eval{\frac{\partial f}{\partial y}}{p}.
\end{align*}
Therefore,
\begin{equation*}
    \eval{\frac{\partial}{\partial \theta}}{p} = -y \eval{\frac{\partial}{\partial x}}{p} + x \eval{\frac{\partial}{\partial y}}{p}.
\end{equation*}
This can be associated with the Euclidean vector $(-y, x)$, which is indeed tangent to the unit circle since it is orthogonal to the position vector $(x,y)$: $(-y, x) \cdot (x, y) = 0$.
It can also be shown that the pushforward of the map $\imath$, $D\imath \colon T_p \mathbb{S}^1 \to T_{\imath(p)}$ acts on $\eval*{\partial_\theta}{p}$ to give the expression we just derived. 
Really, this is a formal way of stating that a tangent vector in the subspace can be treated as a tangent vector in the ambient space, which is to be expected.

We can also define tangent spaces using curves and velocities.
Informally, it involves a tangent vector being associated with the velocity of a particle whose trajectory is given by a smooth function on the manifold.
Considering our intuition about tangent vectors, this is reasonable.
We have chosen to use the definition via derivations due to it being more general and being able to be defined without any mention of coordinates.
It also follows from our earlier discussion on the differential of a function and the directional derivative, thus appearing to be a natural extension of those topics.

\section{Cotangent Vectors}
%! WHENEVER I AM NEXT ABLE TO, SWAP AROUND R^M AND R^N IN ALL PARTS AFTER THIS (EXCEPT APPENDIX ON VECTOR CALCULUS)
%! CHANGE THe SECTION ON INTEGRATION OF COVECTOR FIELDS TO HAVE PROPER DEFINITION OF PUSHFORWARD
%! CONTINUE WITH INTEGRATION OF K-FORMS

We have now formalized the idea of an infinitesimal quantity using tangent vectors.
Specifically, a tangent vector $v$ represents an infinitesimal change, and it gives a rate of change (directional derivative) when it acts on a function.
It is reasonable to think that one can now make sense of the $dx$ in an integral, but some immediate problems arise.
Consider
\begin{equation*}
    \int_C f(x) \diff x,
\end{equation*}
where $C$ is a curve in $\R^2$, and the function $f$ is of the type $C \to \R$.
The integral is always, no matter the definition, some sort of sum.
Here, we would be summing a scalar multiple of a tangent vector at every point on $C$.
One thing we have to deal with is the fact that there are an infinite number of tangent vectors, which make up the tangent space, at every point.
This is not actually a problem, as choosing a specific parameterization of $C$ will pick a tangent vector at each point. 
The collection of these choices of tangent vectors is called a \emph{(tangent) vector field}.

The actual problem relates to summing these vectors.
In general, there is not a meaningful way to sum tangent vectors that come from different tangent spaces.
A way to see this is by choosing a specific curve and drawing the tangent space at two points.

\begin{figure}[!htbp]
\centering
\begin{tikzpicture}
\begin{axis}[
    width=12cm,
    height=8cm,
    axis lines=middle,
    xlabel={$x$},
    ylabel={$y$},
    xmin=-2, xmax=5.5,
    ymin=-2, ymax=2,
    xtick=\empty,
    ytick=\empty,
    samples=100,
    domain=-2:5,
    enlargelimits=false,
]

% Draw the curve: (1/2)f(x) = 0.125x^3 - 0.625x^2 - 0.125x + 1.25
\addplot[thick, blue, domain=-2:5] {0.125*x^3 - 0.625*x^2 - 0.125*x + 1.25};

% Calculate tangent slopes: f'(x) = 0.375x^2 - 1.25x - 0.125
% At x=-1: f(-1) = -0.125 - 0.625 + 0.125 + 1.25 = 0.625, slope = 0.375 + 1.25 - 0.125 = 1.5
% At x=3: f(3) = 0.125*27 - 0.625*9 - 0.125*3 + 1.25 = 3.375 - 5.625 - 0.375 + 1.25 = -1.375, slope = 0.375*9 - 1.25*3 - 0.125 = 3.375 - 3.75 - 0.125 = -0.5

% Draw tangent line at P1 (x=-1)
\addplot[red, thick, domain=-1.5:-0.5] {0.625 + 1.5*(x-(-1))};

% Draw tangent line at P2 (x=3)
\addplot[red, thick, domain=2.2:3.8] {-1.375 - 0.5*(x-3)};

% Mark points on top
\addplot[only marks, mark=*, mark size=2pt, black] coordinates {(-1, 0.625)};
\addplot[only marks, mark=*, mark size=2pt, black] coordinates {(3, -1.375)};

% Labels
\node[above left] at (axis cs:-1, 0.625) {$p_1$};
\node[above right] at (axis cs:3, -1.375) {$p_2$};

\end{axis}
\end{tikzpicture}
\caption{A curve $C$ with tangents at points $p_1$ and $p_2$}
\label{fig:curve_tangents}
\end{figure}

How would we add a tangent vector of $p_1$ to a tangent vector of $p_2$?
We can assign them to vectors in $\R^2$ using some basis and add them component-wise, but that is only possible because we have \emph{embedded} the curve $C$ in $\R^2$.
Tangent vectors are \emph{intrinsic} to the manifold itself, so an operation with them should not depend on the manifold being embedded within another manifold.
Even if we were to accept it as valid, our resulting vector would not necessarily be tangent to either $p_1$ or $p_2$; it would not be a tangent vector.
Thus, our integral is not well-defined.

A sensible response would be to slightly alter the definition to try and make it work.
If we introduce another function, say $g \colon C \to \R$, we can have a tangent vector acting on it to return a real number at each point.
We then run into another problem.
Since the chosen tangent vectors depend on our parameterization, it follows that the value of our integral would also depend on the parameterization.
This does not make sense if the integral is supposed to carry some sort of geometric information about the manifold and the object we are integrating over the manifold.
If two sets of equations describe the same manifold, then we should be able to integrate using either and receive the same result.

There are even more issues with this definition.
We can thus conclude that it does not properly represent what we mean by an integral.
However, this raises a question.
Our naive formulation of an integral with tangent vectors was based on the seemingly sound reasoning of what the $dx$ meant.
Is it an infinitesimal?
If not, why did it seem like one?
We will see that it was not completely misguided.
Instead, we simply require one more concept.
In order to transfer the information contained within a tangent vector into something we can sum, we need an object that takes tangent vectors and returns real numbers, effectively measuring them.
Finding such an object is now our goal.

We will call these objects \emph{cotangent vectors}.
At some point $p$ in a manifold $M$, they should map $T_p M$ to $\R$.
There will obviously be many different ways to measure a certain tangent vector, so there will be many different cotangent vectors at any given point.
We will call the space containing all of them the \emph{cotangent space}, labelling it $T^*_p M$.
Each point has its own cotangent space, and creating a collection of cotangent vectors by picking one for each point gives us a \emph{cotangent field}.
Alternative names for all of these concepts involve replacing `cotangent' with `covector'.
We will use these two sets of terms interchangeably for convenience.

Note that all of these new concepts that we have introduced are not so new.
Rather, they mirror ideas we have already had relating to tangent vectors.
This is no coincidence, and the two sets of concepts can be described as being dual to each other.
Familiarity with dual spaces in linear algebra lets us say that $T^*_p M$ is the dual space of $T_p M$.
In order to show what this means, we will need to concretely formulate the idea of covectors.
This is done by thinking about the properties our object needs to have.

Covectors are maps from $T_p M$ to $\R$.
This means that they are functionals, objects that take a function as input and return a number.
Here, the functions are tangent vectors, which is evident from the fact they are directional derivatives (the map itself).
Moreover, they are linear.
This condition is imposed on them so that they measure tangent vectors in a sensible way.
Taking measurements of sums should become a sum of measurements, and measuring a scaled vector should be a scaled version of the original vector.

To complete the picture, we will show how covectors form a tangent space.
The typical notation for covector fields is the Greek letter $\omega$, with the covector at some point $p$ in $M$ being denoted by $\omega_p$.
In cases where we deal with multiple covector fields, we may use other Greek letters such as $\alpha$ or $\beta$.
The following definitions of addition and multiplication should feel natural; we are saying that these objects combine and scale linearly.
Note that $a$ is some scalar.
\begin{align*}
    (\alpha + \beta)(v) &= \alpha(v) + \beta(v) \\
    (a\omega)(v) &= a(\omega(v))
\end{align*}
With this, $T^*_p M$ is a vector space.
Due to the nature of covectors being linear functionals of tangent vectors, we have a duality between tangent and cotangent spaces.
In a sense, they are mirrored versions of each other.
As with any vector space, we are able to give $T^*_p M$ a basis.
Before we do this, we will gain some visual intuition for covectors first.
Consider the following plot of the function $f(x,y) = x\sin y + y\sin x$, where the colours represent the value of $f(x,y)$ at a particular point.

\begin{figure}[!htbp]
\centering
\begin{tikzpicture}
\begin{axis}[
    width=10cm,
    height=10cm,
    view={0}{90},
    colormap/jet,
    xlabel={$x$},
    ylabel={$y$},
    colorbar,
    colorbar style={
        ylabel={$f(x,y)$}
    },
    xmin=-10, xmax=10,
    ymin=-10, ymax=10,
    domain=-10:10,
    y domain=-10:10,
]

% Create heatmap with interpolated shading
\addplot3[
    surf,
    shader=interp,
    samples=50,
] 
{x*sin(deg(y)) + y*sin(deg(x))};

% Overlay contour lines showing constant function values
\addplot3[
    contour gnuplot={
        levels={-16,-14,-12,-10,-8,-6,-4,-2,0,2,4,6,8,10,12,14,16},
        labels=false,
    },
    samples=90,
    samples y=90,
    thin,
    contour/draw color=black,
] {x*sin(deg(y)) + y*sin(deg(x))};

\end{axis}
\end{tikzpicture}
\caption{Plot of $f(x,y) = x\sin y + y\sin x$}
\label{fig:heatmap}
\end{figure}

The important information here are the contour lines, which are level sets $\{(x,y) \mid f(x,y) = c\}$ for some constant $c$.
In our diagram, they have been drawn for $c \in \{-16, -14, \ldots, 14, 16\}$.
They are close together where the function is steeper and further apart where the function is less steep.
It is important to note that this is just an arbitrary selection of level sets.
There are, in fact, an infinite number of level sets, since there are an infinite number of constants that we can choose from.
That is, no matter how far we zoom into the plot, we will always find a new line between two other ones.

If the plot represented a topographic map, where the elevation is given by $f(x,y)$, we could figure out the change in elevation from one point to another by counting the number of contour lines we cross.
When we `mention' count in this context, we are including crossing over a line by a noninteger amount, e.g. we could be halfway between two lines.
It is a continuous measurement.
Our movement can be represented by a vector, and the count measures this vector.
This is how we will visualize covectors.

The above plot can be interpreted as a covector field \cite{covector_visualization}.
If we were to zoom into a particular point, the curved contour lines would resemble stacks of equally spaced lines tangent to the contour line at that point.
If the covector is an element of $T^*_p M$, then a tangent vector in $T_p M$ can be placed on this plane of stack lines.
The covector evaluated at the tangent vector is the number of stack lines it crosses.
Again, this works in a continuous way, and the number is not necessarily an integer.
Thus, it might be more accurate to call a covector a uniform 'density' of stack lines.

We can see from \cref{fig:parallel_stack_lines} that the covector acts in a linear way, which matches our definition.
The linearity is from the fact that we are, in a way, measuring the length of the vector.
Also, note that the stack has an orientation (shown by the black line).
This is because a function increases from one direction and decreases from the opposite direction.
Covectors in general have an orientation, and this is an important fact about them.

\begin{figure}[!htbp]
\centering
\begin{tikzpicture}[scale=1.0]
    % Diagonal stack lines (visualizing a covector)
    \foreach \k in {-3.5,-3.0,...,3.5} {
        \draw[thick,gray!65] (-3,\k-0.5) -- (3,\k+0.5);
    }
    % Orientation arrow
    \draw[->,thick,black] (3.65862,-0.40344) -- (3.5,1.5);
    % Reference point
    \filldraw[black] (-1,-1) circle (1.5pt) node[below left] {$p$};
    % Invisible path to extend bounding box
    \path (3.65862,-0.40344) ++(0.3,0);
    % Vector v
    \draw[->,thick,blue] (-1,-1) -- (1.3,0.7) node[midway,below right] {$v$};
    % Vector w
    \draw[->,thick,teal] (-1,-1) -- (-0.2,1.5) node[midway,left] {$w$};
    % Vector v + w
    \draw[->,thick,purple] (-1,-1) -- (2.1,3.2) node[pos=0.6,right] {$v+w$};
    % Endpoint markers
    \filldraw[blue] (1.3,0.7) circle (1.5pt);
    \filldraw[teal] (-0.2,1.5) circle (1.5pt);
    \filldraw[purple] (2.1,3.2) circle (1.5pt);
\end{tikzpicture}
\caption{Covector acting on tangent vectors}
\label{fig:parallel_stack_lines}
\end{figure}

We have shown that a set of stack lines is a visualization of covectors in that it describes their behaviour of acting on tangent vectors and representing linear functionals.
Now, we will link covectors to a concept that we have already studied.
Let $\omega$ be the covector field represented by the plot of $f(x,y)$, and let $\omega_p$ be the covector at some point $p$.
This is, by our definition, a linear map from $T_p M$ to $\R$.
We already know of such a map: the pushforward $Df_p$ of a function $f\colon M \to \R$.

Technically, this pushforward maps to $T_p \R$, not $\R$.
However, these two spaces can be associated with each other in a one-to-one way; the tangent space of $\R$ at $p$ can be seen as a copy of $\R$ anchored to that point.
Due to this association, we can consider the pushforward to `effectively' map to $\R$.
Since we already know that the pushforward is linear (from the linearity of derivatives), it follows that it is a covector.
Considering that pushforwards seemed to have a link to integration, looking exactly like integrands, this seems reasonable.

Furthermore, we know that the pushforward measures something, and we know what exactly it measures.
The number $Df_p(v)$ is the directional derivative of $f$ along $v$ at $p$.
In this context, where $\omega = Df$, the number $\omega_p(v)$ represents the same thing.
It can be interpreted as the rate at which a particle at $p$ would cross the stack lines if it had a velocity $v$.
The fact that $\omega_p(v)$ and $v(f)$ give the same number and represent the same concept again shows their dual nature.
We will later see that not all covector fields are the pushforward of some function.
This is only relevant to fields, so our intuition for individual covectors is unaffected; we do not need to worry about it now.

Pushforwards can also be used to give a reason as to why the stack lines are tangent to the contour lines.
Moving along a level set will not change the value of the function, by definition.
Thus, the pushforward will output zero in this direction.
If a tangent vector is tangent to a stack line, it will not cross any of them; the covector will thus act on it to give zero.
These two ideas combine via $\omega = Df$ to give us the information that the stack lines and level sets are in the same direction as each other at a given point.
They are tangent.
Note that the actual direction of the covector is perpendicular to the stack lines.

A relevant note to make is that pushforwards in general are not covectors.
Given a function $f \colon M \to N$, the pushforward is a map $Df_p \colon T_p M \to T_p N$.
This outputs tangent vectors, making it fundamentally different from a covector.
We were only able to associate them when dealing with the pushforward of functions $f \colon M \to \R$, due to $T_p \R$ essentially being the same thing as $\R$.
The association between pushforwards and covectors is more accurately an association between the pushforward and an operation that we will introduce later.
For now, it is not needed.

\cref{fig:1-form_hyperplanes} shows a similar visualization of a covector.
It is the three-dimensional analogue, replacing the stack lines with a stack of planes.
As we go to higher dimensions, covectors will maintain this geometric interpretation, even though it becomes impossible to visualize (without some tricks).
In general, a covector can be seen as an oriented stack of hyperplanes.
A covector field can be seen as some sort of topographic map, where the contours resemble these stacks of hyperplanes near a point.
Although they are more accurately called contour hyperplanes, we will always refer to them as contour lines.

\begin{figure}[!htbp]
    \centering
    \scalebox{1.5}{\includegraphics{1-form_hyperplanes}}
    \captionsetup{justification=centering}
    \caption{Visualization of covectors as hyperplanes
    \\Credits: Maschen -- Own work, \href{https://creativecommons.org/public-domain/cc0/}{CC0}, via Wikimedia Commons}
    \label{fig:1-form_hyperplanes}
\end{figure}

Now, consider the covector field associated with the function $f(x, y) = x$.
This is shown in \cref{fig:heatmap_fx}.
Its contour lines are straight, parallel, and have equal spacing.
They point to the right.
The covector at any point will look identical to the contour lines, except that it will always have unit spacing between stack lines.
A vector that points completely vertically will not cross any stack lines, so its measurement will read zero.
In general, only the $x$-component of a tangent vector will contribute to the measurement.
Due to the unit spacing, the measurement will be exactly the $x$-component of the vector.

\begin{figure}[!htbp]
\centering
\tikzsetnextfilename{heatmap_fx}
\begin{tikzpicture}
\begin{axis}[
    width=10cm,
    height=10cm,
    view={0}{90},
    colormap/jet,
    xlabel={$x$},
    ylabel={$y$},
    colorbar,
    colorbar style={
        ylabel={$f(x,y)$}
    },
    xmin=-10, xmax=10,
    ymin=-10, ymax=10,
    domain=-10:10,
    y domain=-10:10,
]

% Create heatmap with interpolated shading
\addplot3[
    surf,
    shader=interp,
    samples=50,
] 
{x};

% Overlay contour lines showing constant function values
\addplot3[
    contour gnuplot={
        levels={-8,-6,-4,-2,0,2,4,6,8},
        labels=false,
    },
    samples=90,
    samples y=90,
    thin,
    contour/draw color=black,
] {x};

\end{axis}
\end{tikzpicture}
\caption{Plot of $f(x,y) = x$}
\label{fig:heatmap_fx}
\end{figure}

Note that (tangent) vectors are objects that do not themselves depend on a chosen coordinate system.
The $x$-component chosen by the aforementioned covector is the first component the vector would have if it were expressed with Cartesian coordinates.
It is the number of parallel, unit-spaced stack lines that would be crossed by the vector.
Covectors are concepts completely independent of coordinates; this is a very important fact.

The covectors associated with $f(x,y) = y$ similarly pick out the $y$-component of a tangent vector.
We have thus found two covector fields whose covectors do completely different things.
Using both of them, we can pick out both parts of a tangent vector.
This strongly suggests that they will form our basis for the cotangent space.
Indeed, the natural choice of basis immediately follows from simply visualizing covectors and considering the simplest cases.

The basis for $T_p \R^2$ will be $\{\d{x}, \d{y}\}$.
These pick out the components of tangent vectors, so $\d{x}(\partial_x) = 1$, $\d{x}(\partial_y) = 0$, $\d{y}(\partial_x) = 0$, and $\d{y}(\partial_y) = 1$.
Generalizing this, the basis for $T^*_p \R^n$ will be $\{\d{x}^1, \ldots, \d{x}^n\}$, with
\begin{equation*}
    \eval{\d{x}^i}{p} \left(\eval{\partial_{x_j}}{p}\right) = \begin{cases}
        1 & \text{if } i = j,\\
        0 & \text{if } i \neq j.
    \end{cases}
\end{equation*}
Here, we have specified that both the covector and tangent vector act at the point $p$ with an evaluation bar.
This is technically the proper way to do it, but it is often omitted, as we did in the paragraph above.
This is done when context makes it clear what (possibly arbitrary) point we are talking about.
Since we very often talk about both covector fields $\omega$ and covectors $\omega_p$, the distinction between them is often made via notation.
For tangent spaces, we use an uppercase letter such as $V$ to distinguish them from tangent vectors $v$; this is needed less frequently.

A general covector $\omega_p$ can be expressed as a linear combination of the basis covectors.
It can be seen as a set of stack lines obtained by a combination of horizontal and vertical stack lines.
\begin{equation*}
    \omega_p = \sum_{i=1}^{n} (\omega_p)_i \d{x}^i
\end{equation*}
A covector field is expressed similarly.
Each component $\omega_i$ is a smooth, scalar-valued function of points $p$ on $M$.
\begin{equation*}
    \omega = \sum_{i=1}^{n} \omega_i \d{x}^i
\end{equation*}
One caveat of this is that we are generally not able to write such an expression for a covector field that is valid for an entire manifold.
It is always valid locally, since any manifold resembles $\R^n$ near a point, but the geometry of the manifold might mean that it is not possible globally.
Since we have decided not to delve too deep into the theory of manifolds, this is not a major problem for us.

Examples of covectors include $\d{x}$, $5 \d{x} + 2\d{y}$, and $\d{x} - 6\d{z}$.
An example of a covector field is $x^2 z \d{x} + 2 \d{y} - y\sin x \d{z}$, where inputting some values for $x$, $y$, and $z$ turns it into a covector.
The way covectors act on basis tangent vectors, and the fact that they are linear, is enough to evaluate any covector at a given tangent vector.
For instance, given $\omega_p = 3 \d{x} + 2 \d{y}$ and $v = 5 \partial_x - \partial_y$, we have
\begin{align*}
    \omega_p(v) &= \omega_p (5 \partial_x - \partial_y) \\
    &= 5 \omega_p (\partial_x) - \omega_p (\partial_y) \\
    &= 5(3 \d{x} + 2 \d{y}) (\partial_x) - (3 \d{x} + 2 \d{y}) (\partial_y) \\
    &= 5 \cdot 3 \d{x}(\partial_x) + 5 \cdot 2 \d{y}(\partial_x) - 1 \cdot 3 \d{x}(\partial_y) - 1 \cdot 2 \d{x}(\partial_y) \\
    &= 15 \cdot 1 + 10 \cdot 0 - 3 \cdot 0 - 2 \cdot 1 \\
    &= 13.
\end{align*}
We did this the long way on purpose.
We split the expression twice, using the fact that covectors are linear maps and that the addition of covectors is linear.
Note that evaluating a covector involves inputting a vector.
Evaluating a covector field involves inputting a point and then a vector.
Thus, they can be thought of as one-step and two-step evaluations, respectively.

Let us apply this and find the cotangent space for a point on the unit circle, $T^*_p \mathbb{S}^1$ (again, not a proof).
Previously, we found that the tangent space $T_p \mathbb{S}^1$ is spanned by the vector
\begin{equation*}
    v = - \sin t_0 \eval{\frac{\partial}{\partial x}}{p} + \cos t_0 \eval{\frac{\partial}{\partial y}}{p},
\end{equation*}
where $t=t_0$ corresponds to p by the parameterization $\gamma \colon [0, 2\pi) \to \mathbb{S}^1$ with $\gamma(t) = (\cos t, \sin t)$.
We get the above vector by differentiating $\gamma$ with respect to $t$, so we can label the basis tangent vector $\partial_t$.
Now, since $T_p \mathbb{S}^1$ is one-dimensional, it makes sense that $T^*_p \mathbb{S}^1$ is also one-dimensional.
Its basis vector will be the covector $\d{t}$, with $\d{t}(\partial_t) = 1$.
We wish to write this covector in the form $\omega_p = A \d{x} + B \d{y}$.
This can be done by simply evaluating $\omega_p$ at $v = \partial_t$.
\begin{equation*}
    \omega_p(v) = \left( A \eval{\d{x}}{p} + B \eval{\d{y}}{p} \right) \left( - \sin t_0 \eval{\frac{\partial}{\partial x}}{p} + \cos t_0 \eval{\frac{\partial}{\partial y}}{p} \right)
\end{equation*}
This time, we will approach this the quick way by simply picking out the components.
\begin{equation*}
    \omega_p(v) = - A \sin t_0 + B \cos t_0 = 1
\end{equation*}
Using the Pythagorean identity for sine and cosine, we see that $A = - \sin t_0$ and $B = \cos t_0$ is a valid solution.
Hence, our basis covector is
\begin{equation*}
    \omega_p = \eval*{\d{t}}{p} = \omega_p(v) = - \sin t_0 \d{x} + \cos t_0 \d{y},
\end{equation*}
and $T^*_p \mathbb{S}^1 = \operatorname{span}\{\eval{\d{t}}{p} \}$.

\section{Integration of Covector Fields}

Now, having provided a sufficient introduction to covectors, we will show how they can be integrated.
They were developed for this purpose, and their notation purposefully resembles integrands.
This is especially apparent from our choice in the symbol used to represent basis covectors.\footnote{Our use of upright symbols is an intentional choice to note that these are distinct from the $dx$ used for differentials previously.}
Although pushforwards -- and the more primitive notions of differentials -- looked like integrands, this was more of a superficial resemblance.
We did not have the machinery to go further with any ideas.
Still, we are only able to work with integrating curves, but we will build up from there.

The general idea of integrating a covector field does not differ much at all from how integration of an $\R \to \R$ function works.
Integration is always a way to sum quantities in a `limiting' way; the individual contributions can be thought of as infinitely small.
Given a one-dimensional manifold $C$, which is just a curve, we can integrate a covector field $\omega$ over this.
At some point $p$ on $C$, the field $\omega$ specifies a covector $\omega_p$, which acts on the tangent vector $v \in T_p C$ to give $\omega_p(v)$.
These measurements are accumulated over $C$ to give us the result of an integral.

Since each measurement represents the number of stack lines of a covector that are pierced by a tangent vector (the exact one will be shown not to matter) at that point, the integral will represent the total accumulation of these numbers.
However, we already know that covectors are what the covector field resembles near a point.
It reasonably follows that the integral represents the number of contour lines of the covector field that are pierced by the curve $C$ \cite{covector_integration_visualization}.
Integration takes the local measurements to give an overall, global measurement.

To see how exactly this works and how we can evaluate our integral, we introduce the concept of the \emph{pullback}.
If we could transform our problem of integrating over the curve $C$ to (an interval of) the real line, our job would be done.
We know how to integrate over the latter.
Suppose we have some function $\phi\colon \R \to C$.
We can use this to `pull' the covector from $T_p C$ to $\R$.
How is this done?
We know that the pushforward $D\phi_p$ is a map from $T_p \R$ to $\T_p C$, and the covector $\omega_{\phi(p)}$ is a map from $T_{\phi(p)} C$ to $\R$.
Thus, precomposing $D\phi_p$ with $\omega_{\phi(p)}$ gives us a function $\omega_{\phi(p)} \circ D\phi_p \colon T_p \R \to \R$.

The domain of the inner map and the codomain of the outer maps become the domain and codomain of our composite map.
That is just how composing maps and functions works.
A notation for the pullback we described above is $\phi^*$.
Given its relation to the pushforward, it makes sense that an alternative notation for $D\phi$ is $\phi_*$.
Now, putting this together, we can use some map $\phi$ to pull $\omega$ back to an interval of the real line, which we can integrate over.
Our formula for the integral of $\omega$ over a curve $C$ parameterized by $\gamma: [a,b] \to C$, which decides its orientation, is defined as
\begin{equation*}
    \int_\gamma \omega \coloneq \int_a^b \gamma^* \omega.
\end{equation*}
We need to find an expression for the pullback $\gamma^*\omega$.
Note that $\gamma^*\omega$ at a point $t \in [a,b]$ is given by $\omega_{\gamma(t)} \circ D\gamma_t$.
This is a map from $T_p[a,b]$ to $\R$, making it a covector.
Since we are working with an interval of $\R$ (which is one-dimensional), our covector will be of the form $f(t) \d{t}$.
Now, we must find $f(t)$.
This can be done by inputting any tangent vector into both sides of the equation $\omega_{\gamma(t)} \circ D\gamma_t = f(t) \d{t}$, since the equality is true for all tangent vectors.
The simplest option is the basis tangent vector.
Due to the one-to-one association between $T_p [a,b]$ and $T_p \R$, this is $\partial_t$.
On the RHS, we get
\begin{equation*}
    (f(t) \d{t})(\partial_t) = f(t).
\end{equation*}
On the LHS, we evaluate the pushforward $D\gamma_t$.
We know that the differential $D\gamma_t(v) = \mathbf{J}_\gamma v = \nabla_v \gamma(t) = v(\gamma(t))$, where $v \in T_{\gamma(t)} [a,b]$.
This gives us $D\gamma_t(\partial_t) = \partial_t (\gamma(t)) = \gamma'(t)$.
Hence, $(\gamma^*\omega)_t = \omega_{\gamma(t)}(\gamma'(t))$.
Our formula for the integral is therefore
\begin{equation*}
    \int_\gamma \omega = \int_{a}^{b} \omega_{\gamma(t)}(\gamma'(t)) \d{t}.
\end{equation*}
Whilst this could still be considered to be the integral of a covector field (hence the upright $\d{t}$), that was not the problem with our original integral.
The problem was that we were integrating it over some curve.
Now, it perfectly coincides with a standard Riemann integral over $[a,b] \subset \R$.
This means that we have now defined integration over $\omega$ and given it in a form that can be easily evaluated.
As a side note, we will consider all Riemann integrals $\int_a^b f(x) \d{x}$ as the integral of the covector field $f(x) \d{x}$ over $[a,b]$.
This is not doing anything new -- just relabelling.

For our new definition of the integral over a curve to be sensible, it would have to be independent of the way we parameterize the curve.
To show that is indeed the case, consider $\tilde{\gamma} \colon [\alpha, \beta] \to C$ to be another parameterization of $C$.
Then, there is a bijection between $\gamma$ and $\tilde{\gamma}$, $\varphi \colon [\alpha, \beta] \to [a, b]$, such that $\tilde{\gamma}(\tau) = \gamma(\varphi(\tau))$ and $\varphi'(\tau) > 0$.
The last condition was for $\tilde{\gamma}$ to preserve the orientation of $C$ decided by $\gamma$.
The derivative of $\tilde{\gamma}$ is
\begin{equation*}
    \tilde{\gamma}'(\tau) = \gamma'(\varphi(\tau)) \varphi'(\tau).
\end{equation*}
The integral of $C$ using $\tilde{\gamma}$ is
\begin{equation*}
    \int_{\tilde{\gamma}} \omega = \int_{\alpha}^{\beta} \omega_{\tilde{\gamma}(\tau)}(\tilde{\gamma}'(\tau)) \d{\tau} = \int_{\alpha}^{\beta} \omega_{\gamma(\varphi(\tau))}(\gamma'(\varphi(\tau)) \varphi'(\tau)) \d{\tau}.
\end{equation*}
By the linearity of $\omega$,
\begin{equation*}
    \int_{\tilde{\gamma}} \omega = \int_{\alpha}^{\beta} \varphi'(\tau) \omega_{\gamma(\varphi(\tau))}(\gamma'(\varphi(\tau))) \d{\tau}.
\end{equation*}
The substitution $t = \varphi(\tau), \d{t} = \varphi'(\tau) \d{\tau}$ yields (noting the change in limits)
\begin{equation*}
    \int_{\tilde{\gamma}} \omega = \int_{a}^{b} \omega_{\gamma(t)}(\gamma'(t)) \d{t} = \int_\gamma \omega.
\end{equation*}
This means that there is a sensible notion of $\int_C \omega$ given that $C$ has a certain orientation, since all parameterizations of $C$ give the exact same integral.
This is an extremely important fact about covector fields.
They have been chosen as the object to generalize integration precisely because they lend to properties necessary for a definition of integration to make any sense.
If our parameterization $\varphi$ instead reversed the orientation of $C$, so $\varphi(\tau) < 0$, then the calculations above would instead have yielded
\begin{equation*}
     \int_{\tilde{\gamma}} \omega = - \int_{\gamma} \omega,
\end{equation*}
so reversing the direction we integrate along $C$ adds a minus sign to the integral.
This is consistent with our interpretation of the integral.
As long as the curve itself is the same, the number of contour lines crossed by it will be the same.
Since the contour lines have orientation, crossing them in the opposite direction will give the opposite result.
Additionally, a reparameterization can be seen as just using a different pullback, so we have just shown that our choice in pullback does not matter.

We have now answered our initial question of what the $dx$ in the integral means, again.
Instead of being an infinitesimal quantity we sum over, it is an object that takes infinitesimals and measures them so that they can be summed.
Precisely, it is a cotangent vector field.
We have provided our fourth and final interpretation of what $dx$ means.
This one has been sought after specifically for its use in integration.
Our work remains incomplete, however.
There is a lot more we can do with our newly developed theory, which is what we will explore in the next section.

\section{Differential Forms}

Currently, we are only able to integrate over one-dimensional manifolds: curves.
This is because a covector takes a single tangent vector and measures it.
This gives us lengths we can sum over, but we need areas in order to integrate over surfaces (two-dimensional manifolds) and volumes in order to integrate over three-dimensional manifolds.
Thus, we need an object that takes multiple tangent vectors as inputs and returns higher-order measurements.
The first step to see what this object could be is to give covectors the new name of one-forms.
Covector fields are given the name of differential one-forms.

At any given point $p$ on a manifold $M$, a differential one-form evaluates to give a one-form, which is a linear functional of the form $T_p M \to \R$.
It reasonably follows that the object that we require is a differential two-form.
This assigns to each point $p$ a two-form, a linear functional $T_p M \times T_p M \to \R$.
Given two tangent vectors, the value of the two-form will represent the signed area of some shape defined by those vectors.
From the fact that areas of objects are always (in some form or another) the product of lengths, it is reasonable to suppose that our two-form is to be constructed from one-forms.
We need to find this construction.

From this point onwards, we will often use the same notation to reference both a two-form and a differential two-form.
That is, the subscript $p$ used for the actual functional is often omitted for the sake of convenience; this is technically an abuse of notation, however.
Due to the properties of the objects we are dealing with, algebraic results for one will be directly related to results for the other.
Context should allow for the distinction between them when it is needed.

Given one-forms $\alpha$ and $\beta$, we will denote the corresponding two-form as $\alpha \wedge \beta$.
This is called a \emph{wedge product}.
As mentioned above, this is an abuse of notation, and the correct notation for the two-form is $(\alpha \wedge \beta)_p$ or $\alpha_p \wedge \beta_p$.
Suppose we have the tangent vectors $u, v, w \in T_p M$.
Let $\omega = \alpha \wedge \beta$, and let $\lambda, \mu \in \R$.
Our two-form must satisfy some properties in order for the result of its evaluation to represent a signed area.
First, it must be linear in both of its inputs.
\begin{itemize}
    \item $\omega (\lambda u + \mu v, w) = \lambda \omega (u, w) + \mu \omega (v, w)$
    \item $\omega (u, \lambda v + \mu w) = \lambda \omega (u, v) + \mu \omega (u, w)$
\end{itemize}
The technical name for this is \emph{bilinearity}.
We also want $\omega(v, v) = 0$, as the area of a line segment should be zero; this makes $\omega$ an \emph{alternating form}.
From these two properties, it follows that
\begin{equation*}
    \omega(u + v, u + v) = \omega(u, u) + \omega(u, v) + \omega (v, u) + \omega (v, v) = 0,
\end{equation*}
so
\begin{equation*}
    \omega(u, v) = - \omega (v, u).
\end{equation*}
Hence, $\omega$ must be \emph{antisymmetric}.
This is enough information to find a suitable object.
The constituent one-forms of $\alpha \wedge \beta$ act on $u$ and $v$ to give us four numbers: $\alpha(u)$, $\beta(u)$, $\alpha(v)$, and $\beta(v)$.
Then, we can consider the vectors $\langle \alpha(u), \beta(u) \rangle$ and $\langle \alpha(v), \beta(v) \rangle$.
We will define the value of our two-form as the area of the parallelogram formed by these two vectors.
It is thus given by the determinant of the matrix formed using the vectors as rows.
\begin{equation*}
    \alpha \wedge \beta (u, v) = \begin{vmatrix}
    \alpha(u) & \beta(u) \\
    \alpha(v) & \beta(v)
    \end{vmatrix}
\end{equation*}
It can be verified via computation that our chosen formula implies that a two-form is an alternating bilinear functional.
Therefore, we have found what we wanted.
To gain better intuition of what a two-form does, consider the simplest example: $\d{x} \wedge \d{y}$.
Whilst $\d{x}, \d{y} \in T^*_p M$ can have any dimension, they must have dimension greater than one if they are to be combined into a two-form.
That is because the tangent vectors in $T_p M$ must have dimension greater than one so that it is possible to construct a parallelogram with them.
Evaluating $\d{x} \wedge \d{y}$ for tangent vectors $u$ and $v$ gives
\begin{equation*}
    \d{x} \wedge \d{y} (u, v) = \begin{vmatrix}
    \d{x}(u) & \d{y}(u) \\
    \d{x}(v) & \d{y}(v)
    \end{vmatrix}
    = \begin{vmatrix}
        u_x & u_y \\
        v_x & v_y
    \end{vmatrix},
\end{equation*}
which is the area of the parallelogram defined by the projection of $u$ and $v$ onto the $xy$--plane.
Note that, if $M$ is two-dimensional, any two-form can be expressed as a scalar multiple of $\d{x} \wedge \d{y}$.
If $\alpha = A\d{x} + B\d{y}$ and $\beta = C\d{x} + D\d{y}$, then (assuming that the wedge product distributes over addition)
\begin{align*}
    \alpha \wedge \beta &= (A\d{x} + B\d{y}) \wedge (C\d{x} + D\d{y}) \\
    &= AC \d{x} \wedge \d{x} + AD \d{x} \wedge \d{y} + BC \d{y} \wedge \d{x} + BD \d{y} \wedge \d{y} \\
    &= AD \d{x} \wedge \d{y} + BC \d{y} \wedge \d{x} \\
    &= (AD - BC) \d{x} \wedge \d{y}.
\end{align*}
In this case, $\alpha \wedge \beta$ evaluates to give a scaled version of the previously-mentioned area.
The vectors $u$ and $v$ in $T_p M$ are mapped to the vectors $\langle \alpha(u), \beta(u) \rangle$ and $\langle \alpha(v), \beta(v) \rangle$ in $\R^2$, which define a parallelogram.
The new parallelogram is the image of the original parallelogram, spanned by $u$ and $v$, under the action of the two-one forms.

An important note to make is that a two-form is not, in general, the wedge product of two one-forms.
It is true for two-dimensional manifolds, since we only have two basis covectors to work with.
They combine in one way to give $\d{x} \wedge \d{y}$.
We will find the same result is true for three-dimensional manifolds.
A generic two-form can be written $\omega = A \d{x} \wedge \d{y} + B \d{x} \wedge \d{z} + C \d{y} \wedge \d{z}$.
Terms such as $\d{x} \wedge \d{x}$ are zero, and terms such as $\d{y} \wedge \d{x}$ are simply negatives of one of three included two-forms.
Any such two-form can be written as the wedge product of two one-forms.

To see why, consider the second diagram in \cref{fig:k-form_hyperplanes}.
This is an extension of the visualization we had for one-forms using hyperplanes.
The wedge product of two one-forms is the intersection of two sets of hyperplanes, along with an orientation.
This forms a grid of tubes, hyperplanes of one fewer dimension.
A two-form in general can be thought of as a grid of tubes -- not necessarily the intersection of hyperplanes.
We can consider the number of intersections made by the parallelogram, defined by two tangent vectors, with these lines as the result of evaluating a two-form.

In three dimensions, the visualization of the wedge products of two one-forms coincides with the visualization of a generic two-form.
Any grid of tubes can be represented by the intersection of two sets of planes.
Hence, for three-dimensional manifolds, any two-form can be expressed as the wedge product of two one-forms.
Consider the case where $M$ is four-dimensional.
Let $\omega = \d{x} \wedge \d{y} + \d{z} \wedge \d{w}$.
This is a combination of a grid of (planar) tubes along the $zw$--plane and a grid of tubes along the $xy$--plane.
They are perpendicular to each other and cannot be expressed as the intersection of two sets of hyperplanes.

Evaluating $\omega$ consists of taking the parallelogram defined by two tangent vectors and counting the number of tubes crossed in total.
This is equivalent to finding the area of two different scaled parallelograms and summing them together.
For manifolds with dimension greater than three, two-forms are not necessarily the wedge product of two one-forms.
When they are, the two-form is called simple or decomposable.
This fact, and the uniqueness of three-dimensional space, will be relevant when we cover the Hodge star operator and duality.

We have now given a proper geometric interpretation of two-forms, explaining the necessary nuance.
A two-form takes two vectors in a tangent space, maps them to possibly multiple parallelograms, and computes the sum of their areas.
This can equivalently be viewed as the two-form being a grid of tubes, counting the number of tubes (in a continuous way) crossed by the parallelogram defined from the two tangent vectors directly.
The actual computation involves calculating the determinant of a matrix.

Our definitions readily extend to forms of higher degree.
Without first thinking about the properties a three-form needs to possess, we can use the wedge product to combine a two-form with a one-form to obtain a three-form.
It will take three tangent vectors as input and is evaluated by computing the determinant of a $3 \times 3$ matrix.
This is the area of a parallelepiped (three-dimensional parallelogram) in $\R^3$.
\begin{equation*}
    \alpha \wedge \beta \wedge \gamma (u, v, w) = \begin{vmatrix}
    \alpha(u) & \beta(u) & \gamma(u) \\
    \alpha(v) & \beta(v) & \gamma(v) \\
    \alpha(w) & \beta(w) & \gamma(w)
    \end{vmatrix}
\end{equation*}
There is no need to distinguish between $(\alpha \wedge \beta) \wedge \gamma$ and $\alpha \wedge (\beta \wedge \gamma)$, since they can be shown to both be equal to the above determinant, i.e. the wedge product is associative.
A three-form is an alternating trilinear functional, which measures three-dimensional manifolds to give volumes.
Once again, a three-form in general is not necessarily simple.
This can be seen when considering dimensions greater than four.
Irrespective of that fact, it always maps three tangent vectors to possibly multiple parallelepipeds and computes the sum of their areas.

The third diagram in \cref{fig:k-form_hyperplanes} shows a visualization of the wedge product of three one-forms.
It is a grid of points in the case of three dimensions, and in general, it is a grid of hyperplanes of two fewer dimensions than the hyperplanes of the one-forms; we can call them threads.
A three-form in general is a grid of these threads -- not necessarily the intersection of hyperplanes.
It is then valid to think of a three-form as counting the number of threads crossed by a parallelepiped formed from the three tangent vectors.

\begin{figure}[!htbp]
    \centering
    \scalebox{1.5}{\includegraphics{k-form_hyperplanes}}
    \captionsetup{justification=centering}
    \caption{Visualization of $k$-forms as hyperplanes
    \\Credits: Maschen -- Own work, \href{https://creativecommons.org/public-domain/cc0/}{CC0}, via Wikimedia Commons}
    \label{fig:k-form_hyperplanes}
\end{figure}

In general, $\omega = \omega_1 \wedge \omega_2 \wedge \cdots \wedge \omega_k$ is called a differential $k$-form.
It assigns to each point the $k$-form $\omega_p$ (also called a $k$-covector, but we will not use this name), which is a multilinear alternating functional that measures $k$-dimensional manifolds.
Being multilinear means it is linear in all its inputs.
The formula for $\omega_p$ is given by the determinant of a certain $k \times k$ matrix, the area of a $k$-dimensional parallelotope in $\R^k$.
The intuition and visualizations all extend to this most general case.
\begin{equation*}
    \omega_p(v_1,\dots,v_n)=\begin{vmatrix}
    \omega_1(v_1) & \omega_2(v_1) & \cdots & \omega_k(v_1)\\
    \omega_1(v_2) & \omega_2(v_2) & \cdots & \omega_k(v_2)\\
    \vdots & \vdots & \ddots & \vdots \\
    \omega_1(v_k) & \omega_2(v_k) & \cdots & \omega_k(v_k)
    \end{vmatrix}
\end{equation*}
These are all examples of \emph{differential forms}.
They are the subject of our interest, the object that we have slowly worked towards defining in a way that feels natural.
We started with simple enquiries regarding notation that we are familiar with, and this has led to a few concepts -- with more to come -- that are much more general than what we started with.
Differential forms are a very useful tool for the study of higher-level mathematics.
This includes topics where they are the main tool used, as well as those where they provide additional insights and alternative viewpoints to well-established results.

We will define a zero-form to just be a real number.
The space of zero-forms is thus always $\R$, and a differential zero-form is just a smooth function from $M$ to $\R$.
This definition has been provided here without justification, but it should feel reasonable; what could be a better definition?
This is included for the sake of completeness, but more importantly, it will allow us to see a pattern between differential forms of different degrees later on.

We will now consider the algebra of differential forms with the wedge product.
We have already taken for granted that the wedge product is associative and distributes over addition.
Additionally, the wedge product is an alternating product, as
\begin{equation*}
    \alpha \wedge \alpha = \begin{vmatrix}
    \alpha(u) & \alpha(u) \\
    \alpha(v) & \alpha(v)
    \end{vmatrix}
    = \alpha(u)\alpha(v) - \alpha(u)\alpha(v) = 0.
\end{equation*}
It is, however, not commutative.
Considering two one-forms $\alpha$ and $\beta$, their wedge product is
\begin{align*}
    \alpha \wedge \beta &= \begin{vmatrix}
    \alpha(u) & \beta(u) \\
    \alpha(v) & \beta(v)
    \end{vmatrix} \\
    &= \alpha(u)\beta(v) - \alpha(v)\beta(u) \\
    &= - (\alpha(v)\beta(u) - \alpha(u)\beta(v)) \\
    &= - \begin{vmatrix}
    \beta(u) & \alpha(u) \\
    \beta(v) & \alpha(v)
    \end{vmatrix} \\
    &= - \beta \wedge \alpha.
\end{align*}
This shows that the wedge product is anticommutative.
The fact that swapping one-forms around in a wedge product introduces a negative sign is useful for simplifying more complicated expressions.
In general, $\gamma \wedge \delta = (-1)^{kl} \gamma \wedge \delta$, where $\gamma$ is a $k$-form and $\delta$ is an $l$-form.
Finally, note the obvious fact that $\lambda \cdot (\alpha \wedge \beta) = (\lambda \alpha) \wedge \beta = \alpha \wedge (\lambda \beta)$ for all $\lambda \in \R$.
This is essentially saying that scalar multiplication is compatible with the wedge product.

One-forms $\omega_p \in T^*_p M$ span a vector space over the field of real numbers, $\R$.
The properties of the wedge product mentioned above mean that one-forms, along with the operations of addition and the wedge product, form what is called an \emph{exterior algebra}.
This is denoted by $\extp T^*_p M$.
An exterior algebra is a vector space with additional structure.\footnote{Familiarity with abstract algebra may allow for one to recognize that an exterior algebra is an associative algebra with the property that the wedge product is alternating.}
Any $k$-form can be obtained by adding and wedging one-forms together, so they are all elements of this exterior algebra.
Fittingly, an alternative name for the wedge product is the exterior product.

The exterior algebra contains forms of different degrees, though we can consider vector spaces containing only one degree.
The $k$-th exterior power of $T^*_p M$ is the vector subspace of $\extp T^*_p M$ spanned by elements of the form $\d{x}^1 \wedge \d{x}^2 \wedge \cdots \d{x}^k$, where $\d{x}^i \in T^*_p M$ for $i \in \{1, 2, \ldots, k\}$.
Technically, each $\d{x}^i$ should have an evaluation bar.
A $k$-form is an element of $\extp^k T^*_p M$.
Since this only requires us to know what a one-form (covector) and wedge product are, it is a concise way of defining $k$-forms in general -- and hence defining differential $k$-forms.

Assuming that $M$ has $n$ dimensions, the first exterior power of $T^*_p M$ is spanned by $n$ one-forms.
These are our basis covectors, $\d{x}^i$.
Thus, the dimension of $\extp^1 T^*_p M$ is $n$.
The second exterior power is spanned by the two-forms $\d{x}^i \wedge \d{x}^j$ such that $i < j$.
There are $\binom{n}{2}$ such two-forms, so that is the dimension of $\extp^2 T^*_p M$.
Therefore, in general, the dimension of $\extp^k T^*_p M$ is $\binom{n}{k}$. 
The number of dimensions of the exterior algebra will be the sum of the dimensions of every exterior power.
It is equal to
\begin{equation*}
    \sum_{i=0}^{n} \binom{n}{i} = 2^n.
\end{equation*}
As an example, consider the manifold $\R^4$.
The space of zero-forms is $\R$, as it always is.
The space of one-forms has the span $\{\d{x}, \d{y}, \d{z}, \d{w}\}$.
The space of two-forms has the span $\{\d{x} \wedge \d{y}, \d{x} \wedge \d{z}, \d{x} \wedge \d{w}, \d{y} \wedge \d{z}, \d{y} \wedge \d{w}, \d{z} \wedge \d{w}\}$.
The space of three-forms has the span $\{\d{x} \wedge \d{y} \wedge \d{z}, \d{x} \wedge \d{y} \wedge \d{w}, \d{x} \wedge \d{z} \wedge \d{w}, \d{y} \wedge \d{z} \wedge \d{w}\}$.
Lastly, the space of four-forms has the span $\{\d{x} \wedge \d{y} \wedge \d{z} \wedge \d{w}\}$.
We cannot have a nonzero five-form or one of even higher degree, since that would involve repeating one of the basis covectors.
They would wedge to make zero, so the entire form would be zero.

We may notice that the space of $k$-forms matches with the space of $(4-k)$-forms.
This makes sense, since the binomial coefficient is symmetrical: $\binom{n}{k} = \binom{n}{n - k}$.
However, the idea of $k$-forms and $(n-k)$-forms being dual to each other goes deeper than that.
It turns out that there is a one-to-one correspondence between these, and this is an important fact that will be expanded upon later.

The final part of this section involves us providing an explicit formula for $k$-forms.
Note that the following is not at all necessary for understanding the rest of the exposition.
A differential $k$-form $\omega$ can be expressed (local to $p$) as follows, where $\omega_{i_1 i_2 \ldots i_k}$ are smooth, scalar-valued functions of $p$ on $M$.
\begin{equation*}
    \omega = \sum_{1 \leq i_1 < i_2 < \cdots < i_k \leq n} \omega_{i_1 i_2 \ldots i_k} \d{x}^{i_1} \wedge \d{x}^{i_2} \wedge \cdots \wedge \d{x}^{i_k}
\end{equation*}
This is incredibly cumbersome to write, so we can use the multi-index $I = (i_1, i_2, \ldots, i_k)$ to greatly simplify the expression.
\href{https://en.wikipedia.org/wiki/Multi-index_notation}{Multi-index notation} is not required for anything else in this exposition.
Here, we use a prime symbol ad hoc to denote that we are only summing over increasing multi-indices.
\begin{equation*}
    \omega = {\sum_{\abs{I} = k}}' \omega_I \d{x}^I
\end{equation*}
Then, at any point $p$, the $k$-form $\omega_p$ is given by the following.
\begin{equation*}
    \omega_p = {\sum_{\abs{I} = k}}' (\omega_p)_I \d{x}^I
\end{equation*}

\section{Integration of Differential Forms}

With the theory of differential forms developed, we are ready to apply them to integration.
We have already defined what the integral of a differential one-form -- a covector field -- means.
To reiterate, the integral of a differential one-form $\omega$ over some curve $C$ in $M$, with a certain orientation, is defined by
\begin{equation*}
    \int_C \omega \coloneq \int_a^b \gamma^* \omega = \int_a^b \omega_{\gamma(t)} (\gamma'(t)) \d{t},
\end{equation*}
where $\gamma \colon [a,b] \to C$ is a parameterization of $C$ that preserves its orientation, and $\gamma^* \omega$ is the pullback of $\omega$ by $\gamma$.
The interpretation of this integral is that it counts the number of (oriented) contour lines of $\omega$ crossed by $C$.

\section{The Exterior Derivative}

%TODO INTRODUCE EXTERIOR DERIVATIVE
%TODO INTRODUCE INTEGRATION OF K-FORMS
%TODO MENTION PULLBACKS MORE GENERALLY
%TODO INTRODUCE HODGE STAR

\section{Resources}

\textsc{Tensor Calculus 6: Differential Forms are Covectors by eigenchris} \, \url{https://www.youtube.com/watch?v=XGL-vpk-8dU}

\textsc{Differential Forms by Michael Penn} \, \url{https://youtube.com/playlist?list=PL22w63XsKjqzQZtDZO_9s2HEMRJnaOTX7}

\textsc{A Geometric Approach to Differential Forms by David Bachman (arXiv version 1)} \, \url{https://arxiv.org/pdf/math/0306194v1}

\printbibliography

\begin{appendices}
\crefalias{section}{appsec}
\crefalias{subsection}{appsec}

\section{Vectors} \label{appendix:vectors}

\subsection{Definition of a Vector} \label{appendix:vector_space}

An introduction to the concept of vectors usually involves giving examples of vectors -- such as arrows in a plane -- without giving a proper definition of what a vector is in general.
That makes sense, since such a definition is (relatively) technical and unnecessary for simply working with vectors.
The technical definition is that a vector is an element of a vector space, but that alone simply replaces one question with another.
It follows that a definition of a vector space is now required.

To come up with it, we will follow a method of generalization that can be readily applied to other situations as well.
We know that ordered pairs of real numbers are examples of vectors, so the set containing all such pairs is an example of a vector space.\footnote{This is technically an abuse of language. A vector space is a set with additional structure -- the ability to add and scale its elements in a certain way. However, this distinction is not very useful until we actually define a vector space properly.}
This set is denoted by $\R \times \R$, and $\R^2$ is a useful shorthand.
We will examine the properties of this set in order to obtain a general definition of a vector space.

Two things we can do with our vectors are adding them together and scale them.
For the example we are working with, this is done component-wise: $(a, b) + (b, c) = (a + b, c + d)$ and $c \cdot (a, b) = (ac, bc)$.
Since we can use real numbers to scale vectors, they are also referred to as scalars in this context.
Both of those functions have geometric interpretations if the vectors are viewed as arrows on a plane.
Vector addition involves connecting two vectors tip-to-tail, and the sum is the third side of a triangle (or the diagonal of a parallelogram).
Scalar multiplication involved literally scaling the length of a vector by some number, where multiplication by a negative number reverses direction.

Whilst vectors in general are not arrows, the way they add and scale is fundamental to them.
If we try to extract all the properties of addition and scalar multiplication, we get the following list.
Note that the boldface letters are elements of $\R^2$, whilst the Greek letters are real numbers.
Also, the boldface zero represents (0, 0).
\begin{enumerate}
\item $\mathbf{u} + \mathbf{v} = \mathbf{v} + \mathbf{u}$
\item $\mathbf{u} + (\mathbf{v} + \mathbf{w}) = (\mathbf{u} + \mathbf{v}) + \mathbf{w}$
\item $\mathbf{0} + \mathbf{v} = \mathbf{v}$
\item $-\mathbf{v} + \mathbf{v} = \mathbf{0}$
\item $\lambda(\mu\mathbf{v}) = (\lambda\mu)\mathbf{v}$
\item $1\mathbf{v} = \mathbf{v}$
\item $\lambda(\mathbf{v} + \mathbf{w}) = \lambda\mathbf{v} + \lambda\mathbf{w}$
\item $(\lambda + \mu)\mathbf{v} = \lambda\mathbf{v} + \mu\mathbf{v}$
\end{enumerate}
That leads to our definition of a vector space -- a set, together with vector addition and scalar multiplication, that satisfies the above statements.\footnote{Addition and scalar multiplication must be closed, but this is within their definitions, rather than being a part of the axioms.}
More precisely, the third and fourth statements say that an identity vector (that works for all vectors) and inverse vectors (that each work for a specific vector) exist.
These statements are known as the vector space axioms.
Note that choosing those specific axioms to define a vector space is just that: a choice.\footnote{It is not an arbitrary choice, however. Vectors are an incredibly useful concept both in pure maths and in other subjects -- where they are applied (along with linear algebra in general) perhaps more than any other topic in maths.}
We can explore sets with different structures, obtaining objects like groups and rings instead.

A common notation for vectors is boldface letters, and that is what will be used in this document.
Regular real numbers are never in boldface, though they might use either the Latin or Greek alphabet.
Additionally, whilst there are many different objects that classify as vectors, the only ones that will be considered by us are in fact lists of real numbers.
An ordered list of $n$ real numbers is an element of $\R^n$.
In this context, $n$ is always interpreted as being a positive integer.

The above contains sufficient information to define vectors and work with them on a basic level.
What follows are some additional points of discussion.
Firstly, there are many different notations that exist for vectors.
Arrows, particularly in physics, can also be used to denote vectors, e.g. $\vec{F} = m\vec{a}$.
It is not easy to write boldface symbols in handwriting, so another alternative is underlining the vector, $\underline{v}$, or putting a tilde beneath it, $\underaccent{\sim}{v}$.
For the actual value of the vector, you can write it as a coordinate, as was shown before.
Another common way is to write it as a column of numbers.
\begin{equation*}
    \mathbf{v} = \begin{bmatrix}
        a \\
        b
    \end{bmatrix}
\end{equation*}
It can also be written like $\mathbf{v} = \rvec{a,b}$. Note that the former is called a column vector, whilst the latter is a row vector.
Technically, they are different objects, but this distinction only matters when dealing with matrices.
Another way to write a vector is $\mathbf{v} = \langle a, b \rangle$.
This might be preferred due to avoiding confusion with coordinates, though writing $\mathbf{v} = (a,b)$ is pretty common.
Additionally, given that $\ihat = \langle 1, 0 \rangle$ and $\jhat = \langle 0, 1 \rangle$, you can write a vector as $\mathbf{v} = v_x\ihat + v_y\jhat$, where $v_x$ and $v_y$ are the components of the vector.
Note that these examples were given for $\R^2$, but they extend to general vectors in $\R^n$.

Another point to make is that vector addition is a binary operation.
It takes two vectors as an input and outputs another vector.
On the other hand, scalar multiplication is a binary function.
It takes a vector and a scalar as an input and outputs a vector.
All operations are functions, but the converse is not true in general.
An operation is a function whose domain is some power of the codomain.
For example, vector addition -- in reference to elements of $\R^n$ -- may be written as $+\colon \R^{2n} \to \R^n$.
This is a matter of semantics, and using the word `operation' loosely is both common and unharmful.

\subsection{Vector Calculus} \label{appendix:vector calculus}

Defining vectors is itself not the most fruitful task.
A proper definition of real numbers is both motivated and immediately succeeded by a discussion on how to do calculus with them.
We wish to extend those notions to vectors, and we will find that our definitions are not terribly different.
First, consider the simplest case of a vector-valued function whose domain is $\R$.
Let the function be called $\mathbf{f}$ and let $\mathbf{y} = \mathbf{f}(x)$.
There is nothing stopping us from putting this into the regular definition of the derivative.
\begin{equation*}
    \mathbf{f}'(x) = \frac{d\mathbf{y}}{dx} = \lim_{h \to 0} \frac{\mathbf{f}(x+h) - \mathbf{f}(x)}{h}
\end{equation*}
This works perfectly, so we will have it as the definition of the derivative of $\mathbf{f}$ at a point $x$, provided the limit exists.
How do we actually evaluate this derivative?
For that, consider $\mathbf{f}(x) = a(x)\ihat + b(x)\jhat$.
Putting this into our definition gives the following.
\begin{align*}
    f'(x) &= \lim_{h \to 0} \frac{a(x+h)\ihat + b(x+h)\jhat - (a(x)\ihat + b(x)\jhat)}{h} \\
    &= \lim_{h \to 0} \frac{a(x+h) - a(x)}{h}\ihat + \lim_{h \to 0} \frac{b(x+h) - b(x)}{h}\jhat \\
    &= a'(x)\ihat + b'(x)\jhat
\end{align*}
In general, we can differentiate such functions by differentiating their components.
If instead, we consider a function $f$ which maps from $\R^n$ to $\R$ (a vector to a real number), our job becomes much more difficult.
Our regular definition of a limit will not work, as we have to consider vectors approaching other vectors.
Fortunately, it is not too different from the normal $\varepsilon$--$\delta$ definition.
We say that
\begin{equation*}
    \lim_{\mathbf{x} \to \mathbf{a}} f(\mathbf{x}) = \mathbf{L}
\end{equation*}
if and only if, for all $\varepsilon > 0$, there exists a $\delta > 0$ such that for all $\mathbf{x}$ in the domain of $f$,
\begin{equation*}
    0 < \norm{\mathbf{x} - \mathbf{a}} < \delta \implies \norm{f(\mathbf{x}) - \mathbf{L}} < \varepsilon.
\end{equation*}
This simplifies down to the regular definition when $n=1$, and intuitively represents the exact same idea.
The problem arises when we try to generalize differentiation.
For $n=1$, it involves a limit of a fraction, where we divide by $h$.
However, if $n > 1$, then we now have the vector $\mathbf{h}$, and we do not know how to divide by vectors.
We could attempt to resolve it by instead dividing by $\norm{\mathbf{h}}$, saying that the derivative of $f$ at $x$ is
\begin{equation*}
    \lim_{\mathbf{h} \to \mathbf{0}} \frac{f(\mathbf{x}+\mathbf{h}) - f(\mathbf{x})}{\norm{\mathbf{h}}},
\end{equation*}
when this limit exists.
The problem is that, when $n=1$, the limit simplifies to
\begin{equation*}
    \lim_{h \to 0} \frac{f(x+h) - f(x)}{\abs{h}}.
\end{equation*}
Suppose $f(x) = x$.
Our limit becomes
\begin{equation*}
    \lim_{h \to 0} \frac{h}{\abs{h}}.
\end{equation*}
This limit does not exist, meaning that $f(x)$ is not differentiable anywhere under this definition.
Furthermore, it would suggest that $f(x) = \abs{x}$ is differentiable at $x=0$, which is also not true.
Generally, this limit exists only if it is equal to zero.
This is because a limit exists if and only if both the left- and right-hand limits exist.
When $h \to 0^+$, $\abs{h} = h$.
When $h \to 0^-$, $\abs{h} = -h$.
The two one-sided limits differ by their sign, so they are equal only if they are zero.
Although, we can take our original definition of the derivative and move to $f'(x)$ inside the limit to get
\begin{equation*}
    \lim_{h \to 0} \frac{f(x+h) - f(x) - hf'(x)}{h} = 0.
\end{equation*}
That is, the derivative of $f$ at $x$ if there is some number $f'(x)$ that satisfies the above statement.
Now, replacing $h$ with $\abs{h}$ will not make a difference.
That means we can generalize our derivative for any $n$.
The derivative of a function $f$ at $\mathbf{x}$ is equal to $D$ if it satisfies
\begin{equation*}
    \lim_{\mathbf{h} \to \mathbf{0}} \frac{f(\mathbf{x}+\mathbf{h}) - f(\mathbf{x}) - D\mathbf{h}}{\norm{\mathbf{h}}} = \mathbf{0}.
\end{equation*}
We may write $D$ as $f'(\mathbf{x})$.
It is important to know what kind of object $D$ is.
It cannot be a real number, since that would make $D\mathbf{h}$ a vector, and the rest of the numerator is a scalar.
If we treat $\mathbf{h}$ as a column vector, then $D$ is a row vector -- their product is a $1 \times 1$ matrix which can be associated with a scalar.
This makes $D$ a $1 \times n$ matrix.
The name for it is the \emph{Jacobian matrix}, or just the Jacobian, and notation for it includes $\mathbf{J_f}(\mathbf{x})$ (the subscript denotes the function it is for) and $\mathbf{J}(\mathbf{x})$.
We can also consider $\mathbf{J_f}$ to be a matrix of functions of $\mathbf{x}$.

Now that we have introduced matrices, we can use them to make the final generalization to functions $\mathbf{f}$ that map from $\R^n$ to $\R^m$ (vectors to vectors).
It follows from everything we have discussed so far that the derivative of $\mathbf{f}$ at $\mathbf{x}$ is an $m \times n$ matrix $\mathbf{J_f}(\mathbf{x})$ if it satisfies
\begin{equation*}
    \lim_{\mathbf{h} \to \mathbf{0}} \frac{\mathbf{f}(\mathbf{x}+\mathbf{h}) - \mathbf{f}(\mathbf{x}) - \mathbf{J_f}(\mathbf{x})\mathbf{h}}{\norm{\mathbf{h}}} = \mathbf{0}.
\end{equation*}
An $m \times n$ matrix multiplied by an $n \times 1$ matrix gives an $m \times 1$ matrix or an $m$-dimensional column vector (the same as the rest of the numerator).
Note that, since a derivative is a matrix, it is a linear map from $\R^n$ to $\R^m$.
This is in-line with the one-dimensional case.
We can then go on to develop familiar concepts with our generalized derivative.
For example, a function is differentiable if there exists some value of $\mathbf{J_f}(\mathbf{x})$ such that the above limit equals $\mathbf{0}$; a function being differentiable implies it is continuous.

One problem we have is that our definition of the derivative is not explicit.
It states that $\mathbf{f}'(\mathbf{x}) = \mathbf{J_f}(\mathbf{x})$ given that $\mathbf{J_f}(\mathbf{x})$ satisfies some condition, but it does not tell us how we actually find the Jacobian.
This in contrast to the case of $n=m=1$, where finding the derivative involves evaluating a limit -- with the function being differentiable if the limit exists.
To find an explicit formula for the derivative, we will first introduce a new concept: the \emph{directional derivative}.
Intuitively, it tells you the rate of change of a function in some particular direction (multiplied by a number).

First, we will start with a function $\mathbf{f}$ from $U$ to $\R^m$, with $U \subseteq \R^n$.
Assume that $\mathbf{f}$ is differentiable at some point $\mathbf{x}$.
Let $\mathbf{v}$ be a fixed element of $\R^n$, and have $\mathbf{h} = t\mathbf{v}$ with $t$ being real.
Since $\mathbf{f}$ is differentiable, we have
\begin{equation*}
    \lim_{t\mathbf{v} \to \mathbf{0}} \frac{\mathbf{f}(\mathbf{x}+t\mathbf{v}) - \mathbf{f}(\mathbf{x}) - t\mathbf{J_f}(\mathbf{x})\mathbf{v}}{\norm{t\mathbf{v}}} = \mathbf{0},
\end{equation*}
assuming $\mathbf{v} \neq \mathbf{0}$.
This can be rewritten as
\begin{equation*}
    \lim_{t \to 0} \frac{\mathbf{f}(\mathbf{x}+t\mathbf{v}) - \mathbf{f}(\mathbf{x}) - t\mathbf{J_f}(\mathbf{x})\mathbf{v}}{\abs{t}\norm{\mathbf{v}}} = \mathbf{0}.
\end{equation*}
Since $\mathbf{v}$ is fixed (does not depend on $t$), we get
\begin{equation*}
    \lim_{t \to 0} \frac{\mathbf{f}(\mathbf{x}+t\mathbf{v}) - \mathbf{f}(\mathbf{x}) - t\mathbf{J_f}(\mathbf{x})\mathbf{v}}{\abs{t}} = \mathbf{0},
\end{equation*}
which is equivalent to
\begin{equation*}
    \mathbf{J_f}(\mathbf{x})\mathbf{v} = \lim_{t \to 0} \frac{\mathbf{f}(\mathbf{x}+t\mathbf{v}) - \mathbf{f}(\mathbf{x})}{t}.
\end{equation*}
The final statement is also true for $\mathbf{v} = \mathbf{0}$, so it is true for all $\mathbf{v}$.
This limit is of the same form as the derivative for $n=1$, as it gives an explicit value of the Jacobian applied to any vector $\mathbf{v}$.
However, since we had to assume that the function is differentiable to obtain this, the existence of this limit does not imply that $\mathbf{J_f}(\mathbf{x})$ exists.
It is still a useful concept, and we call it the directional derivative of $\mathbf{f}$ along the vector $\mathbf{v}$.
The notation for it is $\nabla_\mathbf{v}\mathbf{f}$.
Since $\nabla_\mathbf{v}\mathbf{f}(\mathbf{x}) = \mathbf{J_f}(\mathbf{x})\mathbf{v}$, we have
\begin{equation*}
    \eval{\frac{d}{dt} \mathbf{f}(\mathbf{x+t\mathbf{v}})}{t=0} = \eval{\mathbf{f}'(\mathbf{x+t\mathbf{v}})}{t=0} \mathbf{v} = \mathbf{f}'(\mathbf{x})\mathbf{v} = \nabla_\mathbf{v}\mathbf{f}(\mathbf{x}).
\end{equation*}
This just gives us as an alternative way to write the directional derivative.
If we choose $\mathbf{v}$ to be $\ehat_i$, which is the $i$-th standard basis vector for $\R^n$ (for $\R^3$, we have $\ihat$, $\jhat$, and $\khat$),
then the directional derivative becomes the partial derivative.
This works for any function that outputs to $\R^m$, but the partial derivative is conventionally defined only for real-values functions.
\begin{equation*}
    \nabla_{\ehat_i}\mathbf{f}(\mathbf{x}) = \lim_{t \to 0} \frac{f(\mathbf{x} + t\ehat_i) - \mathbf{f}(\mathbf{x})}{h}
\end{equation*}
Notation for this includes $\partial f / \partial x_i$, $\partial_i f$, and $f_i$.
This definition make sense intuitively, as the partial derivative is a rate of change in some direction -- restricted to specific directions.
For the next result, note that for any $m \times n$ matrix $\mathbf{A}$, the $j$-th column is given by $\mathbf{A}\ehat_j$.
Then, we can get the $i$-th component of that column by taking the dot product with $\ehat_i$.
Thus, $A_{ij} = (A\ehat_j) \cdot \ehat_i$.
Here, $\ehat_i$ is the $i$-th standard basis vector for $\R^n$, and $\ehat_j$ is the $j$-th standard basis vector for $\R^m$.

For $\mathbf{A} = \mathbf{J_f}(\mathbf{x}) = \mathbf{f}'(\mathbf{x})$, noting that $\mathbf{f}(\mathbf{x}) = (f_1(\mathbf{x}), \ldots, f_m(\mathbf{x}))$, we have
\begin{equation*}
    J_{ij}(\mathbf{x}) = (\mathbf{f}'(\mathbf{x})\ehat_j) \cdot \ehat_i = \nabla_{\ehat_j}\mathbf{f}(\mathbf{x}) \cdot \ehat_i = \nabla_{\ehat_j}f_i(\mathbf{x}) = \frac{\partial f_i}{\partial x_j}.
\end{equation*}
The formula for the generic entry $J_{ij}(\mathbf{x})$ completely describes our matrix $\mathbf{J_f}(\mathbf{x})$, though may write it out in full as
\begin{equation*}
    \mathbf{J_f}(\mathbf{x}) = \begin{bmatrix}
        \dfrac{\partial f_1}{\partial x_1} & \dfrac{\partial f_1}{\partial x_2} & \cdots & \dfrac{\partial f_1}{\partial x_n} \\[1em]
        \dfrac{\partial f_2}{\partial x_1} & \dfrac{\partial f_2}{\partial x_2} & \cdots & \dfrac{\partial f_2}{\partial x_n} \\[0.5em]
        \vdots & \vdots & \ddots & \vdots \\[0.5em]
        \dfrac{\partial f_m}{\partial x_1} & \dfrac{\partial f_m}{\partial x_2} & \cdots & \dfrac{\partial f_m}{\partial x_n}
    \end{bmatrix}.
\end{equation*}
This is the explicit form of the Jacobian matrix of $\mathbf{f}$ at $\mathbf{x}$.
Evaluating it at $\mathbf{x} \in \R^n$ gives us an $m \times n$ matrix of real numbers.
From our previous results, we know that differentiability of $\mathbf{f}$ at $\mathbf{x}$ implies the existence of all directional derivatives at $\mathbf{x}$, and by extension, all partial derivatives.
This representation of the derivative $\mathbf{J_f}$ depends on what we choose as our coordinate system.
To provide a coordinate-independent representation, we would have to use differential forms -- further motivating their study.

\end{appendices}

\end{document}