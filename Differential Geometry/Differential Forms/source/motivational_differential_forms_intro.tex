\documentclass[a4paper,12pt]{article}

\usepackage[left=25mm, right=25mm, top=25mm, bottom=25mm]{geometry}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage[style=iso]{datetime2}
\usepackage[explicit]{titlesec}
\usepackage{amsthm}
\usepackage{mathrsfs}
\usepackage{array}
\usepackage{graphicx}
\usepackage{mathtools} % Provides \mathrlap command
\usepackage{xparse}
\usepackage{float}
\usepackage{fixdif}
\usepackage[skip=1em,indent]{parskip}
\usepackage{caption}
\usepackage{tocloft}
\usepackage{enumitem}
\usepackage[toc, page]{appendix}
\usepackage{accents}
\usepackage[perpage, symbol*]{footmisc} % for dagger footnotes
\DefineFNsymbols{sym}{\textdagger \textdaggerdbl \textsection \textparagraph %
{\textdagger\textdagger} {\textdaggerdbl\textdaggerdbl} %
{\textsection\textsection} {\textparagraph\textparagraph}}
\setfnsymbol{sym}
\interfootnotelinepenalty=10000

\usepackage{tikz}
\usepackage{tikz-3dplot}
\tikzset{>=latex} % for LaTeX arrow head
\usepackage{pgfplots} % for the axis environment
\usetikzlibrary{calc,decorations.markings,external}
\tikzexternalize[prefix=tikz/]

\pgfplotsset{compat=1.18, every tick label/.append style={font=\footnotesize}}

\graphicspath{ {./Images/} }

\usepackage[sorting=none]{biblatex}
\addbibresource{motivational_differential_forms_intro.bib}

% ---------------- %
\usepackage{etoolbox}
\usepackage[dvipsnames]{xcolor}
\usepackage[linkcolor=black]{hyperref}
\usepackage[nameinlink]{cleveref}

% Configure cleveref for appendix sections and figures
\crefname{appsec}{Appendix}{appendices}
\Crefname{appsec}{Appendix}{Appendices}
\crefname{figure}{Figure}{Figures}
\Crefname{figure}{Figure}{Figures}


\definecolor{RoyalRed}{RGB}{157, 16, 45}
\definecolor{RoyalOrange}{RGB}{249, 146, 69}

\makeatletter %* Creates a footnote colour option
\def\@footnotecolor{black}
\define@key{Hyp}{footnotecolor}{%
 \HyColor@HyperrefColor{#1}\@footnotecolor%
}
\patchcmd{\@footnotemark}{\hyper@linkstart{link}}{\hyper@linkstart{footnote}}{}{}
\makeatother

\hypersetup{
  colorlinks = true,
  linkcolor = RoyalRed,
  citecolor = RoyalOrange,
  footnotecolor = black,
}
% ---------------- %
\makeatletter
\def\th@plain{%
  \thm@notefont{}% same as heading font
  \itshape % body font
}
\def\th@definition{%
  \thm@notefont{}% same as heading font
  \normalfont % body font
}
\makeatother

\newcommand*\diff{\mathop{}\!d} % for the differential in integrals
% ---------------- %
\ExplSyntaxOn %* This command is for row vectors; unstarred version has no delimiter scaling for inline; starred version has delimiter scaling for display
\seq_new:N \l_user_rvec_seq

% expl3 worker (does the splitting/formatting)
\cs_new_protected:Npn \dbacc_rvec:n #1
 {
  \seq_clear:N \l_user_rvec_seq
  \seq_set_split:Nnn \l_user_rvec_seq { , } { #1 }
  \seq_use:Nn \l_user_rvec_seq { \enspace }
 }

% plain-name wrapper (no ':' in its name) — safe to call outside ExplSyntaxOn
\cs_new_protected:Npn \dbaccrvec #1 { \dbacc_rvec:n { #1 } }

% user-level \rvec interface (star + optional size + mandatory arg)
\NewDocumentCommand \rvec { s o m }
 {
  \IfBooleanTF{#1}
    { \left[\,\dbaccrvec{#3}\,\right] } % starred -> automatic \left...\right
    { \IfValueTF{#2}
        { \mathopen{#2[}\,\dbaccrvec{#3}\,\mathclose{#2]} } % sized optional arg
        { [\,\dbaccrvec{#3}\,] } % default small delimiters
    }
 }
\ExplSyntaxOff
% ---------------- %
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\Z}{\mathbb{Z}}

\NewCommandCopy{\oldIm}{\Im}
\renewcommand{\Im}{\mathop{\oldIm\mathfrak{m}}}
\NewCommandCopy{\oldRe}{\Re}
\renewcommand{\Re}{\mathop{\oldRe\mathfrak{e}}}

\newcommand{\overbar}[1]{\mkern 1.5mu\overline{\mkern-1.5mu#1\mkern-1.5mu}\mkern 1.5mu}

\DeclareMathOperator{\Res}{Res}

\DeclarePairedDelimiterX{\norm}[1]{\lVert}{\rVert}{#1}
\DeclarePairedDelimiterX{\abs}[1]{\lvert}{\rvert}{#1}
\NewDocumentCommand{\eval}{s m m g}{%
  \IfBooleanTF{#1}
    {% starred version - no scaling
      #2 \vert%
      _{#3}%
      \IfNoValueTF{#4}{}{^{#4}}%
    }
    {% unstarred version - auto-scaled
      \left. #2 \right|%
      _{#3}%
      \IfNoValueTF{#4}{}{^{#4}}%
    }%
}
% ---------------- %
\newcommand{\ihat}{\boldsymbol{\hat{\imath}}}
\newcommand{\jhat}{\boldsymbol{\hat{\jmath}}}
\newcommand{\khat}{\boldsymbol{\hat{k}}}
\newcommand{\ehat}{\mathbf{\hat{e}}}
\newcommand{\T}{\mathrm{T}}
% ---------------- %

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}{Corollary}[theorem]

\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}

% \setlength{\cftsecnumwidth}{3em}

\begin{titlepage}
\title{A Motivational Introduction to Differential Forms}
\author{Abdul Musthakin}
\date{November 2025}
\end{titlepage}

% \renewcommand{\thesection}{\Roman{section}}

\allowdisplaybreaks

\setlength{\parindent}{0pt}

\begin{document}
\maketitle

\tableofcontents

\pagebreak

\section*{Prerequisites}

This exposition aims to require as little prerequisite knowledge as possible in order for one be able to understand the presented concepts.
However, there are still some topics that are absolutely essential.
This includes concepts in single and multivariable calculus, as well introductory linear algebra.
The rigorous underpinnings of these topics, such as the contents of a course on real analysis, are not required.
If relevant, they will be attached as an appendix.

\subsection*{1. Linear Algebra}
Expertise in linear algebra is by no means necessary.
A surface-level understanding of the main ideas should suffice.
\begin{itemize}
    \item the intuitive idea of vectors as ordered lists of numbers or arrows in a plane;
    \item elements of $\R^n$ as vectors;
    \item the concept of matrices being linear maps (linear maps in general are important) and representing transformations, as well as basic ideas relating to them;
    \item vector and matrix operations: dot (scalar) product, matrix multiplication, determinents, transpose, inverse of matrix;
    \item the idea of vectors being more general objects (elements of a vector space);
    \item the basis and dimension of a vector space.
\end{itemize}
Besides general knowledge of vectors and linear algebra, the most important concept here to be aware of is that vectors are much more general than coordinates and arrows.
We will encounter objects that are vectors, but are completely different from the afformentioned `typical' examples.
See \cref{appendix:vector_space} for the definition of a vector space, but note that this is only intended to be a reference.

\subsection*{2. Multivariable Calculus}
Knowledge of some concepts in multivariable calculus is mandatory.
Single-variable calculus is obviously a requirement by extension.
Whilst the topic of differential geometry -- where differential forms usually found -- is quite a bit more advanced, this introductory exposition is supposed to be a natural part or extension to the concepts of multivariable calculus.
Thus, full familiarity with the subject is not required; many ideas will be developed along the way.
It assumed that one is aware of
\begin{itemize}
    \item functions of the type $\mathbb{R}^m \to \mathbb{R}^n$;
    \item partial derivatives and the chain rule;
    \item directional derivatives and their computation;
    \item the ideas behind gradients, divergence, and curl, as well as how to compute them;
    \item the Jacobian matrix of a multivariable function.
\end{itemize}
A formal introduction to vector calculus is presented in \cref{appendix:vector calculus}.
This will include the definition of the limit of vectors-valued functions, and the derivation of the Jacobian matrix.
Knoweldge of some concepts from real analysis are required to understand this, but they are not required for the main body of the exposition.
The more experience one has with multivariable calculus, the easier it will be for them to understand this exposition.
Just as importantly, it will better enable to appreciate the value of what is to be introduced.

The last point holds true for any other topic in maths that may be related to differential forms.
This comes both from the perspective of aquiring more knoweldge, as well as gaining experience and maturity.
For instance, abstract algebra (e.g. group theory), is specifically not a prerequisite.
However, knowledge of concepts from it will frequently be useful.
These concepts are intuitive, so they can be mentioned whenever in an informal manner, avoiding the associated jargon.
We encounter isomorphisms between spaces, describing thema as associating objects in a specific way, and so on.

\pagebreak

\section{Introduction \& Motivation}

Understanding what an integral means, in a general sense, is not particularly difficult.
Given a function, it is pretty easy to draw it as a curve on a graph.
By looking at its graph, and choosing some interval on the $x$-axis, we can see the area between the curve and the $x$-axis.
This (signed) area is what is calculated by an integral.

\begin{figure}[!htbp]
\centering
\begin{tikzpicture}
\begin{axis}[
    width=12cm,
    height=6cm,
    axis lines=middle,
    xlabel={$x$},
    ylabel={$y$},
    xmin=0, xmax=6.5,
    ymin=-1.5, ymax=1.5,
    xtick={1.5708, 5.49779},
    xticklabels={},
    ytick=\empty,
    samples=200,
    domain=0:2*pi,
    enlargelimits=false,
    tick style={black, thick},
    major tick length=0.15cm,
]

% Labels for a and b
\node[below] at (axis cs:1.4, 0) {$a$};
\node[below] at (axis cs:5.7, 0) {$b$};

% Shaded area for positive region (a to π)
\addplot[fill=blue!30, opacity=0.5, domain=1.5708:3.14159, draw=none] 
    {sin(deg(x))} \closedcycle;

% Shaded area for negative region (π to b)
\addplot[fill=red!30, opacity=0.5, domain=3.14159:5.49779, draw=none] 
    {sin(deg(x))} \closedcycle;

% The sin(x) function
\addplot[black, thick, domain=0:2*pi] {sin(deg(x))};

% Vertical lines at x=a and x=b
\addplot[dashed, black] coordinates {(1.5708, -1.5) (1.5708, 1.5)};
\addplot[dashed, black] coordinates {(5.49779, -1.5) (5.49779, 1.5)};

% Label for the function
\node[above, black] at (axis cs:4.71239, 1) {$y = f(x)$};

% Labels for areas
\node[blue!70!black, font=\Large] at (axis cs:2.1, 0.45) {$+$};
\node[red!70!black, font=\Large] at (axis cs:4.52, -0.5) {$-$};

\end{axis}
\end{tikzpicture}
\caption{A definite integral}
\label{fig:definite_integral_sin}
\end{figure}

The area illustrated by the above figure is given by
\begin{equation*}
    \int_{a}^{b} f(x) \diff x.
\end{equation*}
Now, everything that was just stated has many asterisks.
We can have functions that cannot be integrated, or at least, not in the `usual' sense.
Even if we can integrate a function, we might not be able to get our integral (and thus desired area) in terms of functions that we are used to.
Even if we ignore all of that, there is plenty of theory regarding how we define integrals -- extending and generalizing to different applications.
So it turns out that integrals are not that simple, but their idea is.

The above refers specifically to definite integrals, and so too will further discussion.
It would thus be useful to have a brief mention of indefinite integrals for the sake of completeness.
An indefinite integral of some function $f$ is itself not a number, nor even a function.
It is a class of functions,\footnote{An equivalence class.} whose shared property is that their derivative is $f$.
A member of this class is referred to as an antiderivative of $f$, and it can be written as
\begin{equation*}
    F(x) \coloneq c + \int_{a}^{x} f(t) \diff t,
\end{equation*}
where $a$ and $c$ are some constants.
Since any antiderivative (and thus the indefinite integral) of a function can be rewritten in terms of a definite integral, there is nothing regarding the former concept that warrants its mention in the discussions ahead.

Consider the notation we use for integrals.
We have a unique symbol, $\int$, to always know when we are integrating.
We have the integrand after that, which is what we are integrating.
Also, we have the start and end points of our interval which we are integrating over.
Finally, we have something on the end.
It is definitely not useless, as it tells us what variable we are integrating with respect to.
For instance, it is true that
\begin{equation*}
    \int_{a}^{b} f(x) \diff x = \int_{a}^{b} f(t) \diff t = \int_{a}^{b} f(u) \diff u,
\end{equation*}
just as it is true that
\begin{equation*}
    \sum_{i=1}^{n} f(i) = \sum_{j=1}^{n} f(j) = \sum_{k=1}^{n} f(k).
\end{equation*}
The exact symbol we use for the variable does not matter; they are just labels for the same thing.
Now, in the context of sums, you can simply expand out to show that they are the same thing.
They all refer to $f(1) + f(2) + f(3) + \ldots + f(n)$.
It is not so straightforward for integrals, but the idea holds.
We could leave it at that, and say that the differential is just a part of the notation that conveys some useful information.
There is much more to it than that, however.

The word `differential' has many meanings in mathematics, and it can refer to completely different concepts in different fields (or even in the same field).
One thing that it can refer to is an infinitesimal -- a number closer to zero than any real number that is not itself zero.
Leibniz was the first to coin the term for that usage, although the idea of infinitesimals date back to \href{https://plato.stanford.edu/entries/continuity/}{ancient Greek mathematicians}.

Early developments in calculus throughout the 17th and 18th centuries by the likes of Newton and Leibniz were noticeably reliant on the idea of infinitesimals.
That itself was not a bad thing, but that the lack of rigour was.
We would see the development of a more rigorous foundation of calculus by mathematicians in the 19th century, particularly Cauchy.
Infinitesimals would be left in favour of limits, and the latter is the formulation that is universally taught in schools.

\begin{figure}[!htbp]
\centering
\begin{tikzpicture}
\begin{axis}[
    width=12cm,
    height=7cm,
    axis lines=middle,
    xlabel={$x$},
    ylabel={$y$},
    xmin=0, xmax=10,
    ymin=0, ymax=100,
    xtick={2, 7},
    xticklabels={$a$, $b$},
    ytick=\empty,
    samples=100,
    domain=0:10,
    enlargelimits=false,
    tick style={black, thick},
    major tick length=0.15cm,
    xticklabel style={font=\normalsize},
]


% Draw Riemann sum rectangles (left endpoint)
\foreach \i in {0,...,9} {
    \pgfmathsetmacro{\xleft}{2 + \i*0.5}
    \pgfmathsetmacro{\xright}{\xleft + 0.5}
    \pgfmathsetmacro{\height}{-15*(\xleft-5) + (\xleft-5)^3 + 50}
    \addplot[fill=blue!20, draw=blue!60!black, thick] coordinates {
        (\xleft, 0) (\xleft, \height) (\xright, \height) (\xright, 0)
    } -- cycle;
}

% The curve
\addplot[black, very thick, domain=0:10] {-15*(x-5) + (x-5)^3 + 50};

% Label for the function
\node[above right, black] at (axis cs:7, 65) {$y = f(x)$};

\end{axis}
\end{tikzpicture}
\caption{Riemann sum approximation of an integral}
\label{fig:riemann_sum}
\end{figure}

This information about differentials allows us to view the one attached to every integral in way beyond just useful notation.
Putting the formalism and rigorous definitions of integrals aside, they are fancy sums.
The area under a graph can be approximated by rectangles, as is shown by the above figure.
As the width of the rectangles decrease, the sums \href{https://commons.wikimedia.org/wiki/File:Riemann_sum_(leftbox).gif}{get closer} to the actual area under the curve.
That is, the limit of this sum is an integral.

Instead of thinking about it in terms of limits, we can imagine summing an infinite number of rectangles with infinitesimal width.
The height of a rectangle at some value of $x$ can be given by $f(x)$, so if the integral sums over areas, the $dx$ is the infinitesimal width.
This is all very imprecise and non-technical language.
However, it is enough to give us an intuitive feel of what an integral is and what the differential that is always attached to it means.

There is more to the peculiar symbol, though, and integrals are probably not the first place one would have spotted them.
That would instead be with regard to derivatives.
Indeed, the notation $dy/dx$ -- also attributed to Leibniz -- is very commonly used to denote the derivative of $y$ with respect to $x$.
When it is first taught to students at school, the warning will be given that it does not represent a fraction.
It just looks like a fraction, and treating it like a fraction works out almost all the time.

The resemblence it has to a ratio between two quantities is no coincidence, as Leibniz intended it to be just that: a ratio of infinitesimals.
The symbols $dx$ and $dy$ were used to represent infinitely small changes in $x$ and $y$ respectively.
Whilst we have, for the most part, left these imprecise notions behind, they will prove to be somewhat useful in understanding these symbols.\footnote{At the end of the day, the symbols mean whatever we want as long as it consistent with the rest of our maths. That is why I gave the easy way out at the beginning, saying that the differential symbol in the integral can just be considered a part of the notation. However, as is being demonstrated, we can go further.}

\section{Differential of a Function}

Firstly, consider the fact that the derivative has many different notations.
The derivative of a function $f$ at $x$ may be expressed as $f'(x)$.
It follows that
\begin{equation*}
    \frac{dy}{dx} = f'(x).
\end{equation*}
This is just equating two things that are defined to mean the exact same thing.
Now, although the left-hand side is not a fraction, what if we rewrote the equation as follows?
\begin{equation*}
    dy = f'(x) \diff x
\end{equation*}
The right-hand side now looks exactly like an integrand.
This looks promising, but we did say beforehand that $dy/dx$ is not a fraction.
So, are we allowed to do that?
No. The symbols $dx$ and $dy$ have not been given a meaning outside of when they appear in $dy/dx$, so it makes no sense to separate them.
However, when something does not have meaning, we can always attempt to give it one and see where that leads us.

First, let $\Delta x$ be some real number, just as $x$ is.
Then, we can call $df$ the differential of $f$, a function of $x$.
The differential itself is a function of both $x$ and $\Delta x$, is defined as
\begin{equation*}
    df(x, \Delta x) \coloneq f'(x) \Delta x.
\end{equation*}
What was the point of that?
We went from performing an algebraic manipulation in a situation where it does not make sense, to creating a perfectly fine definition.
Since we have made it clear that $df$ is a function of two variables, we can write $df(x, \Delta x)$ as $df(x)$ or even $df$ (we are not changing the meaning).
The differential of the function $f(x) = x$ is given by
\begin{equation*}
    dx = 1 \cdot \Delta x = \Delta x
\end{equation*}
for all values of $x$.
This means that, even though $dx$ is technically still a function of $x$, we can just write $dx = \Delta x$.
With this, and by letting $y = f(x)$, we can rewrite our equation as
\begin{equation*}
    dy = f'(x) \diff x.
\end{equation*}
This looks exactly like what we already had, but now we have reached it by valid means.
We see that the differential $dy$ is defined in terms of $x$ and $dx$, and the differential $dx$ is essentially just some real number.
If we desired, we could go further to obtain $dy/dx = f'(x)$, but this can lead to confusion.
By that equation, $f'(x)$ equals a ratio of differentials that have each been separately defined.
However, $f'(x)$ is can be expressed with Leibniz's notation as $dy/dx$ -- which is not a ratio.\footnote{We cannot redefine the derivative to be a ratio of our two differentials. Their definitions involve the derivative, so it would be circular. Instead, view it as evidence for the self-consistency of Leibniz's notation}
This is just something to note.

We have now obtained our third interpretation of the differential, a function of two real numbers $x$ and $\Delta x$.
This can be extended to higher-order differentials, as well as to the differentials of multivariable functions.
Its usefulness is in the fact that it represents the principal part of the change of a function.
To see what that exactly means, consider some function\footnote{Technically, $f$ is the function and $f(x)$ is the value of the function at $x$.} $y=f(x)$ between two points $x$ and $x + \Delta x$.
Let $x$ be fixed.
The change in $x$ values is $\Delta x$ and the change in $y$ values is $\Delta y \coloneq f(x + \Delta x) - f(x)$.

By the definition of the derivative, $\Delta y / \Delta x \to f'(x)$ as $\Delta x \to 0$.
This means that for small $\Delta x$, we have $\Delta y / \Delta x \approx f'(x)$.
If the difference (or error) is $\varepsilon$, then $\Delta y / \Delta x = f'(x) + \varepsilon$.
Rearranging gives us
\begin{align*}
    \Delta y &= (f'(x) + \varepsilon) \Delta x \\
    &= f'(x) \Delta x + \varepsilon \Delta x \\
    &= dy + \varepsilon \Delta x.
\end{align*}
The change in $y$ is equal to a linear function of $\Delta x$, $dy$, plus a (generally) nonlinear error term.
Thus, $dy$ is the linear (or principal) part of $\Delta y$.
As $\Delta x \to 0$, we require that $\varepsilon \Delta x / \Delta x = \varepsilon \to 0$, i.e. the error term shrinks to zero faster than $\Delta x$.
This means that $\Delta y \to d y$, and $\Delta y \approx dy$ for small $\Delta x$.
An equivalent way to view the differential is as the best linear approximation to a function at some point.

Let us look at one use case of this: approximating the values of functions.
What is the value of $\sqrt{90}$?
Since the function $y = \sqrt{x}$ is strictly increasing, we know that $\sqrt{90}$ will be between $\sqrt{81}=9$ and $\sqrt{100}=10$.
Let $x = 100$ and $\Delta x = -10$, so that $x + \Delta x = 90$.
Thus, $\Delta y = \sqrt{90} - \sqrt{100} = \sqrt{90} - 10$.
This means that $10 - \Delta y$ is the exact value of $\sqrt{90}$, but we do not know what $\Delta y$ is.
Our approximation from before comes in handy here.

The derivative of $\sqrt{x}$ is given by
\begin{equation*}
    f'(x) = \frac{1}{2\sqrt{x}}.
\end{equation*}
At $x=100$, we have $f'(x) = 1/20 = 0.05$.
This is all we need for our approximation.
\begin{align*}
    \sqrt{90} &= \Delta y + 10 \\
    &\approx dy + 10 \\
    &= f'(10) \Delta x + 10 \\
    &= 0.05 \cdot -10 + 10 \\
    &= 9.5
\end{align*}
The actual value of $\sqrt{90}$ is $9.47213595\ldots$, so we were just 0.62\% off.
The computations were incredibly easy, which is the case as long as we have nice values of the function and its derivative to work with.
We can illustrate the efficacy of this method with a diagram.

\begin{figure}[!htbp]
\centering
\begin{tikzpicture}
\begin{axis}[
    width=12cm,
    height=7cm,
    axis lines=middle,
    xlabel={$x$},
    ylabel={$y$},
    xmin=0, xmax=196,
    ymin=0, ymax=14,
    xtick={90, 100},
    xticklabels={90, 100},
    ytick={sqrt(90)},
    yticklabels={$\sqrt{90}$},
    samples=100,
    domain=0:196,
    enlargelimits=false,
    tick style={black, thick},
    major tick length=0.15cm,
    xticklabel style={font=\small},
]

\addplot[red, very thick, domain=30:170] {x/(2*sqrt(90)) + (sqrt(90) - 90/(2*sqrt(90)))};

% The curve
\addplot[black, very thick, domain=0:196] {sqrt(x)};

\draw[fill] (axis cs:{90,sqrt(90)}) circle [radius=1.5pt];
\addplot[black, semithick, dashed, domain=0:100] {sqrt(90)};
\addplot[black, semithick, dashed, domain=0:90] (90, {sqrt(x)});

\addplot[black, semithick, dashed, domain=0:100] (100, {sqrt(x)});

% Label for the function
\node[above right, black] at (axis cs:150, 9) {$y = f(x)$};

\end{axis}
\end{tikzpicture}
\caption{Linear approximation of $\sqrt{90}$}
\label{fig:linaer_approximation}
\end{figure}

We calculated $dy$, the increase in the tangent, as an approximation to $\Delta y$, the increase in the function.
As can be seen from the figure, these are very similar values.
Since $f'(x) \to 0$ as $x \to \infty$ (the curve flattens), this method of approximation gets better for larger square roots.
That does not mean that this method is not useful for other functions, as it is generally applicable to functions that are sufficiently `nice' -- where a discussion on `niceness' is beside the point.

Another example is approximating $\ln 3$.
We know that $\ln e$ is 1, and $e \approx 3$.
Let $x = e$ and $\Delta x = 3 - e$, so that $x + \Delta x = 3$.
Then, $\Delta y = \ln 3 - \ln e = \ln 3 - 1$.
The derivative of $\ln x$ is simply $1/x$, so we have
\begin{equation*}
    \ln 3 \approx \frac{1}{e} \cdot (3 - e) + 1 = \frac{3}{e}.
\end{equation*}
The number $e$ is well-known enough for us to use its value for the approximation, giving us $\ln 3 \approx 1.104$.
The actual value is $1.09861229\ldots$, which means that there is a 0.46\% error.
These approximations are known as linear approximations, and they have a fair bit of applications.
The idea of calculating the linear part of $\Delta y$ can be made more precise with \emph{Taylor's theorem}, though it is not necessary to understand the method itself.

\section{Reinterpreting Formulae}

Now that we have a formal definition of a differential, it would be useful to use what we have to look at commonly-known formulae in this new context.
First is the chain rule for derivatives.
Given two functions $f$ and $g$, the derivative of their composition $f \circ g$ is given by
\begin{equation*}
    f'(g(x)) g'(x).
\end{equation*}
If we write $y = f(g(x))$ and $u = g(x)$, then the previous fact can be rewritten as
\begin{equation*}
    \frac{dy}{dx} = \frac{dy}{du} \frac{du}{dx}
\end{equation*}
using Leibniz's notation.
The latter way is writing is more elegant both in this case, and for compositions of more than two functions.
It reinforces the idea of the derivative being a fraction, even though it is not.
If we were to, however, consider the terms on the RHS as being differentials, then we do have fractions -- which can cancel.
\begin{align*}
    \frac{dy}{du} \frac{du}{dx} = \frac{y'(x) \diff x}{u'(x) \diff x} \frac{u'(x) \diff x}{dx} = y'(x)
\end{align*}
Of course this is not a proof of the chain rule.
It does not show that the fraction $dy/du$ is the same as the derivative of $y$ with respect to $u$, and it does not work when $u'(x) = 0$ wherever we are differentiating at.
Regardless, it shows the chain rule to be intuitively very simple -- just `cancelling' fractions.

Our definition of differentials can also be shown to be insightful with regard to integration by substitution and the method of solving differential equations by separation of variables.
That would make sense, as they are both linked to the chain rule (with the former being an inverse to some degree).
Given two functions $f$ and $g$, the integration by substitution formula states that
\begin{equation*}
    \int_{a}^{b} f(g(x))g'(x) \diff x = \int_{g(a)}^{g(b)} f(u) \diff u.
\end{equation*}
By making the substitution $u = g(x)$, we know that the differential of $u$ is given by $du = g'(x) \diff x$.
This suggests that the formula above is a literal substitution of one variable with another.
The limits being transferred from $x=a$ and $x=b$ to $u=g(a)$ and $u=g(b)$ make sense as well.\footnote{We could have had the limits on the RHS integral be $x=a$ and $x=b$ if we really wanted. This shows that nothing `actually' changes between the two integrals.}
Without the integral symbols, the equation would just be something that follows directly from the definition of a differential.
If it were possible to integrate a differential in the sense of it being the whole integrand -- not just integrating a function -- then the formula would be completely trivial.

Now, on to the topic of differential equations.
Let $f$, $g$, and $h$ be functions that satisfy the differential equation
\begin{equation*}
    \frac{dy}{dx} = g(x)h(y),
\end{equation*}
with $y=f(x)$.
If we treat the $dy$ in $dy/dx$ as a differential (which is a bit awkward notationally but valid), then we have a fraction that can be separated.
Multiplying both sides by $dx$ and dividing by $h(y))$ gives
\begin{equation*}
    \frac{dy}{h(y)} = g(x) \diff x.
\end{equation*}
This is really the same as
\begin{equation*}
    \frac{f'(x)}{h(f(x))} \diff x = g(x) \diff x.
\end{equation*}
Since $dx$ is some real number, we can cancel it out and then integrate both sides with respect to $x$ in order to solve the equation.
However, our integrals will have $dx$ at the end anyway.
What was the point of cancelling them out beforehand?
The $dx$ in an integral is still different from the $dx$ we have defined to be equal to some real number $\Delta x$.
Once again, if we could just integrate the differentials directly, things would be much simpler.

\section{Generalizing Differentials}

As was suggested at the end of the previous section, there is a different way to handle differentials that works well with integration.
Before that, we will have to perform quite a bit of work to generalize the notion of a differential; our current definition only works for functions of the type $\R \to \R$.
We will first cover higher-order differentials, which correspond to higher-order derivatives.
This is technically not a `generalization' in that it requires no new definitions.
The second-order differential, or second differential, of a function is given by finding the differential of the differential.
\begin{equation*}
    d^2 y \coloneq d(dy) = d(f'(x)\diff x) = (df'(x)) \diff x = f''(x) (dx)^2
\end{equation*}
Dividing through by $(dx)^2$ gives an equation very similar to Leibniz's notation for higher-order derivatives.
However, note that it is not a definition (as it would be if we were considering it as just notation).
\begin{equation*}
    \frac{d^2 y}{(dx)^2} = f''(x)
\end{equation*}
This is an interesting detail, again showing the efficacy and practicality of the notation.
Repeated applications of the differential gives us the general formula for the $n$-th order differential of $f$.
\begin{equation*}
    d^n y = d^n f(x, dx) = f^{(n)}(x) (dx)^n,
\end{equation*}
where $f^{(n)}$ is the $n$-th derivative of $f$.
This is pretty simple, but it only works under the assumption that $dx$ is itself not a function of $x$.
This might not be the case.
An example where $dx$ depends on $x$ is if $x$ depends on some parameter $t$.
The first-order differential is still given by $df(x, \Delta x) = f'(x) \Delta x = f'(x)\diff x$.
Since $x$ depends on $t$, $\Delta x$ depends on $t$ as well.\footnote{This is not a logical consequence, but rather a sensible choice we make. We choose to define $\Delta x$ as an actual change $x(t+\Delta t) - x(t)$, now that we are able to, instead of it just being some real number which represents the concept of change only when put in context.}
The principal part of the change $\Delta x$ is given by $dx = x'(t) \diff t$.
The reasoning for this is not new, as we are simply applying our definition of the differential to $x$ now that it is a function of $t$.
Then, $df$ is given by
\begin{equation*}
    df = f'(x)x'(t) \diff t.
\end{equation*}
This is essentially just the chain rule applied to differentials.
Finding $d^2 f$ will involve differentiating with respect to $t$ instead of $x$ to avoid concerning ourselves with whether $dx$ can be expressed as a function of $x$.
For this, we need the product rule.
\begin{equation*}
    d^2 f = f''(x) (x'(t)\diff t)^2 + f'(x) x''(t) (dt)^2
\end{equation*}
Simplifying this using $dx = x'(t)\diff t$ and $d^2 x = x''(t) (dt)^2$ gives us
\begin{equation*}
    d^2 f = f''(x) (dx)^2 + f'(x) d^2 x.
\end{equation*}
Note that differentiating with respect to $x$ directly would still have given us the same answer (even if it might not be technically valid).
This result generalizes to third derivatives, fourth derivatives, and so on.
The general formula for the $n$-th order differential is not at all insightful, so it will not be provided.
Higher-order differentials in general are not particularly relevant to the rest of our discussion, but they have been included for completeness.

Now we will consider multivariable functions.
Let $f\colon \R^n \to \R$, so it maps $n$ real numbers to a single real number.
It can equivalently be considered a function of $n$ independent variables, and we may write $y = f(x_1, \ldots, x_n)$.
There are multiple sensible ways of defining the differential.
We will first cover the simpler of the two approaches, before we cover the second and show that they are actually the same thing.
The total change in $y$ is given by
\begin{equation*}
    \Delta y = \Delta f \coloneq f(x_1 + \Delta x_1, \ldots, x_n + \Delta x_n) - f(x_1, \ldots, x_n),
\end{equation*}
where $\Delta x_i$ is a real number.
We are able to split this change into a linear (principal) part and a nonlinear part (which we call the error).
Each $x_i$ will contribute to the total change, both in its principal part and error part.
The former is given by
\begin{equation*}
    \frac{\partial y}{\partial x_i} \Delta x_i,
\end{equation*}
which we will calse the partial differential of $y$ with respect to $x_i$.
The error is simply $\varepsilon_i \Delta x_i$.
This is a natural extension of the one-dimensional case, with the primary difference being that we have now many more terms.
We can find the total principal change in $y$ due to the changes in all the variables by summing the partial differentials.
The result of this sum will be called the total differential of $y$.
\begin{equation*}
    dy \coloneq \sum_{i=1}^{n} \frac{\partial y}{\partial x_i} \Delta x_i
\end{equation*}
Since $dx_i(\Delta x_1, \ldots, \Delta x_n) = \Delta x_i$ (the partial derivative equals one for this term and zero for all others), we can rewrite the differential as
\begin{equation*}
    dy = \sum_{i=1}^{n} \frac{\partial y}{\partial x_i} \diff x_i.
\end{equation*}
As all $\Delta x_i \to 0$, we have $\varepsilon_i \to 0$, so $dy \to \Delta y$.
Just as in the case of one variable, this is approximately equal to the actual change in $y$: $\Delta y \approx \diff y$ (for small changes in $x_i$).
Their use cases are also similar.
However, their increased generality makes them much more applicable to situations -- including estimating error in measurements.

Since our function $f$ takes a list of numbers, and a list of numbers can be represented by a vector, we can consider $f$ to map $m$-dimensional vectors to real numbers.
This will not change anything about the above formula; we are now just considering the components of $\mathbf{x}$ and $d\mathbf{x}$.
We can go a step further by consider functions that output vectors as well.
Let $\mathbf{f}\colon \R^m \to \R^n$.
The only thing that changes is that we are now differentiating vectors, which we do by differentiating their components separately.
This gives us a slightly modified formula for the total differential.
It is just a change of notation.
\begin{equation*}
    d\mathbf{f} = \sum_{i=1}^{n} \frac{\partial\mathbf{f}}{\partial x_i} \diff x_i
\end{equation*}
When we generalize to vector-valued functions, the changes in our function are no longer a real number.
It thus makes sense that the differential is not a real number anymore, either.
Functions that map vectors to vectors will be as general as we go for now.
Although, as we stated before, we will offer a different perspective on what we just derived.
Once again, consider $\mathbf{f}\colon \R^m \to \R^n$, with $\mathbf{x}$ and $\Delta\mathbf{x}$ being in $\R^m$.
The total change in $\mathbf{f}$ is given by
\begin{equation*}
    \Delta \mathbf{f} = \mathbf{f}(\mathbf{x} + \Delta\mathbf{x}) - \mathbf{f}(\mathbf{x}).
\end{equation*}
This is a rewritten version of what we had before.
We want this total change to be equal to a linear change, as well as an error term.
Note that we had $\Delta f = df + \varepsilon\Delta x$ in the one-dimensional case, with $df = f'(x) \diff x$.
Our multivariable version of the derivative is the Jacobian matrix, $\mathbf{J_f}$.
The linear change is then $\mathbf{J_f}\Delta\mathbf{x}$.
The error term is given by\footnote{It is not $\norm{\boldsymbol{\varepsilon}}\Delta\mathbf{x}$ instead because that would give an $n$-dimensional vector whilst the other terms are $m$-dimensional.} $\norm{\Delta\mathbf{x}}\boldsymbol{\varepsilon}$, with $\boldsymbol{\varepsilon} \to \mathbf{0}$ as $\Delta\mathbf{x} \to \mathbf{0}$.
We thus know that the total change in $\mathbf{f}$ can be expressed as follows.
\begin{equation*}
    \Delta\mathbf{f} = \mathbf{J_f}\Delta\mathbf{x} + \norm{\Delta\mathbf{x}}\boldsymbol{\varepsilon}
\end{equation*}
The differential of $\mathbf{f}$ is given by
\begin{equation*}
    d\mathbf{f}(\mathbf{x}, \Delta\mathbf{x}) = \mathbf{J_f}\Delta\mathbf{x}.
\end{equation*}
For $\mathbf{f}(\mathbf{x}) = \mathbf{x}$, it can be shown that the Jacobian $\mathbf{J_f} = \mathbf{I}$, the identity matrix.
This means that $d\mathbf{x} = \mathbf{I}\Delta\mathbf{x} = \Delta\mathbf{x}$ for all $\mathbf{x}$.
We can therefore rewrite the differential of $\mathbf{f}$ as
\begin{equation*}
    d\mathbf{f} = \mathbf{J_f} \diff\mathbf{x}.
\end{equation*}
This differential is exactly the same as the total differential we previously derived.
To show this, we simply perform the matrix multiplication.
\begin{equation*}
    \mathbf{J_f} \diff\mathbf{x} = \begin{bmatrix}
        \dfrac{\partial f_1}{\partial x_1} & \cdots & \dfrac{\partial f_1}{\partial x_n} \\[0.5em]
        \vdots & \ddots & \vdots \\[0.5em]
        \dfrac{\partial f_m}{\partial x_1} & \cdots & \dfrac{\partial f_m}{\partial x_n}
    \end{bmatrix}
    \begin{bmatrix}
        dx_1 \vphantom{\dfrac{\partial f_1}{\partial x_n}}\\[0.5em]
        \vdots \\[0.5em]
        dx_n \vphantom{\dfrac{\partial f_m}{\partial x_n}}
    \end{bmatrix}
    = \begin{bmatrix}
        \dfrac{\partial f_1}{\partial x_1}\diff x_1 + \cdots + \dfrac{\partial f_1}{\partial x_n}\diff x_n\\[0.5em]
        \vdots \\[0.5em]
        \dfrac{\partial f_m}{\partial x_1}\diff x_1 + \cdots + \dfrac{\partial f_m}{\partial x_n}\diff x_n
    \end{bmatrix}
    = \sum_{i=1}^{n} \frac{\partial\mathbf{f}}{\partial x_i} \diff x_i
\end{equation*}
Whilst the two results are equivalent, the form involving the Jacobian is usually preferred.
It is much more compact and clear (as long as the definition and notation is understood).
The Jacobian better represents the nature of a derivative of as a linear map and not necessarily a number.
As a bonus, $d\mathbf{f} = \mathbf{J_f} \diff\mathbf{x}$ maintains a similar form to the one-dimensional version of the differential.
The other form may be better-suited for quick computations, however.

We have now obtained an interpretation of the differential for a more general class of functions. 
It is a function of $\mathbf{x}$ and $d\mathbf{x}$, representing the principal change in the function.
This generalization, and the work done to formalize the derivative of functions of type $\R^m \to \R^n$, falls under the concept of the \href{https://en.wikipedia.org/wiki/Fr%C3%A9chet_derivative}{Fre\'chet derivative}.
This derivative can be defined on even more general spaces, but that is beyond the scope of our discussion.

A useful concept that is related to -- but distinct from -- our definition of the differential is the directional derivative.
\begin{equation*}
    \nabla_\mathbf{v}\mathbf{f}(\mathbf{x}) \coloneq \lim_{t \to 0} \frac{\mathbf{f}(\mathbf{x}+t\mathbf{v}) - \mathbf{f}(\mathbf{x})}{t}
\end{equation*}
Here, $\nabla_\mathbf{v}\mathbf{f}$ is the directional derivative of $\mathbf{f}$ along some vector $\mathbf{v}$.
This is a weaker concept of the derivative than what we were previously using.
If function $\mathbf{f}$ is differentiable at a point $\mathbf{x}$ (the Jacobian exists), then all directional derivatives at that point will exist.
The converse of this statement is not true.
The relationship between the directional derivative and the Jacobian is that $\mathbf{J_f v} = \nabla_\mathbf{v}\mathbf{f}(\mathbf{x})$, meaning that $d\mathbf{f}(\mathbf{x}, \mathbf{v}) = \nabla_\mathbf{v}\mathbf{f}(\mathbf{x})$.
This will be useful later.

\section{Tangent Spaces}

In order to move on to the next step (of developing a `differential' that works well with integration), we will have to shift our perspective and create new definitions.
Previously, we have been considering $d\mathbf{f}$ as a function of two numbers, $\mathbf{x}$ and $\Delta\mathbf{x}$.\footnote{Henceforth, we will refrain from using $d\mathbf{x}$ in the context of the differential, as it will adopt a different meaning.}
Another way to view it is to consider the differential at a specific point $\mathbf{x}$.
We then have $d\mathbf{f_x}$ as a function of just $\Delta\mathbf{x}$.
All we have done here is fix one of our variables.

The next step will be to reinterpret $\Delta\mathbf{x}$.
It currently just represents some number which can be thought of as a change from $\mathbf{x}$.
Whilst it can be of any size, in practice, we want it to be small.
The smaller it is, the close the principal change $d\mathbf{f}$ is to the total change $\Delta\mathbf{f}$.
Although the following language is not precise, we can say that these changes would be equal if $\Delta\mathbf{x}$ was infinitely small.
A point established near the start of this discussion was that the concept of infinitesimals is dated.
However, instead of abandoning them completely, we will show one way in which they can be formalized.

Whilst the concept of an infinitesimal itself is vague (as is), it is closely related to tangents.
Regardless of whether a derivative is interpreted as the ratio of infinitesimals, or as a limit, it can be thought of as the gradient of a tangent line.
Quantities that are derivatives of other quantities are always tangent to them.
For example, the instantaneous velocity vector $\mathbf{v} = \mathbf{\dot{x}}$ is tangent to the position vector $\mathbf{x}$.
Also, in the definition of the derivative, the limit can be viewed as the gradient of a secent line approaching the gradient of a tangent line.
With this information, we can reinterpret an infinitesimal change as a tangent.
Thus, $\Delta\mathbf{x}$ can be considered a tangent vector.

The differential $d\mathbf{f_x}$ can now be considered a function of tangent vectors.
Its domain is the set of all such tangent vectors, and we will call this the tangent space.
Specifically, it is the tangent space of $\R^m$ at the point $\mathbf{x}$, and we denote it by $T_\mathbf{x}\R^m$.
What actually are the tangent vectors here?
To see that, it would be helpful to simplify to the plane $\R^2$ first.
Although all the elements of $\R^2$ line flat on the plane, they do not -- in general -- start at $\mathbf{x}$.
There is, however, an assoication that can be made between $\R^2$ and $T_\mathbf{x}\R^2$; the tangent space can be thought of as a copy of $\R^2$ attached to a particular point.
This generalizes to $\R^m$.

It would seem that the notion of a tangent space is not particularly useful.
That is the case if we look at nothing but $T_\mathbf{x}\R^n$.
Tangent spaces work with a much more general class of objects called \emph{manifolds}.
We will not get too technical with manifolds in this discussion.
An informal definition of a manifold is an object that resembles Euclidean space ($\R^n$) near each point.
For instance, a circle is a manifold.
We denote the unit circle by $S^1$.
The tangent space of $S^1$ at some point $p$, $T_pS^1$, is simply the space of all the vectors that lie on the line tangent to $p$.

\begin{figure}[!htbp]
\centering
\begin{tikzpicture}[scale=1.4]
    % Draw the unit circle
    \draw[black, thick] (0,0) circle (2cm);
    
    % Mark the point in the first quadrant (at 45 degrees)
    \pgfmathsetmacro{\angle}{45}
    \pgfmathsetmacro{\px}{2*cos(\angle)}
    \pgfmathsetmacro{\py}{2*sin(\angle)}
    
    % Draw the tangent line
    \draw[red, thick] ({\px - 1.5*sin(\angle)}, {\py + 1.5*cos(\angle)}) -- 
                       ({\px + 1.5*sin(\angle)}, {\py - 1.5*cos(\angle)});
    
    % Draw the point
    \filldraw (\px,\py) circle (2pt) node[above right] {$p$};
    
    % Draw axes
    \draw[-latex] (-2.5,0) -- (2.5,0) node[right] {$x$};
    \draw[-latex] (0,-2.5) -- (0,2.5) node[above] {$y$};
\end{tikzpicture}
\caption{Tangent space of $S^1$ at point $p$}
\label{fig:circle_tangent}
\end{figure}

Similarly, the tangent space of a sphere at $p$, $T_pS^2$ is the space of all the vectors that lie on the plane tangent to the sphere at $p$.
This is shown in \cref{fig:sphere_tangent}.
An interesting thing to note, and the reason why a circle is $S^1$ and a sphere is $S^2$, is that these are one- and two-dimensional objects respectively.
A way of seeing this is by noting that a circle resembles a line near a point, and a sphere resembles a plane near a point.
This is not what people are usually taught when they first learn about shapes.
Both interpretations are correct \cite{shape_dimension_convention} depending on the convention we have for what the dimension of a shape is.

\begin{figure}[htbp]
\centering
\tdplotsetmaincoords{70}{110}

\begin{tikzpicture}[scale=1.2]
    %--- 2D shaded "sphere" (visual only) ---
    \def\R{2.5} % sphere radius
    \shade[ball color=white!90!blue, opacity=0.85] (0,0) circle (\R);

    %--- 3D drawing using tikz-3dplot ---
    \begin{scope}[tdplot_main_coords]

        % coordinates of point p on sphere: p = R * (1,1,1)/sqrt(3)
        \pgfmathsetmacro{\pval}{\R/sqrt(3)}
        \coordinate (O) at (0,0,0);
        \coordinate (P) at (\pval,\pval,\pval);

        % Build an orthonormal tangent basis at P:
        % u = (1,-1,0)/sqrt(2)
        \pgfmathsetmacro{\ux}{1/sqrt(2)}
        \pgfmathsetmacro{\uy}{-1/sqrt(2)}
        \pgfmathsetmacro{\uz}{0}

        % n = (1,1,1)/sqrt(3)  (unit normal)
        \pgfmathsetmacro{\nx}{1/sqrt(3)}
        \pgfmathsetmacro{\ny}{1/sqrt(3)}
        \pgfmathsetmacro{\nz}{1/sqrt(3)}

        % v = n x u  (already unit length because n and u are unit & orthogonal)
        \pgfmathsetmacro{\vx}{\ny*\uz - \nz*\uy} % = 1/sqrt(6)
        \pgfmathsetmacro{\vy}{\nz*\ux - \nx*\uz} % = 1/sqrt(6)
        \pgfmathsetmacro{\vz}{\nx*\uy - \ny*\ux} % = -2/sqrt(6)

        % plane half-size (controls how large the drawn patch is)
        \pgfmathsetmacro{\psize}{1.6}

        % compute plane corners: P + s*(±u ± v)
        \pgfmathsetmacro{\Ax}{\pval + \psize*(-\ux - \vx)}
        \pgfmathsetmacro{\Ay}{\pval + \psize*(-\uy - \vy)}
        \pgfmathsetmacro{\Az}{\pval + \psize*(-\uz - \vz)}
        \coordinate (A) at (\Ax,\Ay,\Az);

        \pgfmathsetmacro{\Bx}{\pval + \psize*( \ux - \vx)}
        \pgfmathsetmacro{\By}{\pval + \psize*( \uy - \vy)}
        \pgfmathsetmacro{\Bz}{\pval + \psize*( \uz - \vz)}
        \coordinate (B) at (\Bx,\By,\Bz);

        \pgfmathsetmacro{\Cx}{\pval + \psize*( \ux + \vx)}
        \pgfmathsetmacro{\Cy}{\pval + \psize*( \uy + \vy)}
        \pgfmathsetmacro{\Cz}{\pval + \psize*( \uz + \vz)}
        \coordinate (C) at (\Cx,\Cy,\Cz);

        \pgfmathsetmacro{\Dx}{\pval + \psize*(-\ux + \vx)}
        \pgfmathsetmacro{\Dy}{\pval + \psize*(-\uy + \vy)}
        \pgfmathsetmacro{\Dz}{\pval + \psize*(-\uz + \vz)}
        \coordinate (D) at (\Dx,\Dy,\Dz);

        % draw the tangent plane patch
        \filldraw[fill=red!20,opacity=0.6,draw=red!60] (A) -- (B) -- (C) -- (D) -- cycle;

        % optional: draw the plane edges a bit thicker and semi-transparent
        \draw[red!60,very thick,opacity=0.6] (A) -- (B) -- (C) -- (D) -- cycle;

        % draw the great circle (equator in z=0 plane)
        % Front half (solid) - top half, y > 0
        \draw[thick,domain=-75:100,samples=40,variable=\t]
            plot ({\R*cos(\t)},{\R*sin(\t)},0);
        % Back half (dashed) - bottom half, y < 0
        \draw[dashed,thick,domain=90:290,samples=40,variable=\t]
            plot ({\R*cos(\t)},{\R*sin(\t)},0);

        % 3D axes
        \draw[-latex] (0,0,0) -- (7,0,0) node[right] {$x$};
        \draw[-latex] (0,0,0) -- (0,3.5,0) node[above] {$y$};
        \draw[-latex] (0,0,0) -- (0,0,3.5) node[above] {$z$};

        % mark P
        \filldraw (P) circle (1.8pt) node[above right=2pt] {$p$};

    \end{scope}

    % outer sphere outline in 2D (to sit on top of the shading)
    \draw[thick] (0,0) circle (\R);

\end{tikzpicture}
\caption{Tangent space of $S^2$ at point $p$}
\label{fig:sphere_tangent}
\end{figure}

With these examples, we can see that a tangent space is generally distinct from the manifold.
Now, we have found a way to describe the domain of the differential.
What about the codomain?
In all cases, $d\mathbf{f_x}$ represents a (very small) change in $\mathbf{f}(\mathbf{x})$.
Using the same reasoning as before, an infinitesimal change can be formalized be considering $d\mathbf{f_x}$ to be a tangent vector.
Thus, the codomain is the tangent space of $\R^2$ at the point $\mathbf{f}(\mathbf{x})$: $T_{\mathbf{f}(\mathbf{x})}\R^2$.
Putting this together, we have the differential $D\mathbf{f_x} \colon T_\mathbf{x}\R^m \to T_{\mathbf{f}(\mathbf{x})}\R^n$ defined by $D\mathbf{f_x}(\Delta\mathbf{x}) = \mathbf{J_f} \Delta \mathbf{x}$.
The change in notation from $d\mathbf{f_x}$ to $\mathrm{D}\mathbf{f_x}$ is to avoid confusion with other concepts later on.

Another name for this map, especially in the context of $\R^m$ and $\R^n$ being replaced by manifolds $M$ and $N$, is the \emph{pushforward}.
It pushes tangent vectors on $M$ forward to tangent vectors on $N$.
This is a point where we can stop generalizing.
Although we have focused on Euclidean spaces, the idea of the pushforward is applicable to manifolds in general.
We have done this without straying from our initial intuition about the differential, instead simply introducing new definitions and reinterpreting ideas.

Before we go on further, we will make another change in our notation.
Normally, we denote the components of a vector $\mathbf{v}$ are denoted by $v_i$, with the index always on the bottom right.
We will now denote some vector components like this, whilst denoting other vector components with their index on the top right, e.g. $v^i$.
This is not random.
The reason for this relates to how the components change under coordinate transformations (\emph{covariance} and \emph{contravariance}).
However, this is not important for our discussion, so it can simply be taken as a fact that this is something we do.

Another minor change to our notation will be that we will now only use boldface letters for vectors that are explicitly denoted to be Euclidean.
This means that tangent vectors will be written just like regular variables, also meaning that we will now use $f$ instead of $\mathbf{f}$ when concerned with functions of tangent vectors.
This is all to align ourselves with the broader field that contains the topics we are discussing: \emph{differential geometry}.

A formal definition of a tangent space can be given using directional derivatives.
This makes sense, as the directional derivative (the map itself) gives you the change in a function tangent to some vector.
Then, the tangent space is the space of all directional derivatives.
Here, we have shifted our perspective from the function $f$ in $\nabla_\mathbf{v} f$ to the vector $\mathbf{v}$.
The directional derivative $\nabla_\mathbf{v}$ is a tangent vector, which only makes sense if one's idea of a vector is that its an element of a vector space (instead of a specific example of a vector).

Given this definition, we can create a basis for the tangent space $T_p\R^n$.
We know that the standard basis for $\R^n$ is $\{\ehat_1, \ldots, \ehat_n\}$, and that the partial derivative operator $\partial_{x_i} = \nabla_{\ehat_i}$.
The partial derivatives give us the change in the directions of $\ehat_i$, so a linear combination of them gives us the change the direction of any Euclidean vector $\mathbf{v}$.
This means that we have
\begin{equation*}
    \nabla_\mathbf{v} = \sum_{i=1}^{n} v^i \eval{\frac{\partial}{\partial x_i}}{p}.
\end{equation*}
Here, $\eval*{\partial/\partial x_i}{p}$ and $\eval*{\partial_{x_i}}{p}$ are the same thing; $\{\eval*{\partial_{x_1}}{p}, \ldots, \eval*{\partial_{x_n}}{p}\}$ is the standard basis for $T_p\R^n$.
Note that since there is a one-to-one association between a tangent vector and a directional derivative, we can make sense of $\nabla_v$, where $v$ is a tangent vector.
This is not circular, because we have not made any new definitions.
We can also define a tangent vector $v$ acting on a function $f$ by it taking its associated directional derivative: $v(f) \coloneq \nabla_v f$.
The formula for the pushforward can be rewritten as $Df_p(v) = v(f(p))$.
The purpose of all of this is to move away from needing to mention Euclidean vectors, which is helpful when we talk about manifolds in general.

We can use the basis for the tangent space of $\R^n$ to find the tangent spaces of other manifolds.
Let us apply what we have developed to the simple example of the unit circle, $S^1$, though be aware of the fact that it will not be a formal proof.
We can parameterize the circle by the function $\gamma \colon [0, 2\pi) \to S^1$ given by $\gamma(t) = (\cos t, \sin t)$.
Suppose that our point $p$ on $S^1$ corresponds to $t = t_0$.
The derivative of $\gamma(t)$ at $t = t_0$ is given by
\begin{equation*}
    \gamma'(t_0) = (-\sin t_0, \cos t_0).
\end{equation*}
This vector is tangent (in a geometric sense) to the circle, which can be verified by the fact that the dot product of $\gamma(t)$ with $\gamma'(t)$ equals zero.
The derivative $\gamma'(t)$ can be intepreted both as a Euclidean vector and as a tangent vector -- there is a one-to-one association between these in this case.
We can properly express it as a tangent vector by using the standard basis of $T_p \R^2$ stated above.
\begin{equation*}
    v = -\sin t_0 \eval{\frac{\partial}{\partial x}}{p} + \cos t_0 \eval{\frac{\partial}{\partial y}}{p}
\end{equation*}
Our tangent space will be a straight line, so any point on the line can be obtained by multiplying some tangent vector in $T_p S^1$ by some scalar.
That is, our tangent vector spans the entire tangent space: $T_p S^1 = \operatorname{span}\{v\}$.

More typically, the tangent space is defined using \emph{derivations}.
A derivation is a linear map $\mathcal{D}$ that satisfies $\mathcal{D}(fg) = g\mathcal{D}(f) + f\mathcal{D}(g)$, which looks very similar to the product rule for derivatives.
It can be shown that there is a one-to-one correspondence between directional derivatives and derivations, so the definitions of tangent vectors using them are equivalent.
We can also define tagent spaces using curves and velocities; considering our intuition about tangent vectors, this is reasonable.
The definition presented here has been chosen due to aligning the most with the previous content in our discussion.

We have now introduced tangent spaces, which puts us in a good place to define the object (or rather, type of object) we have sought after.
One last thing that is important to be aware of is the fact that, whilst the concepts in our generalizations apply to manifolds in general, the specific definitions do not.
A manifold is generally not a vector space, so we deal with points $p$ on the manifold instead of vectors $\mathbf{x}$.
Also, the Jacobian only works for Euclidean spaces, but the idea of the pushforward is the same.
Finally, the definition of the directional derivative would have to be slightly altered to work on manifolds for our definition of the tangent space to be generally applicable.

\section{Cotangent Vectors}

We have now formalized the idea of an infinitesimal quantity using tangent vectors.
Specifically, a tangent vector $v$ represents an infinitesimal change, and it gives a rate of change (directional derivative) when it acts on a function.
It is reasonable to think that one can now make sense of the $dx$ in an integral, but some immediate problems arise.
Consider
\begin{equation*}
    \int_C f(x) \diff x,
\end{equation*}
where $C$ is a curve in $\R^2$, and the function $f$ is of the type $C \to \R$.
The integral is always, no matter the definition, some sort of sum.
Here, we would be summing a scalar multiple of a tangent vector at every point on $C$.
One thing we have to deal with is the fact that there are an infinite number of tangent vectors, which make up the tangent space, at every point.
This is not actually a problem, as choosing a specific parameterization of $C$ will pick a tangent vector at each point. 
The collection of these choices of tangent vectors is called a \emph{(tangent) vector field}.

The actual problem relates to summing these vectors.
In general, there is not a meaningful way to sum tangent vectors that come from different tangent spaces.
A way to see this is by choosing a specific curve and drawing the tangent space at two points.

\begin{figure}[!htbp]
\centering
\begin{tikzpicture}
\begin{axis}[
    width=12cm,
    height=8cm,
    axis lines=middle,
    xlabel={$x$},
    ylabel={$y$},
    xmin=-2, xmax=5.5,
    ymin=-2, ymax=2,
    xtick=\empty,
    ytick=\empty,
    samples=100,
    domain=-2:5,
    enlargelimits=false,
]

% Draw the curve: (1/2)f(x) = 0.125x^3 - 0.625x^2 - 0.125x + 1.25
\addplot[thick, blue, domain=-2:5] {0.125*x^3 - 0.625*x^2 - 0.125*x + 1.25};

% Calculate tangent slopes: f'(x) = 0.375x^2 - 1.25x - 0.125
% At x=-1: f(-1) = -0.125 - 0.625 + 0.125 + 1.25 = 0.625, slope = 0.375 + 1.25 - 0.125 = 1.5
% At x=3: f(3) = 0.125*27 - 0.625*9 - 0.125*3 + 1.25 = 3.375 - 5.625 - 0.375 + 1.25 = -1.375, slope = 0.375*9 - 1.25*3 - 0.125 = 3.375 - 3.75 - 0.125 = -0.5

% Draw tangent line at P1 (x=-1)
\addplot[red, thick, domain=-1.5:-0.5] {0.625 + 1.5*(x-(-1))};

% Draw tangent line at P2 (x=3)
\addplot[red, thick, domain=2.2:3.8] {-1.375 - 0.5*(x-3)};

% Mark points on top
\addplot[only marks, mark=*, mark size=2pt, black] coordinates {(-1, 0.625)};
\addplot[only marks, mark=*, mark size=2pt, black] coordinates {(3, -1.375)};

% Labels
\node[above left] at (axis cs:-1, 0.625) {$p_1$};
\node[above right] at (axis cs:3, -1.375) {$p_2$};

\end{axis}
\end{tikzpicture}
\caption{A curve $C$ with tangents at points $p_1$ and $p_2$}
\label{fig:curve_tangents}
\end{figure}

How would we add a tangent vector of $p_1$ to a tangent vector of $p_2$?
We can assign them to vectors in $\R^2$ using some basis and add them component-wise, but that is only possible because we have \emph{embedded} the curve $C$ in $\R^2$.
Tangent vectors are \emph{instrinsic} to the manifold itself, so an operation with them should not depend on the manfiold being embedded within another manifold.
Even if we were to accept it as valid, our resulting vector would not necessarily be tangent to either $p_1$ or $p_2$; it would not be a tangent vector.
Thus, our integral is not well-defined.

A sensible response would be to slightly alter the definition to try and make it work.
If we introduce another function, say $g \colon C \to \R$, we can have a tangent vector acting on it to return a real number at each point.
We then run into another problem.
Since the chosen tangent vectors depend on our parameterization, it follows that the value of our intergral would also depend on the parameterization.
This does not make sense if the integral is supposed to carry some sort of geometric information about the manifold and the object we are integrating over the manifold.
If two sets of equations describe the same manifold, then we should be able to integrate using either and receive the same result.

There are even more issues with this definition.
We can thus conclude that it does not properly represent what we mean by an integral.
However, this raises a question.
Our naive formualtion of an integral with tangent vectors was based on the seemingly sound reasoning of what the $dx$ meant.
Is it an infinitesimal?
If not, why did it seem like one?
We will see that it was not completely misguided.
Instead, we simply require one more concept.
In order to transfer the information contained within a tangent vector into something we can sum, we need an object that takes tangent vectors and returns real numbers, effectively measuring them.
Finding such an object is now our goal.

We will call these objects \emph{cotangent vectors}.
At some point $p$ in a manifold $M$, they should map $T_p M$ to $\R$.
There will obviously be many different ways to measure a certain tangent vector, so there will be many different cotangent vectors at any given point.
We will call the space containing all of them the \emph{cotangent space}, labelling it $T^*_p M$.
Each point has its own cotangent space, and creating a collection of cotangent vectors by picking one for each point gives us a \emph{cotangent field}.
Alternative names for all of these concepts involve replacing `cotangent' with `covector'.
We will use these two sets of terms interchangeably for convenience.

Note that all of these new concepts that we have introduced are not so new.
Rather, they mirror ideas we have already had relating to tangent vectors.
This is no coincidence, and the two sets of concepts can be described as being dual to each other.
Familiarity with dual spaces in linear algebra lets us say that $T^*_p M$ is the dual space of $T_p M$.
In order show what this means, we will need to concretely formulate the idea of covectors.
This is done by thinking about the properties our object needs to have.

Covectors are maps from $T_p M$ to $\R$.
This means that they are functionals, objects that take a function as input and return a number.
Here, the functions are tangent vectors, which is evident from the fact they are directional derivatives (the map itself).
Moreover, they are linear.
This condition is imposed on them so that they measure tangent vectors in a sensible way.
Taking measurements of sums should become a sum of measurements, and measuring a scaled vector should be a scaled version of the original vector.

To complete the picture, we will show how covectors form a tangent space.
The typical notation for covector fields is the Greek letter $\omega$, with the covector at some point $p$ in $M$ being denoted by $\omega_p$.
In cases where we deal with multiple covector fields, we may use other Greek letters such as $\alpha$ or $\beta$.
The following definitions of addition and multiplication should feel natural; we are saying that these objects combine and scale linearly.
Note that $a$ is some scalar.
\begin{align*}
    (\alpha + \beta)(v) &= \alpha(v) + \beta(v) \\
    (a\omega)(v) &= a(\omega(v))
\end{align*}
With this, $T^*_p M$ is a vector space.
Due to the nature of covectors being linear functionals of tangent vectors, we have a duality between tangent and cotangent spaces.
In a sense, they are mirrored versions of each other.
As with any vector space, we are able to give $T^*_p M$ a basis.
Before we do this, we will gain some visual intuition for covectors first.
Consider the following plot of the function $f(x,y) = x\sin y + y\sin x$, where the colours represent the value of $f(x,y)$ at a particular point.

\begin{figure}[!htbp]
\centering
\begin{tikzpicture}
\begin{axis}[
    width=10cm,
    height=10cm,
    view={0}{90},
    colormap/jet,
    xlabel={$x$},
    ylabel={$y$},
    colorbar,
    colorbar style={
        ylabel={$f(x,y)$}
    },
    xmin=-10, xmax=10,
    ymin=-10, ymax=10,
    domain=-10:10,
    y domain=-10:10,
]

% Create heatmap with interpolated shading
\addplot3[
    surf,
    shader=interp,
    samples=50,
] 
{x*sin(deg(y)) + y*sin(deg(x))};

% Overlay contour lines showing constant function values
\addplot3[
    contour gnuplot={
        levels={-16,-14,-12,-10,-8,-6,-4,-2,0,2,4,6,8,10,12,14,16},
        labels=false,
    },
    samples=90,
    samples y=90,
    thin,
    contour/draw color=black,
] {x*sin(deg(y)) + y*sin(deg(x))};

\end{axis}
\end{tikzpicture}
\caption{Plot of $f(x,y) = x\sin y + y\sin x$}
\label{fig:heatmap}
\end{figure}

The important information here are the contour lines, which are level sets $\{(x,y) \mid f(x,y) = c\}$ for some constant $c$.
In our diagram, they have been drawn for $c \in \{-16, -14, \ldots, 14, 16\}$.
They are close together where the function is steeper, and further apart where the function is less steep.
It is important to note that this is just an arbitrary selection of level sets.
There are, in fact, an infinite number of level sets, since there are an infinite number of constants that we can choose from.
That is, no matter how far we zoom into the plot, we will always find a new line between two other ones.

If the plot represented a topographic map, where the elevation is given by $f(x,y)$, we could figure out the change in elevation from one point to another by counting the number of contour lines we cross.
When we mention count in this context, we are including crossing over a line by a non-integer amount, e.g. we could be halfway between two lines.
Our movement can be represented by a vector, and the count measures this vector.
This is how we will visualize covectors.

The above plot can be interpreted as a cotangent field \cite{covector_visualization}.
If we were too zoom into a particular point, the curved contour lines resemble stacks of -- equally spaced -- lines tangent to the contour line at that point.
Then, we can place a tangent vector (tangent to some manifold, not to the function we are talking about) at that point and count the number of stack lines it crosses.
We can see from \cref{fig:parallel_stack_lines} that the covector acts in a linear way, which matches our definition.
The linearity is from the fact that it is, in a way, measuring the length of the vector.
The fact that this is done in reference to itself means that we do not need an external ruler (a metric).
We will touch on this later.

Also, note that the stack has an orientation (shown by the black line).
This is because a function increases from one direction and decreases from the opposite direction.
Covectors in general have an orientation, and this is an important fact about them.

\begin{figure}[!htbp]
\centering
\begin{tikzpicture}[scale=1.0]
    % Diagonal stack lines (visualizing a covector)
    \foreach \k in {-3.5,-3.0,...,3.5} {
        \draw[thick,gray!65] (-3,\k-0.5) -- (3,\k+0.5);
    }
    % Orientation arrow
    \draw[->,thick,black] (3.65862,-0.40344) -- (3.5,1.5);
    % Reference point
    \filldraw[black] (-1,-1) circle (1.5pt) node[below left] {$p$};
    % Invisible path to extend bounding box
    \path (3.65862,-0.40344) ++(0.3,0);
    % Vector v
    \draw[->,thick,blue] (-1,-1) -- (1.3,0.7) node[midway,below right] {$v$};
    % Vector w
    \draw[->,thick,teal] (-1,-1) -- (-0.2,1.5) node[midway,left] {$w$};
    % Vector v + w
    \draw[->,thick,purple] (-1,-1) -- (2.1,3.2) node[pos=0.6,right] {$v+w$};
    % Endpoint markers
    \filldraw[blue] (1.3,0.7) circle (1.5pt);
    \filldraw[teal] (-0.2,1.5) circle (1.5pt);
    \filldraw[purple] (2.1,3.2) circle (1.5pt);
\end{tikzpicture}
\caption{Covector acting on tangent vectors}
\label{fig:parallel_stack_lines}
\end{figure}

We have shown that this diagram is a visualization of covectors in that it describes their behaviour of acting on tangent vectors and representing linear functionals.
Now, we will link covectors to a concept that we have already studied.
Let $\omega$ be the covector field represented by the plot of $f(x,y)$, and let $\omega_p$ be the covector at some point $p$.
This is, by our definition, a linear map from $T_p M$ to $\R$.
We already know of such a map: the pushforward $Df_p$ of a function $f\colon M \to \R$.

Technically, this pushforward maps to $T_p \R$, not $\R$.
However, these two spaces can be associated with each other in a one-to-one way; the tangent space of $\R$ at $p$ can be seen as a copy of $\R$ anchored to that point.
Due to this association, we can consider the pushforward to `effectively' map to $\R$.
Since we already know that the pushforward is linear (from the linearity of derivatives), it follows that it is a covector.
Considering that pushforwards seemed to have a link to integration, looking exactly like integrands, this seems reasonable.

Furthermore, we know that the pushforward measures something, and we know what exactly it measures.
The number $Df_p(v)$ is the directional derivative of $f$ along $v$ at $p$
In this context, where $\omega = Df$, the number $\omega_p(v)$ represents the same thing.
It can be interpreted as the rate at which a particle at $p$ would cross the stack lines if it had a velocity $v$.
The fact that $\omega_p(v)$ and $v(f)$ give the same number and represent the same concept again shows their dual nature.
We will later see that not all covector fields are the pushforward of some function.
This is only relevant to fields, so our intuition for individual covectors is unaffected; we do not need to worry about it now.

Pushforwards can also be used to give a reason as to why the stack lines are tangent to the contour lines.
Moving along a level set will not change the value of the function, by definition.
Thus, the pushforward will output zero in this direction.
If a tangent vector is tangent to a stack line, it will not cross any of them; the covector will thus act on it to give zero.
These two ideas combine via $\omega = Df$ to give us the information that the stack lines and level sets are in the same direction as each other at a given point.
They are tangent.
Note that the actual direction of the covector is perpendicular to the stack lines.

A relevant note to make is that pushforwards in general are not covectors.
Given a function $f \colon M \to N$, the pushforward is a map $Df_p \colon T_p M \to T_p N$.
This outputs tangent vectors, making it fundamentally different from a covector.
We were only able to associate them when dealing with the pushforward of functions $f \colon M \to \R$, due to $T_p \R$ essentially being the same thing as $\R$.
The association between pushforwards and covectors is more accurately an association between the pushforward and an operation that we will introduce later.
For now, it is not needed.

\cref{fig:1-form_hyperplanes} shows a similar visualization of a covector.
It is simply the three-dimensional analogue to what we have already discussed -- the principles and intuition remain the same.
As we go to higher dimensions, covectors will maintain this geometric interpretation, even though it becomes impossible to visualize (without some tricks).
In general, a covector can be seen as a stack of hyperplanes.
A covector field can be seen as some sort of topographic map, where the contours resemble these stacks of hyperplanes near a point.

\begin{figure}[!htbp]
    \centering
    \scalebox{1.5}{\includegraphics{1-form_hyperplanes}}
    \captionsetup{justification=centering}
    \caption{Visualization of covectors as hyperplanes
    \\Credits: Maschen -- Own work, \href{https://creativecommons.org/public-domain/cc0/}{CC0}, via Wikimedia Commons)}
    \label{fig:1-form_hyperplanes}
\end{figure}

Now, consider the covector field associated with the function $f(x, y) = x$.
This is shown in \cref{fig:heatmap_fx}.
Its contour lines are straight, parallel, and have equal spacing.
They point to the right.
The covector at any point will look identical to the contour lines, except that it will always have unit spacing between stack lines.
A vector that points completely vertically will not cross any stack lines, so its measurement will read zero.
In general, only the $x$-component of a tangent vector will contribute to the measurement.
Due to the unit spacing, the measurement will be exactly the $x$-component of the vector.

\begin{figure}[!htbp]
\centering
\tikzsetnextfilename{heatmap_fx}
\begin{tikzpicture}
\begin{axis}[
    width=10cm,
    height=10cm,
    view={0}{90},
    colormap/jet,
    xlabel={$x$},
    ylabel={$y$},
    colorbar,
    colorbar style={
        ylabel={$f(x,y)$}
    },
    xmin=-10, xmax=10,
    ymin=-10, ymax=10,
    domain=-10:10,
    y domain=-10:10,
]

% Create heatmap with interpolated shading
\addplot3[
    surf,
    shader=interp,
    samples=50,
] 
{x};

% Overlay contour lines showing constant function values
\addplot3[
    contour gnuplot={
        levels={-8,-6,-4,-2,0,2,4,6,8},
        labels=false,
    },
    samples=90,
    samples y=90,
    thin,
    contour/draw color=black,
] {x};

\end{axis}
\end{tikzpicture}
\caption{Plot of $f(x,y) = x$}
\label{fig:heatmap_fx}
\end{figure}

The covectors associated with $f(x,y) = y$ similarly pick out the $y$-component of a tangent vector.
We have thus found two covector fields whose covectors do completely different things.
Using both of them, we can pick out both parts of a tangent vector.
This strongly suggests that they will form our basis for the cotangent space.
Indeed, the natural choice of basis immediately follows from simply visualizing covectors and considering the simplest cases.

The basis for $T_p \R^2$ will be $\{\d{x}, \d{y}\}$.
These pick out the components of tangent vectors, so $\d{x}(\partial_x) = 1$, $\d{x}(\partial_y) = 0$, $\d{y}(\partial_x) = 0$, and $\d{y}(\partial_y) = 1$.
Generalizing this, the basis for $T^*_p \R^n$ will be $\{\d{x}^1, \ldots, \d{x}^n\}$, with
\begin{equation*}
    \eval{\d{x}^i}{p} \left(\eval{\partial_{x_j}}{p}\right) = \begin{cases}
        1 & \text{if } i = j,\\
        0 & \text{if } i \neq j.
    \end{cases}
\end{equation*}
Here, we have specified that both the covector and tangent vector act at the point $p$ with an evaluation bar.
This is technically the proper way to do it, but it is often omitted, as we did in the paragraph above.
This is done when context makes it clear what (possibly arbitrary) point we are talking about.
Since we very often talk about both covector fields $\omega$ and covectors $\omega_p$, the distinction between them is often made via notation.
For tangent spaces, we use an uppercase letter such as $V$ to distinguish them from tangent vectors $v$; this is needed less frequently.

A general covector $\omega_p$ can be expressed as a linear combination of the basis covectors.
It can be seen as a set of stack lines obtained by a combination of horizontal and vertical stack lines.
\begin{equation*}
    \omega_p = \sum_{i=1}^{n} (\omega_p)_i \d{x}^i
\end{equation*}
A covector field is expressed similarly.
\begin{equation*}
    \omega = \sum_{i=1}^{n} \omega_i \d{x}^i
\end{equation*}
Examples of covectors include $\d{x}$, $5 \d{x} + 2\d{y}$, and $\d{x} - 6\d{z}$.
An example of a covector field is $x^2 z \d{x} + 2 \d{y} - y\sin x \d{z}$, where inputting some values for $x$, $y$, and $z$ turn it into a covector.
The way covectors act on basis tangent vectors, and the fact that they are linear, is enough to evaluate any covector at a given tangent vector.
For instance, given $\omega_p = 3 \d{x} + 2 \d{y}$ and $v = 5 \partial_x - \partial_y$, we have
\begin{align*}
    \omega_p(v) &= \omega_p (5 \partial_x - \partial_y) \\
    &= 5 \omega_p (\partial_x) - \omega_p (\partial_y) \\
    &= 5(3 \d{x} + 2 \d{y}) (\partial_x) - (3 \d{x} + 2 \d{y}) (\partial_y) \\
    &= 5 \cdot 3 \d{x}(\partial_x) + 5 \cdot 2 \d{y}(\partial_x) - 1 \cdot 3 \d{x}(\partial_y) - 1 \cdot 2 \d{x}(\partial_y) \\
    &= 15 \cdot 1 + 10 \cdot 0 - 3 \cdot 0 - 2 \cdot 1 \\
    &= 13.
\end{align*}
We did this the long way on purpose.
We split the expression twice, using the fact that covectors are linear maps, and that the addition of covectors is linear.
Note that evaluating a covector involves inputting a vector.
Evaluating a covector field involves inputting a point and then a vector.
Thus, they can be thought of as one-step and two-step evaluations respectively.

Let us apply this and find the cotangent space for a point on the unit circle, $T^*_p S^1$ (again, not a proof).
Previously, we found that the the tangent space $T_p S^1$ is spanned by the vector
\begin{equation*}
    v = - \sin t_0 \eval{\frac{\partial}{\partial x}}{p} + \cos t_0 \eval{\frac{\partial}{\partial y}}{p},
\end{equation*}
where $t=t_0$ corresponds to p by the parameterization $\gamma \colon [0, 2\pi) \to S^1$ with $\gamma(t) = (\cos t, \sin t)$.
We get the above vector by differentiating $\gamma$ with respect to $t$, so we can label the basis tangent vector $\partial_t$.
Now, since $T_p S^1$ is one-dimensional, it makes sense that $T^*_p S^1$ is also one-dimensional.
Its basis vector will be the covector $\d{t}$, with $\d{t}(\partial_t) = 1$.
We wish to write this covector in the form $\omega_p = A \d{x} + B \d{y}$.
This can be done by simply evaluating $\omega_p$ at $v = \partial_t$.
\begin{equation*}
    \omega_p(v) = \left( A \eval{\d{x}}{p} + B \eval{\d{y}}{p} \right) \left( - \sin t_0 \eval{\frac{\partial}{\partial x}}{p} + \cos t_0 \eval{\frac{\partial}{\partial y}}{p} \right)
\end{equation*}
This time, we will approach this the quick way by simply picking out the components.
\begin{equation*}
    \omega_p(v) = - A \sin t_0 + B \cos t_0 = 1
\end{equation*}
Using the pythagorean identity for sine and cosine, we see that $A = - \sin t_0$ and $B = \cos t_0$ is a valid solution.
Hence, our basis covector is
\begin{equation*}
    \omega_p = \eval*{\d{t}}{p} = \omega_p(v) = - \sin t_0 \d{x} + \cos t_0 \d{y},
\end{equation*}
and $T^*_p S^1 = \operatorname{span}\{\eval{\d{t}}{p} \}$.

%! I AM HERE
%! REVISE EVERYTHING AFTER THIS AND THEN ADD NEW STUFF
%! WORK ON THIS DOCUMENT IS TEMPORARILY LOW PRIORITY UNTIL AFTER MID DECEMBER

An important detail is that covectors, and covector fields in general, look like integrands.
This is especially apparent from our choice in the symbol used to represent basis covectors.\footnote{Our use of upright symbols is an intentional choice to note that these are distinct from the $dx$ used for differentials previously.}
We have done this on purpose.
Although pushforwards -- and the more primitive notions of differentials -- looked like integrands, this was more of a superficial resemblence.
We did not have the machinery to go further with any ideas.
Now, we have pinpointed the exact type of object we need, and we are ready to discuss how they may be integrated.

The general idea of integrating a covector field does not differ much at all from how integration of a $\R \to \R$ function works.
At some point $p$ on a manifold $M$, the field $\omega$ specifies a covector $\omega_p$, which acts on the tangent vector $v \in T_p M$ to give $\omega_p(v)$.
These measurements are accumulated over some curve in $M$ to give us the result of an integral.
Here, accumulating does not mean summing all the numbers we get together.
In that case, almost no integral would have a finite value.

Integration is always -- not just a sum, but a -- limiting process.
To see how exactly this works, and how we can evaluate our integration, we introduce the concept of the \emph{pullback}.
If we could transform our problem of integrating over a general manifold to (an interval of) the real line, our job is done.
We know how to integrate over the latter.
Suppose we have some function $\phi\colon \R \to M$.
We can use this to `pull' the covector from $T_p M$ to $\R$.
How is this done?
We know that the pushfoward $D\phi_p$ is a map from $T_p \R$ to $\T_p M$, and the covector $\omega_{\phi(p)}$ is a map from $T_{\phi(p)} M$ to $\R$.
Thus, precomposing $D\phi_p$ with $\omega_{\phi(p)}$ gives us a function $\omega_{\phi(p)} \circ D\phi_p \colon T_p \R \to \R$.

The domain of the inner map and the codomain of the outer maps become the domain and codomain of our composite map.
That is just how composing maps and functions work.
A notation for the pullback we described above is $\phi^*$.
Given its relation to the pushforward, it makes sense that an alternative notation for $D\phi$ is $\phi_*$.
Now, putting this together, we can use some map $\phi$ to pull $\omega$ back to an interval of the real line, which we can integrate over.
Our formula for the integral of $\omega$ over a curve $\gamma$ in a manifold $M$ parameterized by $\gamma: [a,b] \to M$ is
\begin{equation*}
    \int_\gamma \omega \coloneq \int_a^b \gamma^* \omega.
\end{equation*}
As indicated by the special equality symbol, this is a definition.
We initially developed the idea for the integral pretty easily as an accumulation of the measurements from covectors acting on tangent vectors.
Then, to quantify this accumulation, we introduced the idea of pulling back a covector field from some manifold to an interval of $\R$.
Afterwards, it is just applying already-developed theory to evaluate the integral.
We will now demonstrate how exactly this is done.

We need to find an expression for the pullback $\gamma^*\omega$.
Note that $\gamma^*\omega$ at a point $t \in [a,b]$ is given by $\omega_{\gamma(t)} \circ D\gamma_t$.
This a map from $T_p[a,b]$ to $\R$, making it a covector.
Since we are working with an interval of $\R$ (which is one-dimensional), our covector will be of the form $f(t) \d{t}$.
Now, we must find $f(t)$.
This can be done by inputting any tangent vector into both sides of the equation $\omega_{\gamma(t)} \circ D\gamma_t = f(t) \d{t}$, since the equality is true for all tangent vectors.
The simplest option is the basis tangent vector.
Due to the one-to-one association between $T_p [a,b]$ and $T_p \R$, this is $\partial_t$.
On the RHS, we get
\begin{equation*}
    (f(t) \d{t})(\partial_t) = f(t).
\end{equation*}
On the LHS, we evaluate the pushforward $D\gamma_t$.
We know that the differential $D\gamma_t(v) = \mathbf{J}_\gamma v = \nabla_v \gamma(t) = v(\gamma(t))$, where $v \in T_{\gamma(t)} [a,b]$.
This gives us $D\gamma_t(\partial_t) = \partial_t (\gamma(t)) = \gamma'(t)$.
Hence, $(\gamma^*\omega)_t = \omega_{\gamma(t)}(\gamma'(t))$.
Our formula for the integral is therefore
\begin{equation*}
    \int_\gamma \omega = \int_{a}^{b} \omega_{\gamma(t)}(\gamma'(t)) \d{t}.
\end{equation*}
This is a standard Riemann integral, and we have already seen how we evaluate covectors.
We have now answered our initial question of what the $dx$ in the integral means, again.
Instead of being an infinitesimal quantity we sum over, it is an object that takes infinitesimals and measures them so that they can be summed.
Precisely, it is a cotangent vector field.

We have provided our fourth and final interpretation of what $dx$ means.
This one has been sought after specifically for its use in integration.
Our work remains incomplete, however.
Although we have explained what $\int f(x) \diff x$ means, what about $\int f(x,y) \diff x \diff y$?
The definition we gave works for integrating along curves, but that is it.
Considering the fact that we have (informally) developed the idea of manifolds, which generalize Euclidean space, we should be able to integrate much more than curves.
There is a lot more we can newly-developed theory, which is what we will explore in the next section.

\section{Differential Forms}

%TODO MENTION THAT COVECTOR FIELDS ARE ONE-FORMS
%TODO INTRODUCE EXTERIOR DERIVATIVE
%TODO INTRODUCE WEDGE PRODUCT AND K-FORMS
%TODO INTRODUCE INTEGRATION OF K-FORMS
%TODO MENTION PULLBACKS MORE GENERALLY
%TODO INTRODUCE HODGE STAR

\section{Resources}

\textsc{Tensor Calculus 6: Differential Forms are Covectors by eigenchris} \, \url{https://www.youtube.com/watch?v=XGL-vpk-8dU}

\textsc{Differential Forms by Michael Penn} \, \url{https://youtube.com/playlist?list=PL22w63XsKjqzQZtDZO_9s2HEMRJnaOTX7}


\printbibliography

\begin{appendices}
\crefalias{section}{appsec}
\crefalias{subsection}{appsec}

\section{Vectors} \label{appendix:vectors}

\subsection{Definition of a Vector} \label{appendix:vector_space}

An introduction to the concept of vectors usually involve giving examples of vectors -- such as arrows in a plane -- without giving a proper definition of what a vector is in general.
That makes sense, since such a definition is (relatively) technical and unnecessary for simply working with vectors.
The technical definition is that a vector is an element of a vector space, but that alone simply replaces one question with another.
It follows that a definition of a vector space is now required.

To come up with it, we will follow a method of generalization that can be readily applied to other situations as well.
We know that ordered pairs of real numbers are examples of vectors, so the set containing all such pairs is an example of a vector space.\footnote{This is technically an abuse of language. A vector space is a set with additional structure -- the ability to add and scale its elements in a certain way. However, this distinction is not very useful until we actually define a vector space properly.}
This set is denoted by $\R \times \R$, and $\R^2$ is a useful shorthand.
We will examine the properties of this set in order to obtain a general definition of a vector space.

Two things we can do with our vectors is add them together and scale them.
For the example we are working with, this is done component-wise: $(a, b) + (b, c) = (a + b, c + d)$ and $c \cdot (a, b) = (ac, bc)$.
Since we can use real numbers to scale vectors, they are also referred to as scalars in this context.
Both of those functions have geometric interpretations if the vectors are viewed as arrows on a plane.
Vector addition involves connecting two vectors tip-to-tail and the sum is the third side of a triangle (or the diagonal of a parallelogram).
Scalar multiplication involved literally scaling the length of a vector by some number, where multiplication by a negative number reverses direction.

Whilst vectors in general are not arrows, the way they add and scale is fundamental to them.
If we try to extract all the properties of addition and scalar multiplication, we get the following list.
Note that the boldface letters are elements of $\R^2$, whilst the Greek letters are real numbers.
Also, the boldface zero represents (0, 0).
\begin{enumerate}
\item $\mathbf{u} + \mathbf{v} = \mathbf{v} + \mathbf{u}$
\item $\mathbf{u} + (\mathbf{v} + \mathbf{w}) = (\mathbf{u} + \mathbf{v}) + \mathbf{w}$
\item $\mathbf{0} + \mathbf{v} = \mathbf{v}$
\item $-\mathbf{v} + \mathbf{v} = \mathbf{0}$
\item $\lambda(\mu\mathbf{v}) = (\lambda\mu)\mathbf{v}$
\item $1\mathbf{v} = \mathbf{v}$
\item $\lambda(\mathbf{v} + \mathbf{w}) = \lambda\mathbf{v} + \lambda\mathbf{w}$
\item $(\lambda + \mu)\mathbf{v} = \lambda\mathbf{v} + \mu\mathbf{v}$
\end{enumerate}
That leads to our definition of a vector space -- a set, together with vector addition and scalar multiplication, that satisfies the above statements.\footnote{Addition and scalar multiplication must be closed, but this is within their definitions, rather than being a part of the axioms.}
More precisely, the third and fourth statements say that an identity vector (that works for all vectors) and inverse vectors (that each work for a specific vector) exist.
These statements are known as the vector space axioms.
Note that choosing those specific axioms to define a vector space is just that: a choice.\footnote{It is not an arbitrary choice, however. Vectors are an incredibly useful concept both in pure maths and in other subjects -- where they are applied (along with linear algebra in general) perhaps more than any other topic in maths.}
We can explore sets with different strucutres, obtains objects like groups and rings instead.

A common notation for vectors is boldface letters, and that is what will be used in this document.
Regular real numbers are never in boldface, though they might use either the Latin or Greek alphabet.
Additionally, whilst there are many different objects that classify as vectors, the only ones that will be considered by us are in fact lists of real numbers.
An ordered listed of $n$ real numbers is an element of $\R^n$.
In this context, $n$ is always interpreted as being a positive integer.

The above contains sufficient information to define vectors and work with them on a basic level.
What follows are some additional points of discussion.
Firstly, there are many different notations that exist for vectors.
Arrows, particularly in physics, can also be used to denote vectors, e.g. $\vec{F} = m\vec{a}$.
It is not easy to write boldface symbols in handwriting, so another alternative is underlining the vector, $\underline{v}$, or putting a tilde beneath it, $\underaccent{\sim}{v}$.
For the actual value of the vector, you can write it as a coordinate, as was shown before.
Another common way is to write it as a column of numbers.
\begin{equation*}
    \mathbf{v} = \begin{bmatrix}
        a \\
        b
    \end{bmatrix}
\end{equation*}
It can be written also be written like as $\mathbf{v} = \rvec{a,b}$. Note that the former is called a column vector, whilst the latter is a row vector.
Technically, they are different objects, but this distinction only matters when dealing with matrices.
Another way to write a vector is $\mathbf{v} = \langle a, b \rangle$.
This might be preferred due to avoiding confusion with coordinates, though writing $\mathbf{v} = (a,b)$ is pretty common.
Additionally, given that $\ihat = \langle 1, 0 \rangle$ and $\jhat = \langle 0, 1 \rangle$, you can write a vector as $\mathbf{v} = v_x\ihat + v_y\jhat$, where $v_x$ and $v_y$ are the components of the vector.
Note that these examples were given for $\R^2$, but they extend to general vectors in $\R^n$.

Another point to make is that vector addition is a binary operation.
It takes two vectors as an input and outputs another vector.
On the other hand, scalar multiplication is a binary function.
It takes a vector and a scalar as an input and outputs a vector.
All operations are functions, but the converse is not true in general.
An operation is a function whose domain is some power of the codomain.
For example, vector addition -- in reference to elements of $\R^n$ -- may be written as $+\colon \R^{2n} \to \R^n$.
This is a matter of semantics and using the word operation loosely is both common and unharmful.

\subsection{Vector Calculus} \label{appendix:vector calculus}

This subappendix assumes the knowledge in Appendix~\ref{appendix:vector_space} and Appendix~\ref{appendix:matrices}.
Defining vectors is itself not the most fruitful task.
A proper definition of real numbers is both motivated and immediately succeeded by a discussion on how to do calculus with them.
We wish to extend those notions to vectors, and we will find that our definitions are not terribly different.
First, consider the simplest case of a vector-valued function, whose domain is $\R$.
Let the function be called $\mathbf{f}$ and let $\mathbf{y} = \mathbf{f}(x)$.
There is nothing stopping us from putting this into the regular definition of the derivative.
\begin{equation*}
    \mathbf{f}'(x) = \frac{d\mathbf{y}}{dx} = \lim_{h \to 0} \frac{\mathbf{f}(x+h) - \mathbf{f}(x)}{h}
\end{equation*}
This works perfectly, so we will have it as the definition of the derivative of $\mathbf{f}$ at a point $x$, provided the limit exists.
How do we actually evaluate this derivative?
For that, consider $\mathbf{f}(x) = a(x)\ihat + b(x)\jhat$.
Putting this into our definition gives the following.
\begin{align*}
    f'(x) &= \lim_{h \to 0} \frac{a(x+h)\ihat + b(x+h)\jhat - (a(x)\ihat + b(x)\jhat)}{h} \\
    &= \lim_{h \to 0} \frac{a(x+h) - a(x)}{h}\ihat + \lim_{h \to 0} \frac{b(x+h) - b(x)}{h}\jhat \\
    &= a'(x)\ihat + b'(x)\jhat
\end{align*}
In general, we can differentiate such functions by differentiating their components.
If instead, we consider a function $f$ which maps from $\R^n$ to $\R$ (a vector to a real number), our job becomes much more difficult.
Our regular definition of a limit will not work, as we have to consider vectors approaching other vectors.
Fortunately, it is not too different from the normal $\varepsilon$-$\delta$ definition.
We say that
\begin{equation*}
    \lim_{\mathbf{x} \to \mathbf{a}} f(\mathbf{x}) = \mathbf{L}
\end{equation*}
if and only if, for all $\varepsilon > 0$, there exists a $\delta > 0$ such that for all $\mathbf{x}$ in the domain of $f$,
\begin{equation*}
    0 < \norm{\mathbf{x} - \mathbf{a}} < \delta \implies \norm{f(\mathbf{x}) - \mathbf{L}} < \varepsilon.
\end{equation*}
This simplifies down to the regular definition when $n=1$, and intuitively represents the exact same idea.
The problem arises when we try to generalize differentiation.
For $n=1$, it involves a limit of a fraction, where we divide by $h$.
However, if $n > 1$, then we now have the vector $\mathbf{h}$, and we do not know how to divide by vectors.
We could attempt to resolve it by instead dividing by $\norm{\mathbf{h}}$, saying that the derivative of $f$ at $x$ is
\begin{equation*}
    \lim_{\mathbf{h} \to \mathbf{0}} \frac{f(\mathbf{x}+\mathbf{h}) - f(\mathbf{x})}{\norm{\mathbf{h}}},
\end{equation*}
when this limit exists.
The problem is that, when $n=1$, the limit simplifies to
\begin{equation*}
    \lim_{h \to 0} \frac{f(x+h) - f(x)}{\abs{h}}.
\end{equation*}
Suppose $f(x) = x$.
Our limit becomes
\begin{equation*}
    \lim_{h \to 0} \frac{h}{\abs{h}}.
\end{equation*}
This limit does not exist, meaning that $f(x)$ is not differentiable anywhere under this definition.
Furthermore, it would suggest that $f(x) = \abs{x}$ is differentiable at $x=0$, which is also not true.
Generally, this limit exists only if it is equal to zero.
This is because a limit exists if and only if both the left- and right-hand limits exist.
When $h \to 0^+$, $\abs{h} = h$.
When $h \to 0^-$, $\abs{h} = -h$.
The two one-sided limits differ by their sign, so they are equal only if they are zero.
Although, we can take our original definition of the derivative and move to $f'(x)$ inside the limit to get
\begin{equation*}
    \lim_{h \to 0} \frac{f(x+h) - f(x) - hf'(x)}{h} = 0.
\end{equation*}
That is, the derivative of $f$ at $x$ if there is some number $f'(x)$ that satisfies the above statement.
Now, replacing $h$ with $\abs{h}$ will not make a difference.
That means we can generalize our derivative for any $n$.
The derivative of a function $f$ at $\mathbf{x}$ is equal to $D$ if it satisfies
\begin{equation*}
    \lim_{\mathbf{h} \to \mathbf{0}} \frac{f(\mathbf{x}+\mathbf{h}) - f(\mathbf{x}) - D\mathbf{h}}{\norm{\mathbf{h}}} = \mathbf{0}.
\end{equation*}
We may write $D$ as $f'(\mathbf{x})$.
It is important to know what kind of object $D$ is.
It cannot be a real number, since that would make $D\mathbf{h}$ a vector, and the rest of the numerator is a scalar.
If we treat $\mathbf{h}$ as a column vector, then $D$ is a row vector -- their product is a $1 \times 1$ matrix which can be associated with a scalar.
This makes it a matrix.
The name for it is the \emph{Jacobian matrix}, or just the Jacobian, and notation for it includes $\mathbf{J_f}$ (the subscript represents the function it is for) and $\mathbf{J}$.

Now that we have introduced matrices, we can use them to make the final generalization to functions $\mathbf{f}$ that map from $\R^n$ to $\R^m$ (vectors to vectors).
It follows from everything we have discussed so far that the derivative of $\mathbf{f}$ at $\mathbf{x}$ is an $m \times n$ matrix $\mathbf{J_f}$ if it satisfies
\begin{equation*}
    \lim_{\mathbf{h} \to \mathbf{0}} \frac{\mathbf{f}(\mathbf{x}+\mathbf{h}) - \mathbf{f}(\mathbf{x}) - \mathbf{J_f h}}{\norm{\mathbf{h}}} = \mathbf{0}.
\end{equation*}
An $m \times n$ matrix multiplied by a $n \times 1$ matrix gives a $m \times 1$ matrix or an $m$-dimensional column vector (the same as the rest of the numerator).
Note that, since a derivative is a matrix, it is a linear map from $\R^n$ to $\R^m$.
This is in-line with the one-dimensional case.
We can then go on to develop familiar concepts with our generalized derivative.
For example, a function is differentiable if there exists some value of $\mathbf{J_f}$ such that the above limit equals $\mathbf{0}$; a function being differentiable implies it is continuous.

One problem we have is that our definition of the derivative is not explicit.
It states that $\mathbf{f}'(\mathbf{x}) = \mathbf{J_f}$ given that $\mathbf{J_f}$ satisfies some condition, but it does not tell us how we actually find $\mathbf{J_f}$.
This in contrast to the case of $n=m=1$, where finding the derivative involves evaluating a limit -- with the function being differentiable if the limit exists.
To find an explicit formula for the derivative, we will first introduce a new concept: the \emph{directional derivative}.
Intuitively, it tells you the rate of change of a function in some particular direction (multiplied by a number).

First, we will start with a function $\mathbf{f}$ from $U$ to $\R^m$, with $U \subseteq \R^n$.
Assume that $\mathbf{f}$ is differentiable at some point $\mathbf{x}$.
Let $\mathbf{v}$ be a fixed element of $\R^n$, and have $\mathbf{h} = t\mathbf{v}$ with $t$ being real.
Since $\mathbf{f}$ is differentiable, we have
\begin{equation*}
    \lim_{t\mathbf{v} \to \mathbf{0}} \frac{\mathbf{f}(\mathbf{x}+t\mathbf{v}) - \mathbf{f}(\mathbf{x}) - t\mathbf{J_f v}}{\norm{t\mathbf{v}}} = \mathbf{0},
\end{equation*}
assuming $\mathbf{v} \neq \mathbf{0}$.
This can be rewritten as
\begin{equation*}
    \lim_{t \to 0} \frac{\mathbf{f}(\mathbf{x}+t\mathbf{v}) - \mathbf{f}(\mathbf{x}) - t\mathbf{J_f v}}{\abs{t}\norm{\mathbf{v}}} = \mathbf{0}.
\end{equation*}
Since $\mathbf{v}$ is fixed (does not depend on $t$), we get
\begin{equation*}
    \lim_{t \to 0} \frac{\mathbf{f}(\mathbf{x}+t\mathbf{v}) - \mathbf{f}(\mathbf{x}) - t\mathbf{J_f v}}{\abs{t}} = \mathbf{0},
\end{equation*}
which is equivalent to
\begin{equation*}
    \mathbf{J_f v} = \lim_{t \to 0} \frac{\mathbf{f}(\mathbf{x}+t\mathbf{v}) - \mathbf{f}(\mathbf{x})}{t}.
\end{equation*}
The final statement is also true for $\mathbf{v} = \mathbf{0}$, so it is true for all $\mathbf{v}$.
This limit is of the same form as the derivative for $n=1$, as it gives an explicit value of the Jacobian applied to any vector $\mathbf{v}$.
However, since we had to assume that the function is differentiable to obtain this, the existence of this limit does not imply that $\mathbf{J_f}$ exists.
It is still a useful concept, and we call it the directional derivative of $\mathbf{f}$ along the vector $\mathbf{v}$.
The notation for it is $\nabla_\mathbf{v}\mathbf{f}$.
Since $\nabla_\mathbf{v}\mathbf{f}(\mathbf{x}) = \mathbf{J_f v}$, we have
\begin{equation*}
    \eval{\frac{d}{dt} \mathbf{f}(\mathbf{x+t\mathbf{v}})}{t=0} = \eval{\mathbf{f}'(\mathbf{x+t\mathbf{v}})}{t=0} \mathbf{v} = \mathbf{f}'(\mathbf{x})\mathbf{v} = \nabla_\mathbf{v}\mathbf{f}(\mathbf{x}).
\end{equation*}
This just gives us as an alternative way to write the directional derivative.
If we choose $\mathbf{v}$ to be $\ehat_i$, which is the $i$-th standard basis vector for $\R^n$ (for $\R^3$, we have $\ihat$, $\jhat$, and $\khat$),
then the directional derivative becomes the partial derivative.
This works for any function that outputs to $\R^m$, but the partial derivative is conventially defined only for real-values functions.
\begin{equation*}
    \nabla_{\ehat_i}\mathbf{f}(\mathbf{x}) = \lim_{t \to 0} \frac{f(\mathbf{x} + t\ehat_i) - \mathbf{f}(\mathbf{x})}{h}
\end{equation*}
Notation for this includes $\partial f / \partial x_i$, $\partial_i f$, and $f_i$.
This definition make sense intuitively, as the partial derivative is a rate of change in some direction -- restricted to specific directions.
For the next result, note that for any $m \times n$ matrix $\mathbf{A}$, the $j$-th column is given by $\mathbf{A}\ehat_j$.
Then, we can get the $i$-th component of that column by taking the dot product with $\ehat_i$.
Thus, $A_{ij} = (A\ehat_j) \cdot \ehat_i$.
Here, $\ehat_i$ is the $i$-th standard basis vector for $\R^n$, and $\ehat_j$ is the $j$-th standard basis vector for $\R^m$.

For $\mathbf{A} = \mathbf{J_f} = \mathbf{f}'(\mathbf{x})$, noting that $\mathbf{f}(\mathbf{x}) = (f_1(\mathbf{x}), \ldots, f_m(\mathbf{x}))$, we have
\begin{equation*}
    J_{ij} = (\mathbf{f}'(\mathbf{x})\ehat_j) \cdot \ehat_i = \nabla_{\ehat_j}\mathbf{f}(\mathbf{x}) \cdot \ehat_i = \nabla_{\ehat_j}f_i(\mathbf{x}) = \frac{\partial f_i}{\partial x_j}.
\end{equation*}
The formula for the generic entry $J_{ij}$ completely describes our matrix $\mathbf{J_f}$, though may write it out in full as
\begin{equation*}
    \mathbf{J_f} = \begin{bmatrix}
        \dfrac{\partial f_1}{\partial x_1} & \dfrac{\partial f_1}{\partial x_2} & \cdots & \dfrac{\partial f_1}{\partial x_n} \\[1em]
        \dfrac{\partial f_2}{\partial x_1} & \dfrac{\partial f_2}{\partial x_2} & \cdots & \dfrac{\partial f_2}{\partial x_n} \\[0.5em]
        \vdots & \vdots & \ddots & \vdots \\[0.5em]
        \dfrac{\partial f_m}{\partial x_1} & \dfrac{\partial f_m}{\partial x_2} & \cdots & \dfrac{\partial f_m}{\partial x_n}
    \end{bmatrix}.
\end{equation*}
This is the explicit form of the Jacobian matrix of $\mathbf{f}$ at $\mathbf{x}$.
From our previous results, we know that differentiability of $\mathbf{f}$ at $\mathbf{x}$ implies the existence of all directional derivatives at $\mathbf{x}$, and by extension, all partial derivatives.
This representation of the derivative $\mathbf{J_f}$ depends on what we choose as our coordinate system.
To provide a coordinate-independent representation, we would have to use differential forms -- further motivating their study.

\end{appendices}

\end{document}